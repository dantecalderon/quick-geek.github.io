<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Autonomous driving on the sidewalk through OpenCV and Tensorflow</title>
  <meta name="description" content="The creation of autonomous machines is a popular topic today and a lot of interesting things happen here at the amateur level. 

 The oldest and best ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-6974184241884155",
      enable_page_level_ads: true
    });
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>Autonomous driving on the sidewalk through OpenCV and Tensorflow</h1><div class="post__text post__text-html js-mediator-article">  The creation of autonomous machines is a popular topic today and a lot of interesting things happen here at the amateur level. <br><br>  The oldest and best known <a href="https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013">online</a> course was <a href="https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013">Udacity</a> . <br><br>  So, in autonomous machines there is a very fashionable approach - Behavioral Cloning, the essence of which is that the computer learns to behave like a person (behind the wheel), relying only on the recorded input and output data.  Roughly speaking, there is a base of pictures from the camera and the corresponding steering angle. <br><a name="habracut"></a><br>  In theory, having trained a neural network on this data, we can give it a steer machine. <br>  This approach is based on an <a href="https://devblogs.nvidia.com/deep-learning-self-driving-cars/">article from Nvidia</a> . <br><br>  There are many implementations made mainly by Udacity students: <br><br><ul><li>  <a href="https://medium.com/%40ksakmann/behavioral-cloning-make-a-car-drive-like-yourself-dc6021152713">Time</a> </li><li>  <a href="https://medium.com/deep-learning-turkey/behavioral-cloning-udacity-self-driving-car-project-generator-bottleneck-problem-in-using-gpu-182ee407dbc5">Two</a> </li><li>  <a href="https://towardsdatascience.com/implementing-neural-network-used-for-self-driving-cars-from-nvidia-with-interactive-code-manual-aa6780bc70f4">Three</a> </li></ul><br>  Even more interesting is the use in real projects.  For example, the <a href="http://www.donkeycar.com/">Donkey Car</a> machine is controlled by a specially trained <a href="https://wroscoe.github.io/keras-lane-following-autopilot.html">neural network</a> . <br><br>  Such a rich infosphere directly pushes into action, all the more so since my <a href="https://habr.com/ru/post/358230/">robot tank</a> since the <a href="https://habr.com/ru/post/426675/">previous article</a> had come to a certain dead end in its development, and it urgently needed fresh ideas.  It was a bold dream - to walk through the park with your tank, which, in general, is no worse than a domestic dog.  Things are easy - to teach a tank to ride on the sidewalk in the park. <br><br>  So, what is a sidewalk in terms of a computer? <br><br>  Some area in the picture that is different in color from other areas. <br><br>  It so happened that in the parks accessible to me, the pavement turned out to be the grayest object in the picture. <br><br>  (The grayest refers to the minimum difference between the RGB values).  This property is gray and will be the key to recognizing the sidewalk. <br><br>  Another important gray parameter is brightness.  Autumn photos consist of gray a little less than completely, so that the differences from the road from the roadside are only in shades. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bm/ms/-r/bmms-r_lkka2o98ijtar60bcabu.jpeg" alt="tank in a park"></div><br>  A couple of the most obvious approaches are in pre-calibration - set up the robot so that the road takes up most of the screen and <br><br><ul><li>  take average brightness (in HSV format) </li><li>  or the average RGB of a piece, guaranteed to consist of a road (in this case, it will be the lower left corner). </li></ul><br>  Having established such criteria for recognizing the pavement, we run through the picture and get some kind of outline of the road. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2l/ee/zp/2leezpaaxpxj2itjjzkosogz7js.jpeg"></div><br>  The next step is to turn a coarse spot into action - go straight or turn right or left. <br><br>  We go straight, if the right edge is visible and the angle is less than 45 degrees from the vertical. <br><br>  Turn left if the right edge is visible and the angle deviates from the vertical downwards. <br>  Turn right if we do not see the right edge. <br><br>  The right edge of the frivolous spot is rather bleak to solve this problem with the help of geometry.  Let the artificial intelligence be better off looking for patterns of inclination in these fragments. <br><br>  This is where neural networks come to the rescue. <br><br>  The original pictures are washed out, compressed and cut, select the gray pavement and select the 64x64 black and white masks. <br><br>  We decompose these masks into 3 piles - Left, Right, Straight and train the neural network classifier on them. <br><br>  Collecting and preparing data is a tedious task, it took a couple of months. <br><br>  Here are samples of masks: <br><br>  Left: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uk/hb/he/ukhbhes851fdxkdam0nz6zlelwk.png"></div><br>  Right: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/80/wg/0c/80wg0c8krzbnnaenh-dfe_tjgvg.png"></div><br>  Straight: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/4q/2_/qc4q2_fu1j9loatpljcoqbtzmng.png"></div><br>  To work with the neural network, I used Keras + Tensorflow. <br><br>  At first there was an idea to take the structure of a neural network from Nvidia, but, obviously, it is intended for several other tasks and does not do very well with the classification.  As a result, it turned out that the simplest neural network from any tutorial on multi-category classification yields quite acceptable results. <br><br><pre><code class="python hljs">model = Sequential() activation = <span class="hljs-string"><span class="hljs-string">"relu"</span></span> model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, padding=<span class="hljs-string"><span class="hljs-string">"same"</span></span>, input_shape=input_shape)) model.add(Activation(activation)) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, padding=<span class="hljs-string"><span class="hljs-string">"same"</span></span>)) model.add(Activation(activation)) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">500</span></span>)) model.add(Activation(activation)) model.add(Dense(cls_n)) opt = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>) model.add(Activation(<span class="hljs-string"><span class="hljs-string">"softmax"</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">"categorical_crossentropy"</span></span>, optimizer=opt, metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>])</code> </pre> <br>  Having trained the first version of the network, I ran into its incompatibility with the Raspberry Pi.  Before that, I used Tensorflow version 1.1, with the help of shamanism, collected by <a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi">one very clever person</a> . <br><br>  Unfortunately, this version is outdated and could not read models from Keras. <br><br>  However, recently, people from Google have finally condescended and collected TF under the Raspberry Pi, albeit under the new version of Raspbian - Stretch.  Stretch was all good, but a year ago, OpenCV wasn‚Äôt going for me, so the tank went to Jessie. <br><br>  Now, under the pressure of change, I had to switch to Stretch.  Tensorflow got up without any problems (although it took several hours).  OpenCV for the year also did not stand still and version 4.0 has already been released.  So we managed to assemble it under Stretch, so there are no more obstacles for migration. <br><br>  There were doubts how Raspberry would pull in such a monster as Tensorflow in realtime, but everything turned out to be generally acceptable - despite the initial network load of a few seconds, the classification itself is able to work several times per second without significant memory and CPU overshoot. <br><br>  As a result, most of the problems and mistakes happen at the stage of recognition of the road. <br>  The neural network misses very rarely, despite the simplicity of the structure. <br><br>  With the updated firmware, the tank cuts through the park. <br><br>  As a result of injuries, the robot constantly blows to the right, so that without artificial intelligence, he quickly leaves on the lawn. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tg/lq/8g/tglq8g0ignz9dcq6cex5jiekjzu.gif"></div><br>  You can now walk it in the morning and detect oncoming dogs. <br><br>  References: <br><br><ul><li>  <a href="https://devblogs.nvidia.com/deep-learning-self-driving-cars/">Nvidia article on deep learning for autonomous machines</a> </li><li>  <a href="https://wroscoe.github.io/keras-lane-following-autopilot.html">Autopilot Training for Donkey Car</a> </li><li>  <a href="https://github.com/tprlab/pitanq">Repo with tank firmware</a> </li><li>  <a href="https://github.com/tprlab/pi-opencv">Repo with collected OpenCV for Raspbian</a> </li><li>  <a href="https://github.com/tprlab/pitanq-selfwalk">Repo with script and data for training the neural network</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/439928/">https://habr.com/ru/post/439928/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>