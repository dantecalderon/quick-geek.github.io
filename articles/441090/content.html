<div class="post__text post__text-html js-mediator-article">  <i>Hacking music to democratize derivative content</i> <br><br><blockquote>  <b>Disclaimer:</b> all intellectual property, projects and methods described in this article are disclosed in patents US10014002B2 and US9842609B2. </blockquote><br>  I wish to return to 1965, knock on the front door of the Abbey Road studio with a pass, go inside - and hear the real voices of Lennon and McCartney ... Well, let's try.  Input: The Beatles' Mid-Quality MP3 Song <i>We Can Work it Out</i> .  The top track is the input mix, the bottom track is the isolated vocal that our neural network allocated. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/305275806" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  Formally, this problem is known as audio source separation or audio source separation.  It consists in the restoration or reconstruction of one or several source signals that are mixed with other signals as a result of a <i>linear or convolutional</i> process.  This area of ​​research has many practical applications, including improving sound quality (speech) and noise elimination, musical remixes, spatial distribution of sound, remastering, etc. Sound engineers sometimes call this technique demixing.  There are a large number of resources on this topic, from blind signal separation with independent component analysis (ICA) to semi-controlled factorization of non-negative matrices and ending with more recent neural network approaches.  Good information on the first two points can be found in <a href="https://ccrma.stanford.edu/~njb/teaching/sstutorial/">these mini-manuals</a> from CCRMA, which at one time I was very useful. <br><br>  <b>But before diving into the development ... quite a bit of the philosophy of applied machine learning ...</b> <br><br>  I was engaged in processing signals and images even before the slogan “deep learning solves everything” was spread, so I can provide you with a solution as a travel <i>engineering feature</i> and show <b>why a neural network is the best approach for this particular problem</b> .  What for?  Very often, I see people writing something like this: <br><br>  <i>“With deep learning, you no longer need to worry about choosing signs;</i>  <i>it will do it for you. ”</i> <br><br>  or worse ... <br><br>  <i>"The difference between machine learning and deep learning</i> [wait a minute ... deep learning is still machine learning!] <i>In ML you take out the signs yourself, but in deep learning it happens automatically within the network."</i> <br><br>  Such generalizations probably come from the fact that DNNs can be very effective in exploring good hidden spaces.  But so it is impossible to generalize.  It really upsets me when recent graduates and practitioners are amenable to the above misconceptions and adopt the “deep-learning-solves-all” approach.  They say that it is enough to sketch a bunch of raw data (even if after a little preliminary processing) - and everything will work as it should.  In the real world, you need to take care of things such as performance, execution in real time, etc. Because of such misconceptions, you will be stuck for a very long time in the experiment mode ... <br><br>  <b>Feature Engineering remains a very important discipline in the design of artificial neural networks.</b>  <b>As in any other ML technique, in most cases it is precisely it that distinguishes effective production-level solutions from unsuccessful or ineffective experiments.</b>  <b>A deep understanding of your data and their nature still means a lot ...</b> <br><br><h1>  From A to Z </h1><br>  Ok, I finished the sermon.  Now let's see why we are here!  As with any data processing problem, we’ll first see how they look.  Take a look at the following vocal excerpt from the original studio recording. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/305288385" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Studio vocals of 'One Last Time', Ariana Grande</font></i> <br><br>  Not too interesting, right?  Well, that's because we visualize the signal <i>in time</i> .  Here we only see the amplitude changes over time.  But you can extract all sorts of other things, such as amplitude envelopes (envelope), rms values ​​(RMS), the rate of change from positive amplitude to negative (zero-crossing rate), etc., but these <i>signs are</i> too <i>primitive</i> and not sufficiently distinctive, to help in our problem.  If we want to extract vocals from an audio signal, we first need to somehow determine the structure of human speech.  Fortunately, the <a href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform">window Fourier transform</a> (STFT) comes to the rescue. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/305391461" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Amplitude spectrum STFT - window size = 2048, overlap = 75%, logarithmic frequency scale [Sonic Visualizer]</font></i> <br><br>  Although I love speech processing and definitely love playing with <i>input filter modeling, kepstrom, sratottami, LPC, MFCC</i> and so on <i>, we’ll</i> skip all this nonsense and focus on the main elements related to our problem so that the article is understandable to as many people as possible. not just signal processing specialists. <br><br>  So, what does the structure of human speech tell us? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/541/f8d/c74/541f8dc74fecdb1e994d560b44da112d.png"><br><br>  Well, we can define three main elements here: <br><br><ul><li>  <b>The fundamental frequency</b> (f0), which is determined by the vibration frequency of our vocal cords.  In this case, Ariana sings in the range of 300-500 Hz. <br></li><li>  A number of <b>harmonics</b> above f0, which follow a similar form or pattern.  These harmonics appear at frequencies that are multiples of f0. <br></li><li>  <b>Unvocalized</b> speech, which includes consonants, such as 't', 'p', 'k', 's' (which are not produced by the vibration of the vocal cords), breathing, etc. All this manifests itself in the form of short bursts in the high-frequency region. </li></ul><br><h1>  The first attempt using the rules </h1><br>  Let's forget for a second what is called machine learning.  Can a vocal extraction method be developed based on our knowledge of the signal?  Let me try ... <br><br>  <b><i>Naive</i> vocal isolation V1.0:</b> <br><br><ol><li>  Identify sites with vocals.  In the original signal a lot of things.  We want to focus on those areas that really contain vocal content, and ignore everything else. <br></li><li>  To distinguish between voiced and unvoiced speech.  As we have seen, they are very different.  Probably, they need to be processed in different ways. <br></li><li>  Estimate the change in fundamental frequency over time. <br></li><li> Based on pin 3, apply some kind of mask to capture the harmonics. <br></li><li>  Do something with fragments of unvoiced speech ... </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/ecf/bb4/82e/ecfbb482ea6c1b29b96a13ce5cf8a5a2.gif"><br><br>  If we work with dignity, the result should be a <i>soft</i> or <i>bitmask</i> , the application of which to the amplitude of the STFT (element-by-element multiplication) gives an approximate reconstruction of the amplitude of the STFT vocal.  Then we combine this vocal STFT with information about the phase of the original signal, calculate the inverse STFT and get the time signal of the reconstructed vocals. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/56d/fb7/012/56dfb70125de59f0e1ee03b58e6c79e6.png"><br><br>  Doing it from scratch is a big job.  But for the sake of demonstration, <a href="https://code.soundsoftware.ac.uk/projects/pyin">let's</a> apply the implementation of <a href="https://code.soundsoftware.ac.uk/projects/pyin">the pYIN algorithm</a> .  Although it is designed to solve step 3, it performs steps 1 and 2 quite properly with the correct settings, tracking the vocal base even in the presence of music.  The example below contains the output after processing this algorithm, without processing unvoiced speech. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/305636014" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  So what...?  He seems to have done all the work, but there is no good quality and close.  Perhaps spending more time, energy and money, we will improve this method ... <br><br>  But let me ask you ... <br><br>  What happens if a <b>few voices</b> appear on the track, but this is often found in at least 50% of modern professional tracks? <br><br>  What happens if vocals are processed with <b>reverberation, delays</b> and other effects?  Let's take a look at the last chorus of Ariana Grande from this song. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/306589126" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Do you already feel pain ...?  I am yes. <br><br>  Such methods on the hard rules very quickly turn into a house of cards.  The problem is too complicated.  C too many rules, too many exceptions and too many different conditions (effects and details settings).  The multi-step approach also implies that errors in one step spread problems to the next step.  Improving each step will be very costly: it will take a large number of iterations to do everything correctly.  And last but not least, it is likely that in the end we will have a very resource-intensive conveyor, which in itself can negate all efforts. <br><br>  <b>In such a situation, it’s time to start thinking about a more <i>comprehensive</i> approach and let ML find out some of the basic processes and operations necessary to solve the problem.</b>  <b>But we still have to show our skills and do feature engineering, and you will see why.</b> <br><br><h1>  Hypothesis: use neural network as a transfer function that translates mixes into vocals </h1><br>  Looking at the achievements of convolutional neural networks in photo processing, why not apply the same approach here? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f0/356/00a/1f035600aadc5f6bb50d7478984aa1d1.png"><br>  <i><font color="gray">Neural networks successfully solve such tasks as image coloring, sharpening and resolution</font></i> <br><br>  In the end, you can also present the sound signal "as an image" using the short-term Fourier transform, right?  Although these <i>sound images</i> do not correspond to the statistical distribution of natural images, they still have spatial patterns (in time and frequency space) in which the network can be trained. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/54c/b43/7fa/54cb437fa908fbe8b4d36bd120e2d009.png"><br>  <i><font color="gray">Left: drum beat and baseline at the bottom, a few synthesizer sounds in the middle, all this mixed with vocals.</font></i>  <i><font color="gray">Right: vocals only</font></i> <br><br>  Conducting such an experiment would be costly because it is difficult to obtain or generate the necessary training data.  But in applied research, I always try to apply this approach: first, <b>to identify a simpler problem that confirms the same principles</b> , but does not require a lot of work.  This allows you to evaluate the hypothesis, to perform iterations faster and correct the model with minimal losses if it does not work as it should. <br><br>  The implied condition is that the <b>neural network must understand the structure of human speech</b> .  A simpler problem may be the following: <i>will the neural network be able to detect the presence of speech on an arbitrary piece of sound recording</i> .  We are talking about a reliable <a href="https://en.wikipedia.org/wiki/Voice_activity_detection">voice activity detector (VAD)</a> implemented in the form of a binary classifier. <br><br><h3>  Designing feature space </h3><br>  We know that beeps, such as music and human speech, are based on temporal dependencies.  Simply put, nothing happens in isolation at a given time.  If I want to know if there is a voice on a particular piece of sound recording, then I need to look at the neighboring regions.  This <i>temporal context</i> provides good information about what is happening in the area of ​​interest.  At the same time, it is desirable to perform the classification with very small time increments in order to recognize the human voice with the highest possible time resolution. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/471/d58/2d4/471d582d4d8dad39249584940137d4e3.gif"><br><br>  Let's count a little ... <br><br><ul><li>  Sampling frequency (fs): 22050 Hz (we lower the sampling rate from 44100 to 22050) <br></li><li>  STFT design: window size = 1024, hop size = 256, interpolation of the <a href="https://en.wikipedia.org/wiki/Mel_scale">chalk scale</a> for the weighing filter based on perception.  Since our input data is <i>real</i> , you can work with half STFT (the explanation is beyond the scope of this article ...), keeping the DC component (optional), which gives us 513 frequency bins. <br></li><li>  Target resolution of classification: one STFT frame (~ 11.6 ms = 256/22050) <br></li><li>  Target time context: ~ 300 milliseconds = 25 STFT frames. <br></li><li>  Target number of teaching examples: 500 thousand <br></li><li>  Assuming that we use a sliding window in steps of 1 STFT timeframe to generate training data, we need about 1.6 hours of tagged sound to generate 500 thousand sample data. </li></ul><br>  With the above requirements, the input and output of our binary classifier is as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f57/01c/bd9/f5701cbd91fe9ba38f89b541b9d4492e.png"><br><br><h3>  Model </h3><br>  Using Keras, we will build a small model of a neural network to test our hypothesis. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">513</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>)) model.add(LeakyReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, decay=<span class="hljs-number"><span class="hljs-number">1e-6</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/447/ebe/19f/447ebe19f6ba953b4af0b4bed1d9e7af.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d1/002/083/7d1002083c78486e36dd92959ec5afbd.png"><br><br>  By separating the 80/20 data into training and testing after ~ 50 epochs, we get <b>accuracy when testing ~ 97%</b> .  This is sufficient proof that our model is able to distinguish vocals in musical sound fragments (and fragments without vocals).  If you check some feature maps from the 4th convolutional layer, then we can conclude that the neural network seems to have optimized its cores for two tasks: filtering the music and filtering the vocals ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/438/bce/536/438bce536c3aa746a3120e2364b512c8.png"><br>  <i><font color="gray">An example of an object map at the output of the 4th convolutional layer.</font></i>  <i><font color="gray">Apparently, the output on the left is the result of kernel operations in an attempt to save vocal content while ignoring music.</font></i>  <i><font color="gray">High values ​​resemble the harmonic structure of human speech.</font></i>  <i><font color="gray">The object map on the right appears to be the result of the opposite task.</font></i> <br><br><h1>  From voice detector to disconnect signal </h1><br>  Having solved a simpler classification problem, how do we get to the real vocal selection of music?  Well, looking at the first <i>naive</i> method, we still want to somehow get the amplitude spectrogram for vocals.  Now it becomes a regression task.  What we want to do is for a specific timeframe from the STFT of the original signal, that is, a mix (with sufficient time context), to calculate the corresponding amplitude spectrum for the vocals in this timeframe. <br><br>  <b>What about the training data set?</b>  <b>(you can ask me at this moment)</b> <br><br>  Damn ... why so.  I was going to review this at the end of the article so as not to be distracted from the topic! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ac/67f/91d/9ac67f91d85022f6bbc75f296ce3f04a.png"><br><br>  If our model is well trained, then for logical inference it is only necessary to implement a simple sliding window to the STFT mix.  After each prediction, move the window to the right by 1 timeframe, predict the next frame with vocals and associate it with the previous prediction.  As for the model, let's take the same model that was used for the voice detector and make small changes: the output waveform is now (513.1), linear activation at the output, MSE as a function of loss.  Now we begin training. <br><br>  <b>Don't rejoice yet ...</b> <br><br>  Although such an I / O representation makes sense, after training our model several times, with different parameters and data normalizations, there are no results.  It seems we are asking too much ... <br><br>  We have moved from a binary classifier to a <i>regression</i> on a 513-dimensional vector.  Although the network is studying the task to some extent, there are still obvious artifacts and interference from other sources in the restored vocals.  Even after adding additional layers and increasing the number of model parameters, the results do not change much.  And then the question arises: <b>how to deceive the “simplify” task for the network, and at the same time achieve the desired results?</b> <br><br>  What if, instead of estimating the amplitude of the STFT vocal, to train the network in obtaining a binary mask, which, when applied to the STFT mix, gives us a simplified, but <b>perceptually acceptable</b> amplitude vocal spectrogram? <br><br>  Experimenting with various heuristics, we came up with a very simple (and, of course, unorthodox in terms of signal processing ...) method of extracting vocals from mixes using binary masks.  Without going into details, the essence is as follows.  Imagine the output as a binary image, where the value '1' indicates the <b>predominant presence of vocal content</b> at a given frequency and timeframe, and the value '0' indicates the prevailing presence of music in a given place.  We can call it the <i>binarization of perception</i> , just to come up with some name.  Visually, it looks pretty ugly, to be honest, but the results are surprisingly good. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16f/857/721/16f85772187f89a73baf7fe0158aba2c.png"><br><br>  Now our problem becomes a peculiar hybrid of regression-classification (very roughly speaking ...).  We ask the model to “classify pixels” at the output as vocal or non-vocal, although conceptually (as well as from the point of view of the MSE loss function used) the task still remains regression. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e21/6a0/654/e216a065488c058c37e2758563ff4052.png"><br><br>  Although this distinction may seem inappropriate for some, in fact it is of great importance in the ability of the model to study the task, the second of which is simpler and more limited.  At the same time, this allows us to keep our model relatively small in terms of the number of parameters, given the complexity of the task, something very desirable for working in real time, which in this case was a design requirement.  After some minor tweaks, the final model looks like this. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/d25/856/416d2585671e97a1f39c9584a30d4bbf.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/04e/4d5/21e04e4d5642a3282aa445846c64c576.png"><br><br><h3>  How to restore the time domain signal? </h3><br>  Essentially, as in the <i>naive method</i> .  In this case, for each pass, we forecast one timeframe of the binary vocal mask.  Again, by implementing a simple sliding window with a step of one timeframe, we continue to evaluate and merge successive timeframes, which ultimately make up the entire vocal binary mask. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a34/2a5/ae1/a342a5ae1b0ca37825978f7b92d574cb.gif"><br><br><h3>  Creating a training set </h3><br>  As you know, one of the main problems when learning with a teacher (leave these toy examples with ready datasets) is the correct data (in quantity and quality) for a specific problem that you are trying to solve.  Based on the input and output concepts described, to teach our model, you first need a significant amount of mixes and corresponding, ideally aligned and normalized vocal tracks.  Such a set can be created in several ways, and we used a combination of strategies, ranging from manually creating pairs [mix &lt;-&gt; vocals] based on several a cappelles found on the Internet, to searching for musical material of rock bands and scraping Youtube.  Just to give you an idea of ​​how time-consuming and painful the process is, a part of the project was the development of such a tool for automatic pairing [mix &lt;-&gt; vocals]: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/617/b71/5ac/617b715acc8d8c913752054f84214c8b.png"><br><br>  You need a really large amount of data in order for the neural network to learn the transfer function for translating mixes into vocals.  Our final set consisted of about 15 million samples of 300 ms mixes and their corresponding vocal binary masks. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21f/01a/02d/21f01a02d22dfc4f2d615e511cf470a6.png"><br><br><h3>  Pipeline architecture </h3><br>  As you probably know, creating an ML model for a specific task is only half the battle.  In the real world, you need to think about the software architecture, especially if you need to work in real time or close to it. <br><br>  In this particular implementation, the reconstruction in the time domain can occur immediately after the prediction of the full binary vocal mask (standalone mode) or, more interestingly, in multi-threaded mode, where we receive and process data, restore the vocal and reproduce the sound - all in small segments, close to streaming and even almost in real time, processing music that is recorded on the fly with minimal delay.  In general, this is a separate topic, and I will leave it for another article on <b>ML-pipelines in real time</b> ... <br><br><h1>  Probably, I said enough, so why not listen to a couple of examples !? </h1><br><h3>  Daft Punk - Get Lucky (studio recording) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/315172280" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Here you can hear some minimal noise from the drums ...</font></i> <br><br><h3>  Adele - Set Fire to the Rain (live recording!) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/315172388" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Notice how, at the very beginning, our model extracts the crowds of the crowd as vocal content :).</font></i>  <i><font color="gray">In this case there is some interference from other sources.</font></i>  <i><font color="gray">Since this is a live recording, it seems acceptable that the extracted vocals are worse in quality than the previous ones.</font></i> <br><br><h1>  Yes, and "something else" ... </h1><br><h1>  If the system works for vocals, why not apply it to other instruments ...? </h1><br>  The article is already quite large, but considering the work done, you deserve to hear the latest demo.  With exactly the same logic as when extracting vocals, we can try to divide stereo music into components (drums, bass, vocals, others), making some changes in our model and, of course, having the appropriate set of training :). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/315173879" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Thank you for reading.  As a final note: as you can see, the actual model of our convolutional neural network is not so special.  The success of this work led to <b>Feature Engineering</b> and an accurate hypothesis testing process, which I will write about in future articles! </div>