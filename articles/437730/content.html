<div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/bd/vf/bj/bdvfbjzxvmt2ijnwu5zjqnz3e1e.jpeg"></p><br><p>  A short story about the "heavy" request and elegant solution to the problem </p><br><p>  Recently, we were alerted at night by alerts: there is not enough disk space.  We quickly figured out what the problem is in ETL problems. </p><br><p>  The ETL task was performed in a table where binary records and dumps are stored.  Every night this task was supposed to remove duplicate dumps and free up space. </p><a name="habracut"></a><br><p>  To search for duplicate dumps we used this query: </p><br><pre><code class="plaintext hljs">id, MIN(id) OVER (PARTITION BY blob ORDER BY id) FROM dumps</code> </pre> <br><p>  The request combines the same dumps by BLOB-field.  Using the window function, we get the ID of the first occurrence of each dump.  Then this request deletes all duplicate dumps. </p><br><p>  The request was executed for some time, and, as can be seen from the logs, I ate a lot of memory.  The graph shows how he scored free disk space every night: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/v2/sn/ye/v2snyevasqsmvjjzpc7lbjs4mpe.png"></a> </p><br><p>  Over time, the request required more and more memory, the dips deepened.  And, looking at the execution plan, we immediately saw where everything goes: </p><br><pre> <code class="plaintext hljs"> Buffers: shared hit=3916, temp read=3807 written=3816 -&gt; Sort (cost=69547.50..70790.83 rows=497332 width=36) (actual time=107.607..127.485 rows=39160) Sort Key: blob, id Sort Method: external merge Disk: 30456kB Buffers: shared hit=3916, temp read=3807 written=3816 -&gt; Seq Scan on dumps (cost=0..8889.32 rows=497332 width=36) (actual time=0.022..8.747 rows=39160) Buffers: shared hit=3916 Execution time: 159.960 ms</code> </pre> <br><p>  <strong>Sorting takes a lot of memory.</strong>  In terms of execution from the test data set, sorting requires approximately 30 MB of memory. </p><br><h3 id="pochemu-tak">  Why is that? </h3><br><p>  PostgreSQL allocates memory for hashing and sorting.  The amount of memory is controlled by the <a href="http://0s.o53xo.obxxg5dhojsxg4lmfzxxezy.nblz.ru/docs/9.6/runtime-config-resource.html"><code>work_mem</code></a> parameter.  The default work_mem size is 4 MB.  If more than 4 MB is needed for hashing or sorting, PostgreSQL temporarily uses disk space. </p><br><p>  Our request obviously consumes more than 4 MB, so the database uses so much memory.  We decided: we will not rush - and did not increase the parameter and expand the storage.  It is better to look for another way to <strong>trim the memory for sorting</strong> . </p><br><h3 id="ekonomnaya-sortirovka">  Economical sorting </h3><br><p>  "How much sorting will eat - depends on the size of the data set and the sort key. You cannot reduce the data set, but the <strong>key size is possible</strong> . </p><br><p>  For the starting point we take the average size of the sort key: </p><br><pre> <code class="plaintext hljs"> avg ---------- 780</code> </pre> <br><p>  Each key weighs 780. To reduce the binary key, it can be hashed.  In PostgreSQL, there is <a href="http://0s.o53xo.obxxg5dhojsxg4lmfzxxezy.nblz.ru/docs/9.6/functions-string.html">md5</a> for this (yes, not security, but for our purpose it will do).  Let's see how much a BLOB hashed with md5 weighs: </p><br><pre> <code class="plaintext hljs"> avg ----------- 36</code> </pre> <br><p>  The size of the key hashed through md5 is 36 bytes.  <strong>The hashed key weighs only 4% of the original version</strong> . </p><br><p>  Next, we run the original query with the hashed key: </p><br><pre> <code class="plaintext hljs"> id, MIN(id) OVER ( PARTITION BY md5(array_to_string(blob, '') ) ORDER BY id) FROM dumps;</code> </pre> <br><p>  And the execution plan: </p><br><pre> <code class="plaintext hljs"> Buffers: shared hit=3916 -&gt; Sort (cost=7490.74..7588.64 rows=39160 width=36) (actual time=349.383..353.045 rows=39160) Sort Key: (md5(array_to_string(blob, ''::text))), id Sort Method: quicksort Memory: 4005kB Buffers: shared hit=3916 -&gt; Seq Scan on dumps (cost=0..4503.40 rows=39160 width=36) (actual time=0.055..292.070 rows=39160) Buffers: shared hit=3916 Execution time: 374.125 ms</code> </pre> <br><p>  With a hashed key, the request consumes only 4 extra megabytes, that is, a little more than 10% of the previous 30 MB.  <strong>So the size of the sort key greatly influences how much memory the sort eats up</strong> . </p><br><h3 id="dalshe--bolshe">  Further more </h3><br><p>  In this example, we have hashed the BLOB using <code>md5</code> .  Hashes created with MD5 should weigh 16 bytes.  And we got more: </p><br><pre> <code class="plaintext hljs">md5_size ------------- 32</code> </pre> <br><p>  Our hash was exactly twice as large, because <code>md5</code> gives the hash in the form of hexadecimal text. </p><br><p>  In PostgreSQL, you can use MD5 for hashing with the <a href="http://0s.o53xo.obxxg5dhojsxg4lmfzxxezy.nblz.ru/docs/current/pgcrypto.html"><code>pgcrypto</code></a> extension.  <code>pgcrypto</code> creates MD5 of type <a href="http://0s.o53xo.obxxg5dhojsxg4lmfzxxezy.nblz.ru/docs/current/datatype-binary.html"><code>bytea</code> (in binary)</a> : </p><br><pre> <code class="plaintext hljs">select pg_column_size( digest('foo', 'md5') ) as crypto_md5_size crypto_md5_size --------------- 20</code> </pre> <br><p>  The hash is still 4 bytes more than it should be.  Just the type <code>bytea</code> uses these 4 bytes to store the length of the value, but we will not leave it that way. </p><br><p>  It turns out that PostgreSQL's <code>uuid</code> type weighs exactly 16 bytes and supports any arbitrary value, so we get rid of the remaining four bytes: </p><br><pre> <code class="plaintext hljs">uuid_size --------------- 16</code> </pre> <br><p>  That's all.  32 bytes with <code>md5</code> converted to 16 with <code>uuid</code> . </p><br><p>  I checked the effects of the change by taking a larger set of data.  The data itself can not be shown, but I will share the results: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/rt/g4/v6/rtg4v65zaqxezkmjkh9cspgh2bo.jpeg"></a> </p><br><p>  As the table shows, the original problem query weighed 300 MB (and woke us in the middle of the night).  With the key <code>uuid</code> sorting it took just 7 MB. </p><br><h3 id="soobrazheniya-vdogonku">  Considerations after </h3><br><p>  A query with a hashed sort key to memory consumes less, but it works much slower: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/b0/cv/vp/b0cvvpozbm_myvxy0-esybfu7gi.jpeg"></a> </p><br><p>  Hashing uses more CPU, so a query with hash is slower.  But we tried to solve the problem with disk space, besides the task is performed at night, so time is not a problem.  We compromised to save memory. </p><br><p>  This is a great example of the fact that it is <strong>not always necessary to try to speed up database queries</strong> .  It is better to use what is balanced and to squeeze the maximum out of a minimum of resources. </p></div>