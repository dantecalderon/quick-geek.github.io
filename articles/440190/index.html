<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to the simplest neural network and its step-by-step implementation</title>
  <meta name="description" content="Once I stumbled upon a book called ‚ÄúCreate your own neural network‚Äù , the author of which is Tariq Rashid and after reading I was satisfied, unlike ma...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-6974184241884155",
      enable_page_level_ads: true
    });
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>Introduction to the simplest neural network and its step-by-step implementation</h1><div class="post__text post__text-html js-mediator-article">  Once I stumbled upon a book called <b>‚ÄúCreate your own neural network‚Äù</b> , the author of which is <b>Tariq Rashid</b> and after reading I was satisfied, unlike many other manuals on neural networks, which are undoubtedly good in their own way, everything in this book served in simple language with enough examples and advice <br><br>  I also want to go through the same book step by step, namely the practical part - <b>writing a simple neural network code</b> . <br><br>  This article is for those who want to do neural networks and machine learning, but so far hardly understand this amazing field of science.  Below, the simplest <b>skeleton</b> of a neural network code will be described, so that many understand the simplest principle of construction and interaction of all that the neural network consists of. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/747/592/94e/74759294e47907f3e8ba14438ab66001.jpg" alt="image"><br><a name="habracut"></a><br>  Theories on machine learning and neural networks in Habr√© are enough.  But if it is necessary for someone, I will leave some links at the end of the article.  And now, let's start writing the code directly, and we will write in python, it <b>will be better</b> if you use <u><a href="https://github.com/jupyter/notebook">jupyter-notebook</a></u> when writing code <br><br><h2>  Step 1. Initialize the network </h2><br>  First, of course, we need to initialize all the active components of our network. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º numpy ‚Äî —ç—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —è–∑—ã–∫–∞ Python, –¥–æ–±–∞–≤–ª—è—é—â–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫—É –±–æ–ª—å—à–∏—Ö –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤ –∏ –º–∞—Ç—Ä–∏—Ü import numpy # –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º scipy.special , -scipy —Å–æ–¥–µ—Ä–∂–∏—Ç –º–æ–¥—É–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–Ω–æ–≥–∏—Ö –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á, –Ω–∞–º –∂–µ –∑–¥–µ—Å—å –Ω—É–∂–Ω–∞ –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –∏–º—è –∫–æ—Ç–æ—Ä–æ–π -–°–∏–≥–º–æ–∏–¥–∞ import scipy.special #–í–µ—Ä–æ—è—Ç–Ω–æ, –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ import matplotlib.pyplot # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—à –∫–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ class neuralNetwork: # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—à–µ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate): #–í –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –º—ã –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π self, –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –¥–∞–Ω–Ω—ã–µ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è, –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ,—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ) # —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ , —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è, –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è self.inodes = inputnodes self.hnodes = hiddennodes self.onodes = outputnodes # –¢—É—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω—ã –≤–µ—Å–∞ –º–∞—Ç—Ä–∏—Ü—ã, wih - –≤–µ—Å –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º , –∞ —Ç–∞–∫ –∂–µ who- –≤–µ—Å –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º self.wih = numpy.random.rand(self.hnodes, self.inodes)) self.who = numpy.random.rand(self.onodes, self.hnodes)) # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è -—ç—Ç–æ –Ω–∞—à –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä, —Ç–æ –µ—Å—Ç—å, –ø–∞—Ä–∞–º–µ—Ç—Ä , –∫–æ—Ç–æ—Ä—ã–π –º—ã –ø–æ–¥–±–∏—Ä–∞–µ–º —Ä—É—á–∫–∞–º–∏, –∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫ –Ω–∞–º —ç—Ç–æ —É–¥–æ–±–Ω–æ –Ω–∞–º, –∏ , –∫–æ–Ω–µ—á–Ω–æ –∂–µ, –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ self.lr = learningrate # –ù–∞—à–∞ –°–∏–≥–º–æ–∏–¥–∞- —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ self.activation_function = lambda x: scipy.special.expit(x)</span></span></code> </pre> <br><h2>  A little bit about how a node looks like in a neural network. </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/32e/7ad/4b4/32e7ad4b4fac123e73d88c44a8acb8ca.png" alt="image"><br><br>  The picture shows the most that there is a knot, only it is usually presented in the form of a circle, and not a rectangle.  As we see, inside a rectangle (well, or a circle) - this is all abstract, there are 2 functions: <br><br>  The 1st Function is engaged in receiving all input data, taking into account the weights, data, and sometimes even taking into account the displacement neuron (a special neuron that simply allows the graphs to move, not to mix into one ugly heap, that's all) <br><br>  The 2nd Function accepts as a parameter the same value that the first function has summarized, and this second function is called the activation function.  In our case, Sigmoid <br><br>  <b>We continue</b> : <br><br><h2>  Part 2. Neural Network Training </h2><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, inputs_list, targets_list)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à —Å–ø–∏—Å–æ–∫ –≤ –¥–≤—É–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤ inputs = numpy.array(inputs_list, ndmin=2).T # –ø–æ—Å—Ç—É–ø–∞—é—â–∏–µ –Ω–∞ –≤—Ö–æ–¥ –¥–∞–Ω–Ω—ã–µ input targets = numpy.array(targets_list, ndmin=2).T #—Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è targets # –ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–∞ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ hidden_inputs = numpy.dot(self.wih, inputs) # –ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤, –≤—ã—Ö–æ–¥—è—â–∏—Ö –∏–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Å–ª–æ—é. –¢—É—Ç –≤ –Ω–∞—à–µ–º —É–∑–ª–µ, –∫—É–¥–∞ –ø–æ—Å—Ç—É–ø–∞–ª–∏ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é hidden_inputs (1—è —Ñ—É–Ω–∫—Ü–∏—è), —ç—Ç–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ–¥–∞–µ—Ç—Å—è –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä –≤ –°–∏–≥–º–æ–∏–¥—É - —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (2—è —Ñ—É–Ω–∫—Ü–∏—è) hidden_outputs = self.activation_function(hidden_inputs) # –ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –∫–æ–Ω–µ—á–Ω–æ–º(–≤—ã—Ö–æ–¥–Ω–æ–º) —Å–ª–æ–µ final_inputs = numpy.dot(self.who, hidden_outputs) # –ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤, –ø–æ–¥–∞—é—â–∏—Ö—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ final_outputs = self.activation_function(final_inputs) # –ó–Ω–∞—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (–û–∂–∏–¥–∞–Ω–∏–µ - –†–µ–∞–ª—å–Ω–æ—Å—Ç—å) output_errors = targets - final_outputs # –û—à–∏–±–∫–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—à–∏–±–∫–æ–π ,–∫–æ—Ç–æ—Ä—É—é –º—ã –ø–æ–ª—É—á–∏–ª–∏ –¥–ª—è &lt;b&gt;–æ—à–∏–±–∫–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è&lt;/b&gt;, –Ω–æ —É–∂–µ &lt;b&gt;—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–æ –≤–µ—Å–∞–º –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ—è–º–∏&lt;/b&gt;(–∏–Ω–∞—á–µ –≥–æ–≤–æ—Ä—è —Å —É—á–µ—Ç–æ–º —É–º–Ω–æ–∂–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤–µ—Å–æ–≤) hidden_errors = numpy.dot(self.who.T, output_errors) # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º (–Ø–≤–ª–µ–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –ª—é–¥–∏ –∑–æ–≤—É—Ç –æ—à–∏–±–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è) self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs)) # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º –∏ –≤—Ö–æ–¥–Ω—ã–º(–¢–∞ –∂–µ –æ—à–∏–±–∫–∞ –æ—à–∏–±–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≤ –¥–µ–π—Å—Ç–≤–∏–∏) self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs)) pass</span></span></code> </pre><br>  <b>And here we are approaching the end</b> <br><br><h2>  Part 3. Neuron network survey </h2><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#–°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é , –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ def query(self, inputs_list): # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–æ–¥–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –¥–≤—É–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤ inputs = numpy.array(inputs_list, ndmin=2).T # –ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ hidden_inputs = numpy.dot(self.wih, inputs) # –ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤, –ø–æ–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ hidden_outputs = self.activation_function(hidden_inputs) #–ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –∫–æ–Ω–µ—á–Ω–æ–º –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ final_inputs = numpy.dot(self.who, hidden_outputs) #–ü–æ–¥—Å—á–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –∫–æ–Ω–µ—á–Ω–æ–º –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ final_outputs = self.activation_function(final_inputs) return final_outputs</span></span></code> </pre> <br><h2>  We finish business </h2><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#–ü–æ–¥–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ , —Å–∫—Ä—ã—Ç–æ–≥–æ ,–≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ–µ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ(—É–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ &lt;b&gt;–Ω–æ–¥&lt;/b&gt;- —É–∑–ª–æ–≤ –≤ —Ä—è–¥—É –≤—Ö–æ–¥–Ω–æ–≥–æ, —Å–∫—Ä—ã—Ç–æ–≥–æ, –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ input_nodes = 3 hidden_nodes = 3 output_nodes = 3 # –í–æ–∑—å–º–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–±—É—á–µ–Ω–∏—è - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Ä–∞–≤–Ω–æ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä... 0.3! learning_rate = 0.3 # –°–æ–∑–¥–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å(n —ç—Ç–æ –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ neuralNetwork , –ø—Ä–∏ –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä __init__ , –∏ –¥–∞–ª—å—à–µ –≤—Å–µ –±—É–¥–µ—Ç –≤–∫–ª—é—á–∞—Ç—å—Å—è –ø–æ —Ü–µ–ø–æ—á–∫–µ n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)</span></span></code> </pre> <br><h2>  PS </h2><br>  Above was presented the simplest model of a neural network capable of computing.  But no particular application was shown. <br><br>  If you wish, you can continue this code by adding the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> handwriting recognition feature. To do this, you can completely figure out (or just have some fun) having this <a href="https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/blob/master/part2_neural_network_mnist_data.ipynb">jupyter file</a> , my task was to show the code and, if possible, chew for what .  Links to the theory, as promised, attach at the end, well, and you will also find Github and the book of Tariq Rashid, I'll leave them too <br><br>  1. <a href="https://github.com/makeyourownneuralnetwork">Github</a> <br>  2. Book <a href="https://vk.com/doc17249920_465375149%3Fhash%3D5015d5784c5efdd6c9%26dl%3Db93d324a6f554e4c8a">"Create your neural network"</a> <br>  3. Machine Learning Theory <a href="https://habr.com/ru/post/427867/">1</a> <br>  4. Machine Learning Theory <a href="https://habr.com/ru/post/312450/">2</a> <br>  5. Machine Learning Theory <a href="https://habr.com/ru/post/319288/">3</a> <br>  6. Machine Learning Theory <a href="https://habr.com/ru/post/342334/">4</a> <br><br>  You can also read <a href="https://www.coursera.org/learn/vvedenie-mashinnoe-obuchenie/">this</a> course. </div><p>Source: <a href="https://habr.com/ru/post/440190/">https://habr.com/ru/post/440190/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>