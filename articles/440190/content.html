<div class="post__text post__text-html js-mediator-article">  Once I stumbled upon a book called <b>“Create your own neural network”</b> , the author of which is <b>Tariq Rashid</b> and after reading I was satisfied, unlike many other manuals on neural networks, which are undoubtedly good in their own way, everything in this book served in simple language with enough examples and advice <br><br>  I also want to go through the same book step by step, namely the practical part - <b>writing a simple neural network code</b> . <br><br>  This article is for those who want to do neural networks and machine learning, but so far hardly understand this amazing field of science.  Below, the simplest <b>skeleton</b> of a neural network code will be described, so that many understand the simplest principle of construction and interaction of all that the neural network consists of. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/747/592/94e/74759294e47907f3e8ba14438ab66001.jpg" alt="image"><br><a name="habracut"></a><br>  Theories on machine learning and neural networks in Habré are enough.  But if it is necessary for someone, I will leave some links at the end of the article.  And now, let's start writing the code directly, and we will write in python, it <b>will be better</b> if you use <u><a href="https://github.com/jupyter/notebook">jupyter-notebook</a></u> when writing code <br><br><h2>  Step 1. Initialize the network </h2><br>  First, of course, we need to initialize all the active components of our network. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#импортируем numpy — это библиотека языка Python, добавляющая поддержку больших многомерных массивов и матриц import numpy # импортируем scipy.special , -scipy содержит модули для оптимизации, интегрирования, специальных функций, обработки изображений и многих других задач, нам же здесь нужна наша функция активации, имя которой -Сигмоида import scipy.special #Вероятно, нам понадобится визуализировать наши данные import matplotlib.pyplot # Определяем наш класс нейронной сети class neuralNetwork: # Инициализация нашей нейронной сети def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate): #В параметрах мы записываем обязательный self, входные данные, данные скрытого слоя, выходные данные ,скорость обучения соответственно) # устанавливаем количество узлов для входного , скрытого слоя, выходного слоя self.inodes = inputnodes self.hnodes = hiddennodes self.onodes = outputnodes # Тут обозначены веса матрицы, wih - вес между входным и скрытым слоем , а так же who- вес между скрытым и выходным слоем self.wih = numpy.random.rand(self.hnodes, self.inodes)) self.who = numpy.random.rand(self.onodes, self.hnodes)) # Скорость обучения -это наш гиперпараметр, то есть, параметр , который мы подбираем ручками, и в зависимости от того, как нам это удобно нам, и , конечно же, нейронной сети self.lr = learningrate # Наша Сигмоида- функция активации self.activation_function = lambda x: scipy.special.expit(x)</span></span></code> </pre> <br><h2>  A little bit about how a node looks like in a neural network. </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/32e/7ad/4b4/32e7ad4b4fac123e73d88c44a8acb8ca.png" alt="image"><br><br>  The picture shows the most that there is a knot, only it is usually presented in the form of a circle, and not a rectangle.  As we see, inside a rectangle (well, or a circle) - this is all abstract, there are 2 functions: <br><br>  The 1st Function is engaged in receiving all input data, taking into account the weights, data, and sometimes even taking into account the displacement neuron (a special neuron that simply allows the graphs to move, not to mix into one ugly heap, that's all) <br><br>  The 2nd Function accepts as a parameter the same value that the first function has summarized, and this second function is called the activation function.  In our case, Sigmoid <br><br>  <b>We continue</b> : <br><br><h2>  Part 2. Neural Network Training </h2><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, inputs_list, targets_list)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Конвертируем наш список в двумерный массив inputs = numpy.array(inputs_list, ndmin=2).T # поступающие на вход данные input targets = numpy.array(targets_list, ndmin=2).T #целевые значения targets # Подсчет сигнала в скрытом слое hidden_inputs = numpy.dot(self.wih, inputs) # Подсчет сигналов, выходящих из скрытого слоя к выходному слою. Тут в нашем узле, куда поступали все данные в переменную hidden_inputs (1я функция), эта переменная подается как параметр в Сигмоиду - функцию активации (2я функция) hidden_outputs = self.activation_function(hidden_inputs) # Подсчет сигналов в конечном(выходном) слое final_inputs = numpy.dot(self.who, hidden_outputs) # Подсчет сигналов, подающихся в функцию активации final_outputs = self.activation_function(final_inputs) # Значение ошибки (Ожидание - Реальность) output_errors = targets - final_outputs # Ошибка скрытого слоя становится ошибкой ,которую мы получили для &lt;b&gt;ошибки выходного слоя&lt;/b&gt;, но уже &lt;b&gt;распределенные по весам между скрытым и выходным слоями&lt;/b&gt;(иначе говоря с учетом умножения соответствующих весов) hidden_errors = numpy.dot(self.who.T, output_errors) # Обновление весов между скрытым слоем и выходным (Явление того, что люди зовут ошибкой обратного распространения) self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs)) # Обновление весов между скрытым слоем и входным(Та же ошибка ошибка обратного распространения в действии) self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs)) pass</span></span></code> </pre><br>  <b>And here we are approaching the end</b> <br><br><h2>  Part 3. Neuron network survey </h2><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Создаем функцию , которая будет принимать входные данные def query(self, inputs_list): # Конвертируем поданный список входных данных в двумерный массив inputs = numpy.array(inputs_list, ndmin=2).T # Подсчет сигналов в скрытом слое hidden_inputs = numpy.dot(self.wih, inputs) # Подсчет сигналов, поданных в функцию активации hidden_outputs = self.activation_function(hidden_inputs) #Подсчет сигналов в конечном выходном слое final_inputs = numpy.dot(self.who, hidden_outputs) #Подсчет сигналов в конечном выходном слое, переданных в функцию активации final_outputs = self.activation_function(final_inputs) return final_outputs</span></span></code> </pre> <br><h2>  We finish business </h2><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Подаем конкретное значение для входного , скрытого ,выходного слоев соответственно(указываем количество &lt;b&gt;нод&lt;/b&gt;- узлов в ряду входного, скрытого, выходного соответственно input_nodes = 3 hidden_nodes = 3 output_nodes = 3 # Возьмем коэффициент обучения - скорость обучения равной, например... 0.3! learning_rate = 0.3 # Создаем нейронную сеть(n это объект класса neuralNetwork , при его создании запустится конструктор __init__ , и дальше все будет включаться по цепочке n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)</span></span></code> </pre> <br><h2>  PS </h2><br>  Above was presented the simplest model of a neural network capable of computing.  But no particular application was shown. <br><br>  If you wish, you can continue this code by adding the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> handwriting recognition feature. To do this, you can completely figure out (or just have some fun) having this <a href="https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/blob/master/part2_neural_network_mnist_data.ipynb">jupyter file</a> , my task was to show the code and, if possible, chew for what .  Links to the theory, as promised, attach at the end, well, and you will also find Github and the book of Tariq Rashid, I'll leave them too <br><br>  1. <a href="https://github.com/makeyourownneuralnetwork">Github</a> <br>  2. Book <a href="https://vk.com/doc17249920_465375149%3Fhash%3D5015d5784c5efdd6c9%26dl%3Db93d324a6f554e4c8a">"Create your neural network"</a> <br>  3. Machine Learning Theory <a href="https://habr.com/ru/post/427867/">1</a> <br>  4. Machine Learning Theory <a href="https://habr.com/ru/post/312450/">2</a> <br>  5. Machine Learning Theory <a href="https://habr.com/ru/post/319288/">3</a> <br>  6. Machine Learning Theory <a href="https://habr.com/ru/post/342334/">4</a> <br><br>  You can also read <a href="https://www.coursera.org/learn/vvedenie-mashinnoe-obuchenie/">this</a> course. </div>