<div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/pa/ue/x8/pauex8un6--wep6-1ehqvmciieg.png"><br><br>  The topic of increasing the productivity of operating systems and searching for bottlenecks is gaining tremendous popularity.  In this article we will talk about a single tool for finding these very places using the example of block stack work in Linux and one case of the host shutdown. <br><br><h2>  Example 1. Test </h2><br><h3>  Nothing works </h3><br>  Testing in our department is synthetics on grocery iron, and later - tests of application software.  We received the Intel Optane CD for testing.  We have already written about testing Optane disks <a href="https://blog.selectel.ru/tag/intel-optane-ssd/">in our blog</a> earlier. <br><br>  The disk was installed in the standard server, collected relatively long ago under one of the cloud projects. <br><a name="habracut"></a><br>  During testing, the disk did not perform well: with a test with a queue depth of 1 request in 1 stream, blocks of 4 KB around ~ 70Kiops.  And this means that the waiting time for an answer is huge: approximately 13 microseconds per request! <br><br>  Strange, because the <a href="https://ark.intel.com/products/97159/Intel-Optane-SSD-DC-P4800X-Series-1_5TB-12-Height-PCIe-x4-3D-XPoint">specification</a> promises “Latency - Read 10 µs”, and we got 30% more, the difference is quite significant.  The disk was rearranged to another platform, a more "fresh" assembly used in another project. <br><br><h3>  Why does it work? </h3><br>  It's funny, but the disk on the new platform has worked as it should.  Performance increased, latency decreased, CPU per shelf, 1 stream per 1 request, 4K bytes in blocks, ~ 106Kiops for ~ 9 microseconds per request. <br><br>  And then it's time to <s>compare the settings</s> to get out of the wide leg pants <b>perf</b> .  After all, we wonder why?  With <b>perf</b> you can: <br><br><ul><li>  Take measurements of hardware counters: the number of instruction calls, cash misses, incorrectly predicted branches, etc.  (PMU events) </li><li>  Remove information from static treyspoints, the number of entries </li><li>  Do dynamic tracing </li></ul><br>  For verification, we used CPU sampling. <br><br>  The bottom line is that <b>perf</b> can collect the entire stack trace of a running program.  Naturally, running <b>perf</b> will delay the operation of the entire system.  But we have a <i>-F #</i> flag, where <i>#</i> is the sampling rate, measured in Hz. <br><br>  Here it is important to understand that the higher the sampling rate, the more chances to catch the call of a particular function, but the more brakes the profiler brings into the system.  The lower the frequency, the greater the chance that we will not see part of the stack. <br><br>  When choosing a frequency, you need to be guided by common sense and one cunning - try not to set an even frequency, so as not to get into a situation when some work on a timer with this frequency gets into the samples. <br><br>  Another point that is misleading at first is that the software should be assembled with the <i>-fno-omit-frame-pointer</i> flag, if this is possible, of course.  Otherwise, in trace instead of function names, we will see solid values ​​of <i>unknown</i> .  For some software, the debugging symbols come as a separate package, for example, <i>someutil-dbg</i> .  It is recommended to install them before running <b>perf</b> . <br><br>  We performed the following actions: <br><br><ul><li>  Taken fio from git: //git.kernel.dk/fio.git, tag fio-3.9 </li><li>  Added option <em>-fno-omit-frame-pointer</em> to CPPFLAGS in the Makefile </li><li>  <em>Run make -j8</em> </li></ul><br><pre><code class="bash hljs">perf record -g ~/fio/fio --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=randread --bs=4k --ioengine=pvsync2 --filename=/dev/nvme0n1 --direct=1 --hipri --filesize=1G</code> </pre> <br>  The -g option is needed to capture the stack of traces. <br><br>  You can view the result with the command: <br><br><pre> <code class="bash hljs">perf report -g fractal</code> </pre> <br>  The option <i>-g fractal is</i> needed so that percentages reflecting the number of samples with this function and shown by <b>perf</b> are relative to the calling function, the number of calls to which is taken as 100%. <br><br>  Towards the end of the fio long call stack on the “fresh build” platform, we will see: <br><br><img src="https://habrastorage.org/webt/_y/pn/jb/_ypnjb3xkf3urq140p0qssevtku.png"><br><br>  And on the “old build” platform: <br><br><img src="https://habrastorage.org/webt/gq/kx/ul/gqkxulpyxspbmfudoxhh7ysdv1e.png"><br><br>  Fine!  But I want beautiful flamegraphs. <br><br><h3>  Building Flamegraphs </h3><br>  To make it beautiful, there are two tools: <br><br><ul><li>  Relatively more static <a href="https://github.com/brendangregg/FlameGraph">flamegraph</a> </li><li>  <a href="https://github.com/Netflix/flamescope">Flamescope</a> , which allows you to select a specific period of time from the collected samples.  This is very useful when the code you are looking for loads the CPU in short bursts. </li></ul><br>  These utilities accept the output <b>perf script&gt; result</b> . <br><br>  Download the <i>result</i> and send it through the pipe in <i>svg</i> : <br><br><pre> <code class="bash hljs">FlameGraph/stackcollapse-perf.pl ./result | FlameGraph/flamegraph.pl &gt; ./result.svg</code> </pre> <br>  Open in the browser and enjoy the clickable image. <br><br>  You can use another method: <br><br><ol><li>  Add <i>result</i> to flamescope / example / </li><li>  Run python ./run.py </li><li>  We come through the browser on the 5000th port of a local host </li></ol><br><h3>  What do we see in the end? </h3><br>  A good fio spends a lot of time in <a href="https://events.static.linuxfound.org/sites/events/files/slides/lemoal-nvme-polling-vault-2017-final_0.pdf">polling</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o1/zg/wy/o1zgwy-l6idzwcxniq16ndbskvo.png"></div><br>  A bad fio spends time anywhere, but not in a pollingue: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3z/er/bz/3zerbzvtrpwznzewdteyf6bexfq.png"></div><br>  At first glance, it seems that polling does not work on the old host, but everywhere there is a 4.15 kernel of the same assembly and polling is enabled by default on NVMe disks.  Check if polling is enabled in <b>sysfs</b> : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /sys/class/block/nvme0n1/queue/io_poll 1</span></span></code> </pre> <br>  During tests, <i>preadv2</i> calls with the <i>RWF_HIPRI</i> flag are <i>used</i> - a necessary condition for polling to work.  And, if you carefully study the flame (or the previous screenshot from the <b>perf report</b> output), you can find it, but it takes a very short amount of time. <br><br>  The second thing to be seen is the differing call stack for the submit_bio () function and the absence of io_schedule () calls.  Let's take a closer look at the difference inside submit_bio (). <br><br>  Slow “old build” platform: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sd/ba/ew/sdbaewdxmxq2qqy7w6xwlkmpuia.png"></div><br>  Fast platform "fresh": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_a/d2/_w/_ad2_wholhggbeewyuoxvqdpuas.png"></div><br>  It seems that on a slow platform the request goes a long way to the device, at the same time getting into the <b>kyber scheduler</b> .  You can read more about I / O schedulers in <a href="https://blog.selectel.ru/blk-mq-tests/">our article</a> . <br><br>  As soon as <b>kyber</b> was turned off, the same fio test showed an average wait time of about 10 microseconds, right as stated in the specification.  Fine! <br><br>  But where is the difference in one microsecond? <br><br><h3>  And if a little deeper? </h3><br>  As already mentioned, <b>perf</b> allows you to collect statistics from hardware counters.  Let's try to see the number of cash misses and instructions for the cycle: <br><br><pre> <code class="bash hljs">perf <span class="hljs-built_in"><span class="hljs-built_in">stat</span></span> -e cycles,instructions,cache-references,cache-misses,bus-cycles /root/fio/fio --clocksource=cpu --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=10</code> </pre> <br><img src="https://habrastorage.org/webt/ue/pc/fo/uepcfo8up5ehpvqb1ophotqjzb8.png"><br><br><img src="https://habrastorage.org/webt/p5/ln/al/p5lnalg0u05xtvc5792ghegtv34.png"><br><br>  From the results, it can be seen that the fast platform executes more instructions per CPU cycle and has a smaller percentage of cache misses during execution.  Of course, we will not go into the details of the work of different hardware platforms within the framework of this article. <br><br><h2>  Example 2. Grocery </h2><br><h3>  Something is going wrong </h3><br>  In the work of a distributed storage system, an increase in the load on the CPU on one of the hosts was observed with an increase in incoming traffic.  The hosts are equal, equivalent and have identical hardware and software. <br><br>  Consider what the CPU load looks like: <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># pidstat -p 1441734 1 Linux 3.13.0-96-generic (lol) 10/10/2018 _x86_64_ (24 CPU) 09:23:30 PM UID PID %usr %system %guest %CPU CPU Command 09:23:44 PM 0 1441734 23.00 1.00 0.00 24.00 4 ceph-osd 09:23:45 PM 0 1441734 85.00 34.00 0.00 119.00 4 ceph-osd 09:23:46 PM 0 1441734 0.00 130.00 0.00 130.00 4 ceph-osd 09:23:47 PM 0 1441734 121.00 0.00 0.00 121.00 4 ceph-osd 09:23:48 PM 0 1441734 28.00 82.00 0.00 110.00 4 ceph-osd 09:23:49 PM 0 1441734 4.00 13.00 0.00 17.00 4 ceph-osd 09:23:50 PM 0 1441734 1.00 6.00 0.00 7.00 4 ceph-osd</span></span></code> </pre> <br>  The problem arose at 09:23:46 and we see that the process worked in kernel space exclusively for the entire second.  Let's look at what was happening inside. <br><br><h3>  Why so slow? </h3><br>  In this case, we took samples from the entire system: <br><br><pre> <code class="bash hljs">perf record -a -g -- sleep 22 perf script &gt; perf.results</code> </pre> <br>  The <i>-a</i> option is needed here in order for <b>perf to</b> shoot traces from all CPUs. <br><br>  Open <b>perf.results</b> with <b>flamescope</b> to track the moment of increased load on the CPU. <br><br><div class="spoiler">  <b class="spoiler_title">Heat map</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://habrastorage.org/webt/ao/db/hq/aodbhqbwotkcwaq99bvmbznkbrg.png"></div><br></div></div><br>  Before us is a "heat map", both axes (X and Y) of which represent time. <br><br>  On the X axis, the space is divided into seconds, and on the Y axis - into sections of 20 milliseconds within seconds X. Time passes from bottom to top and from left to right.  The brightest squares have the largest number of samples.  That is, the CPU at that time worked most actively. <br><br>  Actually, we are interested in the red spot in the middle.  Select it with the mouse, click and see what it hides: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gv/kk/ko/gvkkkomg9vl7u1ylpwx7h8pceqc.png"></div><br>  In general, it is already clear that the problem lies in the slow operation of <i>tcp_recvmsg</i> and <i>skb_copy_datagram_iovec</i> in it. <br><br>  For clarity, compare with the samples of another host, on which the same amount of incoming traffic does not cause problems: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/v6/_j/72/v6_j72zqvscfolhkaipeoyx9lg8.png"></div><br>  Based on the fact that we have the same amount of incoming traffic, identical platforms that have been operating for a long time without stopping, we can assume that problems arose on the side of the iron.  The <i>skb_copy_datagram_iovec</i> function copies data from the kernel structure to a user-space structure to pass on to the application.  There are probably problems with host memory.  At the same time, there are no errors in the logs. <br><br>  Restart the platform.  When loading BIOS we see the message on a broken lath of memory.  Replacement, the host starts and the problem with the overloaded CPU is no longer playing. <br><br><h2>  P.S </h2><br><h3>  System performance with perf </h3><br>  Generally speaking, on a booted system, <b>perf</b> can delay the processing of requests.  The size of these delays also depends on the load on the server. <br><br>  Let's try to find this delay: <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># /root/fio/fio --clocksource=cpu --name=test --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 fio-3.9-dirty Starting 1 process Jobs: 1 (f=1): [r(1)][100.0%][r=413MiB/s][r=106k IOPS][eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=109786: Wed Dec 12 17:25:56 2018 read: IOPS=106k, BW=414MiB/s (434MB/s)(4096MiB/9903msec) clat (nsec): min=8161, max=84768, avg=9092.68, stdev=1866.73 lat (nsec): min=8195, max=92651, avg=9127.03, stdev=1867.13 … ~# perf record /root/fio/fio --clocksource=cpu --name=test --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 fio-3.9-dirty Starting 1 process Jobs: 1 (f=1): [r(1)][100.0%][r=413MiB/s][r=106k IOPS][eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=109839: Wed Dec 12 17:27:50 2018 read: IOPS=106k, BW=413MiB/s (433MB/s)(4096MiB/9916msec) clat (nsec): min=8259, max=55066, avg=9102.88, stdev=1903.37 lat (nsec): min=8293, max=55096, avg=9135.43, stdev=1904.01</span></span></code> </pre> <br>  The difference is not very noticeable, only about ~ 8 nanoseconds. <br><br>  Let's see what happens if we increase the load: <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># /root/fio/fio --clocksource=cpu --name=test --numjobs=4 --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 ... fio-3.9-dirty Starting 4 processes Jobs: 4 (f=4): [r(4)][100.0%][r=1608MiB/s][r=412k IOPS][eta 00m:00s] ~# perf record /root/fio/fio --clocksource=cpu --name=test --numjobs=4 --bs=4k --filename=/dev/nvme0n1p4 --direct=1 --ioengine=pvsync2 --hipri --rw=randread --filesize=4G --loops=1 test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync2, iodepth=1 ... fio-3.9-dirty Starting 4 processes Jobs: 4 (f=4): [r(4)][100.0%][r=1584MiB/s][r=405k IOPS][eta 00m:00s]</span></span></code> </pre> <br>  Here the difference is already becoming noticeable.  It can be said that the system has slowed down by less than 1%, but essentially a loss of about 7Kiops on a highly loaded system can lead to problems. <br><br>  It is clear that this example is synthetic, however, it is very significant. <br><br>  Let's try to run another synthetic test that calculates prime numbers - <i>sysbench</i> : <br><br><pre> <code class="bash hljs">~<span class="hljs-comment"><span class="hljs-comment"># sysbench --max-time=10 --test=cpu run --num-threads=10 --cpu-max-prime=100000 ... Test execution summary: total time: 10.0140s total number of events: 3540 total time taken by event execution: 100.1248 per-request statistics: min: 28.26ms avg: 28.28ms max: 28.53ms approx. 95 percentile: 28.31ms Threads fairness: events (avg/stddev): 354.0000/0.00 execution time (avg/stddev): 10.0125/0.00 ~# perf record sysbench --max-time=10 --test=cpu run --num-threads=10 --cpu-max-prime=100000 … Test execution summary: total time: 10.0284s total number of events: 3498 total time taken by event execution: 100.2164 per-request statistics: min: 28.53ms avg: 28.65ms max: 28.89ms approx. 95 percentile: 28.67ms Threads fairness: events (avg/stddev): 349.8000/0.40 execution time (avg/stddev): 10.0216/0.01</span></span></code> </pre> <br>  Here you can see that even the minimum processing time increased by 270 microseconds. <br><br><h3>  Instead of conclusion </h3><br>  <b>Perf</b> is a very powerful tool for analyzing performance and debugging system operation.  However, as with any other tool, you need to keep yourself in hand and remember that any loaded system under worse supervision works worse. <br><br>  Related Links: <br><br><ul><li>  <a href="http://www.brendangregg.com/perf.html">Examples of one-liners with perf</a> </li><li>  <a href="https://perf.wiki.kernel.org/index.php/Main_Page">Perf wiki</a> </li></ul></div>