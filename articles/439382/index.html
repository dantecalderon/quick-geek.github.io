<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Using Ansible, Terraform, Docker, Consul, Nomad in the Clouds (Alexey Vakhov, Uchi.ru)</title>
  <meta name="description" content="This article is a transcript of the video report by Alexei Vakhov from Uchi.ru ‚ÄúClouds in the Clouds‚Äù 


 Uchi.ru - an online platform for school educ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>Using Ansible, Terraform, Docker, Consul, Nomad in the Clouds (Alexey Vakhov, Uchi.ru)</h1><div class="post__text post__text-html js-mediator-article"><p>  This article is a transcript of the video report by Alexei Vakhov from Uchi.ru ‚ÄúClouds in the Clouds‚Äù </p><br><p>  Uchi.ru - an online platform for school education, more than 2 million schoolchildren regularly have interactive classes with us.  All our projects are hosted completely in public clouds, 100% of applications run in containers, starting from the smallest, for internal use, and ending with large productions for 1k + requests per second.  It so happened that we have 15 isolated docker clusters (not Kubernetes, sic!) In five cloud providers.  Fifteen hundred user applications, the number of which is constantly growing. </p><br><p>  I will tell very specific things: how we switched to containers, how we managed the infrastructure, problems we encountered, what worked and what didn't. </p><br><p>  During the report, we will discuss: </p><br><ul><li>  Technology motivation and business features </li><li>  Tools: Ansible, Terraform, Docker, Github Flow, Consul, Nomad, Prometheus, Shaman - web-interface for Nomad. </li><li>  Using Cluster Federation to Manage Distributed Infrastructure </li><li>  NoOps rollouts, test environments, application diagrams (almost all developers do their own changes) </li><li>  Entertaining stories from practice </li></ul><br><iframe width="560" height="315" src="https://www.youtube.com/embed/C7utdhh6UCk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Who cares, I ask under the cat. </p><a name="habracut"></a><br><p>  My name is Alexey Vakhov.  I work as a technical director in the company Uchi.ru.  We are hosted in public clouds.  We actively use Terraform, Ansible.  Since then, we have completely switched to Docker.  Very pleased.  How satisfied, how satisfied we are, I will tell. </p><br><p><img src="https://habrastorage.org/webt/l3/zh/6x/l3zh6xrikma1bku1ks6vo7hixva.png"></p><br><p>  Company Uchi.ru engaged in the production of products for school education.  We have a main platform on which children solve interactive tasks in various subjects in Russia, in Brazil, in the USA.  We hold online competitions, contests, clubs, camps.  Every year this activity grows. </p><br><p><img src="https://habrastorage.org/webt/ms/m8/8h/msm88h-eoc67vkjnokat3ssq_wo.png"></p><br><p>  From the engineering point of view, the classic web stack (Ruby, Python, NodeJS, Nginx, Redis, ELK, PostgreSQL).  The main feature that many applications.  Applications are hosted worldwide.  Every day there are rollouts in production. </p><br><p>  The second feature is that our schemes change very often.  Ask to raise a new application, stop the old one, add cron for background jobs.  Every 2 weeks there is a new Olympiad - this is a new application.  All this needs to be accompanied, monitored, backed up.  Therefore, the environment is superdynamic.  Dynamism is our main difficulty. </p><br><p><img src="https://habrastorage.org/webt/gm/dn/vu/gmdnvuag9g19b-bkzqbamcrwdw8.png"></p><br><p>  Our work unit is a playground.  In terms of cloud providers, this is Project.  Our site is a completely isolated entity with an API and private subnet.  When we enter the country, we look for local cloud providers.  Not everywhere is Google and Amazon.  Sometimes there are no API to the cloud provider.  Outside we publish VPN and HTTP, HTTPS on balancers.  All other services communicate within the cloud. </p><br><p><img src="https://habrastorage.org/webt/vz/b6/w2/vzb6w2yehx_arktffwsjxildnhe.png"></p><br><p>  Under each site we have created our own Ansible repository.  In the repository there are hosts.yml, playbook, roles and 3 secret folders, about which I will continue to talk about.  This is a terraform, provision, routing.  We are fans of standardization.  Our repository should always be called "site ansible-name."  We standardize each file name, internal structure.  This is very important for further automation. </p><br><p><img src="https://habrastorage.org/webt/v3/a0/kk/v3a0kke8aauqojk4hj5blhgwmt4.png"></p><br><p>  Terraform a year and a half ago, set up, so we use it.  Terraform without modules, without file structure (flat structure is used).  Terraform file structure: 1 server - 1 file, network configuration and other settings.  Using terraform, we describe servers, disks, domains, s3-buckets, networks, and so on.  Terraform at the site fully prepares the iron. </p><br><p><img src="https://habrastorage.org/webt/wq/qz/ly/wqqzly1s4_6brivoh4hkeyvexmu.png"></p><br><p>  Terraform creates a server, then ansibl rolls these servers.  Due to the fact that we use the same version of the operating system everywhere, we all wrote the roles from scratch.  Ansible roles for all operating systems that do not work anywhere are usually published on the Internet.  We all took Ansible roles and left only what we need.  Standardized Ansible Roles.  We have 6 basic playbooks.  When you run Ansible sets the standard list of software: OpenVPN, PostgreSQL, Nginx, Docker.  Kubernetes we do not use. </p><br><p><img src="https://habrastorage.org/webt/fh/x4/i4/fhx4i4ztn5fvmjj6uivdbkdrub8.png"></p><br><p> We use Consul + Nomad.  These are very simple programs.  Run 2 programs written in Golang on each server.  Consul is responsible for Service Discovery, health check, and key-value configuration storage.  Nomad is responsible for scheduling, for rolling out.  Nomad launches containers, provides rollouts, including rolling-update on health check, allows you to run sidecar-containers.  Cluster is easy to expand or vice versa.  Nomad supports distributed cron. </p><br><p><img src="https://habrastorage.org/webt/dc/ic/sm/dcicsmnhzdduqbnxsdpe6ewkvws.png"></p><br><p>  After we enter the site, Ansible executes a playbook located in the provision directory.  A playbook in this directory is responsible for installing the software in the docker cluster that administrators use.  Installed prometheus, grafana and secret software shaman. </p><br><p>  Shaman is a web-dashboard for nomad.  Nomad is low-level and I don‚Äôt really want to let developers to it.  In shaman, we see a list of applications, developers issue a button for application deployment.  Developers can change configurations: add containers, environment variables, start services. </p><br><p><img src="https://habrastorage.org/webt/cm/jw/ry/cmjwryicd-9dvhlpg--bhfrfe9w.png"></p><br><p>  Finally, the final component of the site is routing.  Routing is stored in our K / V storehouse of the consul, that is, there is a bunch between upstream, service, url and so on.  On each balancer, the Consul template rotates, which generates the nginx config and reloads it.  Very reliable thing, we never had a problem with it.  The chip of such a scheme is that the traffic accepts the standard nginx and you can always see which config was generated and work as with the standard nginx. </p><br><p><img src="https://habrastorage.org/webt/rn/ew/7g/rnew7gm_fe4gcc57clp3f41ob68.png"></p><br><p>  Thus, each site consists of 5 layers.  With the help of a terraform, we set up the hardware.  Ansible we perform basic server configuration, set up a docker cluster.  Provision rolls system software.  Routing directs traffic inside the site.  Applications contains user applications and administrator applications. </p><br><p>  We have been debugging these layers for quite a while so that they are as identical as possible.  Provision routing match 100% between sites.  Therefore, for developers, each site is exactly the same. </p><br><p>  If IT professionals switch from project to project, then they end up in a completely typical environment.  In ansible we could not make identical the settings of the firewall, VPN for different cloud providers.  With the network, all cloud providers work differently.  Terraform is different, because it contains specific constructions for each cloud provider. </p><br><p><img src="https://habrastorage.org/webt/36/kg/ti/36kgtipfv8rl9lhjevdjhhiwp5m.png"></p><br><p>  We have 14 production sites.  The question arises: how to manage them?  We made the 15th master platform, in which we let only admins.  It works according to the federation scheme. </p><br><p>  The idea was taken from prometheus.  In prometheus there is a mode when in each site we install prometheus.  Prometheus publish outside through HTTPS basic auth authorization.  Prometheus master takes only the necessary metrics c removed prometheus.  This makes it possible to compare application metrics in different clouds, to find the most downloaded or unloaded applications.  Centralized notification (alerting) goes through the prometheus master for admins.  Developers receive alerts from local prometheus. </p><br><p><img src="https://habrastorage.org/webt/cw/c9/yd/cwc9yd3hpmbhbxvlz0ygacktloa.png"></p><br><p>  The same scheme is configured shaman.  Administrators can deploy through the main site, configure it on any site through a single interface.  A sufficiently large class of problems is solved without leaving this master site. </p><br><p><img src="https://habrastorage.org/webt/kz/ul/hi/kzulhi1jrz8c3bssubwqzremd6g.png"></p><br><p>  I'll tell you how we switched to docker.  This process is very slow.  We moved about 10 months.  In the summer of 2017, we had 0 containers of production.  In April 2018, we have documented and rolled out our latest application in production. </p><br><p><img src="https://habrastorage.org/webt/h5/9t/ib/h59tibngi0uru-f4vehjhnq3e1c.png"></p><br><p>  We are from the world of ruby ‚Äã‚Äãon rails.  There used to be 99% of Ruby on Rails applications.  Rails rolls out through Capistrano.  Technically, Capistrano works in the following way: the developer starts cap deploy, capistrano enters all application servers via ssh, picks up the latest version of the code, collects asset, migrates the database.  Capistrano simulates a new version of the code and sends a USR2 signal to the web application.  On this signal, the web server picks up a new code. </p><br><p><img src="https://habrastorage.org/webt/il/us/4e/ilus4ejwfkd6cvjnmjrcln2ugyy.png"></p><br><p>  The last step in the docker is not done that way.  In the docker you need to stop the old container, raise the new container.  Then the question arises: how to switch traffic?  In the cloudy world, service discovery is responsible for this. </p><br><p><img src="https://habrastorage.org/webt/k3/il/ig/k3iligqv5uphvlp1l3cvkmbggem.png"></p><br><p>  Therefore, we added consul to each site.  Consul was added because they used Terraform.  We wrapped all the nginx configs in the consul template.  Formally, the same thing, but we were already ready to dynamically control the traffic inside the sites. </p><br><p><img src="https://habrastorage.org/webt/u9/qg/ag/u9qgagdnxiwo4lowkqi7micapqu.png"></p><br><p>  Next, we wrote a ruby ‚Äã‚Äãscript that collected an image on one of the servers, pushed it into the registry, then went in ssh to each server, picked up new ones and stopped the old containers, registering them with the consul.  The developers also continued to launch cap deployments, but the services already worked in docker. </p><br><p>  I remember that there were two versions of the script, the second one turned out to be quite advanced, there was a rolling update when a small number of containers stopped, new ones were raised, the consul helfchieks waited and moved on. </p><br><p><img src="https://habrastorage.org/webt/07/fx/gk/07fxgkf2fcvnhs8gpfjfrygalxs.png"></p><br><p>  Then they realized that this is a dead end way.  The script has increased to 600 lines.  The next step is manual scheduling, we replaced Nomad.  Hiding from the developer details of the work.  That is, they still called cap deploy, but inside was a completely different technology. </p><br><p><img src="https://habrastorage.org/webt/4j/o-/ms/4jo-ms4v5mx1q4cn6sqt6n0jr4a.png"></p><br><p>  And in the end, we moved the deployment to the UI and took away access to the servers, leaving the green deployment button and management interface. </p><br><p>  In principle, such a transition was certainly a long one, but we avoided a problem that I met quite a few times. </p><br><p>  There is some kind of legacy stack, system or something like that.  Khachinnaya already just in rags.  The development of a new version begins.  After a couple of months or a couple of years, depending on the size of the company, in the new version less than half of the required functionality was implemented, and the old version still ran into prered.  And that new one also became very legacy.  And it's time to start a new, third version from scratch.  In general, this is an endless process. </p><br><p>  Therefore, we always move the entire stack.  In small steps, crooked, with crutches, but entirely.  We can not update for example the docker engine on one site.  It is necessary to update everywhere, since there is a desire. </p><br><p><img src="https://habrastorage.org/webt/qy/31/jr/qy31jrpbehnyyrcozqaxnj41jk8.png"></p><br><p>  Roll out.  All docker instructions are rolling out 10 nginx containers or 10 redis containers into docker.  This is a bad example, because the images have already been collected, the images are easy.  We packed our rails applications into docker.  The size of docker images was 2-3 gigabytes.  They do not roll out so quickly. </p><br><p><img src="https://habrastorage.org/webt/2q/lx/bf/2qlxbfo5lnhpjfxcjtlljrntgxy.png"></p><br><p>  The second problem came from the hipster web.  Hipster web is always Github Flow.  In 2011, there was a landmark post that Github Flow rules, so the whole web rolls.  What does this look like?  The master branch is always production.  When adding a new functionality, we make a branch.  When doing a merge, we do code-review, we run tests, raises the staging environment.  Business looks staging environment.  At time X, if everything is successful, then we merge the branch to master and roll it out into production. </p><br><p>  It worked perfectly on capistrano, because it was created for that.  Docker always sells us pipeline.  Collected container.  The container can be transferred to the developer, tester, transferred to production.  But at the moment of merge in the master code is different.  All docker images that were collected from the feature-branch are not collected from the master. </p><br><p><img src="https://habrastorage.org/webt/vl/th/8g/vlth8gjbcyby-rpamtltaijqxcy.png"></p><br><p>  How are we done?  We collect image, we put it in local docker registry.  And after that we do the rest of the operations: migrations, deploy in production. </p><br><p><img src="https://habrastorage.org/webt/1i/kb/yu/1ikbyuh88pytsfprkkwdi6_6ppq.png"></p><br><p>  To quickly assemble this image, we use Docker-in-Docker.  On the Internet, everyone writes that this is an anti-pattern, it kreshitsya.  We had nothing like this.  How many already working with him never had any problems.  We forward the / var / lib / docker directory to the main server using Persistent volume.  All intermediate images are on the main server.  Build a new image within a few minutes. </p><br><p><img src="https://habrastorage.org/webt/6q/_g/67/6q_g67tuzx_n078pff6-aisdvdc.png"></p><br><p>  For each application we make a local internal docker registry and our own build volume.  Because docker saves all the layers on the disk and is difficult to clean.  Now we know the disk utilization of every local docker registry.  We know how much disk it requires.  You can receive alerts through a centralized Grafana and clean.  While we clean their hands.  But we will automate it. </p><br><p><img src="https://habrastorage.org/webt/mf/py/wj/mfpywjgwpsj0tbqb6vvvyzz8_1u.png"></p><br><p>  One more thing.  Docker-image collected.  Now this image needs to be decomposed into servers.  When copying a large docker image, the network does not cope.  In the cloud we have 1 Gbit / s.  A global plug-in occurs in the cloud.  Now we deploy a docker image on 4 heavy production servers.  The graph shows the disk worked on 1 pack of servers.  Then the second pack of servers is deployed.  Bottom can be seen recycling channel.  Approximately 1 Gbit / s we almost pull out.  More there is not particularly particularly accelerated. </p><br><p><img src="https://habrastorage.org/webt/lk/eo/hl/lkeohl4fjm-y97kmp3hyogr3p1a.png"></p><br><p>  My favorite production is South Africa.  There is a very expensive and slow iron.  Four times more expensive than in Russia.  There is very bad internet.  Internet modem level, but not buggy.  There we roll out applications in 40 minutes, taking into account tyunig caches, timeout settings. </p><br><p><img src="https://habrastorage.org/webt/zl/8x/ig/zl8xig0k9liveeiuzoykh-ctgwm.png"></p><br><p>  The last problem that worried me before the docker contacted was the load.  In fact, the load is the same as without a docker with an identical iron.  The only nuance we rested just one point.  If from the Docker engine to collect logs via the built-in fluentd driver, then at a load of about 1000 rps the internal buffer fluentd started to litter and requests begin to slow down.  We carried out logging in sidecar containers.  In nomad, this is called log-shipper.  Near the large container of the application hangs a small container.  The only task for him to take logs and send it to a centralized repository. </p><br><p><img src="https://habrastorage.org/webt/8r/fv/7x/8rfv7xzfamwjrwemisa1laingzy.png"></p><br><p>  What were the problems / solutions / challenges.  I tried to analyze what the task was.  The features of our problems are: </p><br><ul><li>  many independent applications </li><li>  constant changes to the infrastructure map </li><li>  Github flow and large docker images </li></ul><br><p><img src="https://habrastorage.org/webt/da/xm/jm/daxmjmflat8cz4a5b-2nmx4pip4.png"></p><br><p>  Our solutions </p><br><ul><li>  Federation of docker clusters.  In terms of handling hard.  But docker is good in terms of rolling out business functionality in production.  We work with personal data and we have certification in each country.  In an isolated area, such certification is easy to pass.  During certification, all questions arise: where are you hosted, how is your cloud provider, where do you store personal data, where do you back up, who has access to the data.  When everything is isolated, the circle of suspects is much easier to describe and follow all this much easier. </li><li>  Orchestration.  It is clear that kubernetes.  He is everywhere.  But I want to say that Consul + Nomad is quite a production solution. </li><li>  Build images.  Quickly collect images in Docker-in-Docker. </li><li>  When using Docker, it is also possible to hold a load of 1000 rps. </li></ul><br><p>  Development direction vector </p><br><p>  Now one of the big problems is the desynchronization of software versions on the platforms.  Previously, we set up the server by hand.  Then we became devops engineers.  Now configure the server using ansible.  Now we have total unification, standardization.  Introduce the usual thinking in the head.  We can not fix PostgreSQL hands on the server.  If you need some kind of fine tuning on exactly 1 server, then we think how to extend this setting everywhere.  If not standardized, then there will be a zoo settings. </p><br><p>  I am delighted and very glad that we get out of the box for free a really very pleasant infrastructure. </p><br><p><img src="https://habrastorage.org/webt/w6/nz/pw/w6nzpwpzt1tsxifw0gksrvoplpg.png"></p><br><p>  You can add me on facebook.  If we do something new good, I'll write about it. </p><br><p>  Questions: </p><br><p>  What is the advantage of Consul Template over Ansible Template, for example, for setting firewall rules and other things? </p><br><p>  Answer: Now we have traffic from external balancers going straight to the containers.  There is no one intermediate layer.  A config is created there that forwards the IP addresses and ports of the cluster.  We also have all the balancing settings in K / V in Consul.  We have an idea to give routing settings to developers through a secure interface so that they don‚Äôt break anything. </p><br><p>  Question: Regarding the homogeneity of all sites.  Are there really no requests from business or from developers that need to roll out something non-standard on this site?  For example, tarantool with cassandra. </p><br><p>  Answer: It happens, but it is a very big rarity.  This we make out an internal separate artifact.  There is such a problem, but it is rare. </p><br><p>  Question: The solution to the delivery problem is to use the private docker registry in each site and from there you can quickly get docker images. </p><br><p>  Answer: All the same, the deployment will rest on the network, since we decompose the docker image on there 15 servers at the same time.  We run into the net.  Inside the network 1 Gbit / s. </p><br><p>  Question: Are such a huge number of docker containers based on roughly the same technology stack? </p><br><p>  Answer: Ruby, Python, NodeJS. </p><br><p>  Question: How often do you test or check your docker images for updates?  How do you solve update problems, for example, when glibc, openssl needs to be fixed in all docker? </p><br><p>  Answer: If you find such an error, vulnerability, then we sit down for a week and fix it.  If you need to roll out, then we can roll out the whole cloud (all applications) from scratch through the federation.  We can click through all the green buttons for application deployment and go away to drink tea. </p><br><p>  Question: Are you going to release your shaman in opensource? </p><br><p>  Answer: Andrew here (pointing at a person from the audience) promises us to lay out a shaman in the fall.  But there you need to add support for kubernetes.  Opensource should always be better. </p></div><p>Source: <a href="https://habr.com/ru/post/439382/">https://habr.com/ru/post/439382/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
</ul></nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>