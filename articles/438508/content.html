<div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/068/8dc/f51/0688dcf5125ee463586f044f255d3b29.png"><br>  <i><font color="gray">Diagram of speech reconstruction method.</font></i>  <i><font color="gray">A person listens to words, as a result, neurons of his auditory cortex are activated.</font></i>  <i><font color="gray">The data is interpreted in four ways: a combination of two types of regression models and two types of speech representations, then goes to the neural network system to extract features, which are subsequently used to adjust vocoder parameters</font></i> <br><br>  Neuroengineers at Columbia University (USA) were the first in the world to <a href="https://zuckermaninstitute.columbia.edu/columbia-engineers-translate-brain-signals-directly-speech">create a system</a> that translates a person’s thoughts into understandable, distinguishable speech, that’s the <a href="">sound recording of words</a> (mp3) synthesized from brain activity. <br><br>  Watching the activity in the auditory cortex, the system recovers, with unprecedented clarity, the words a person hears.  Of course, this is not the articulation of thoughts in the literal sense of the word, but an important step has been taken in this direction.  After all, similar patterns of brain activity occur in the cerebral cortex when a person imagines that he is listening to a speech, or when he mentally speaks words. <br><a name="habracut"></a><br>  This scientific breakthrough using artificial intelligence technologies brings us closer to creating effective neural interfaces that connect computers directly to the brain.  It will also help to communicate to people who can not speak, as well as those who recover from a stroke or for some other reason temporarily or permanently unable to pronounce words. <br><br>  Decades of research have shown that in the process of speech or even mental speaking of words, control models of activity appear in the brain.  In addition, a distinct (and recognizable) pattern of signals occurs when we listen to someone or imagine that we are listening.  Experts have long been trying to record and decipher these patterns in order to “free” the thoughts of a person from the skull - and automatically translate them into oral form. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5af/31e/a95/5af31ea959b040edf0aad7f261d5df22.png"><br>  <i><font color="gray">(A) The top shows the original spectrogram of the speech pattern.</font></i>  <i><font color="gray">Below are the reconstructed auditory spectrograms of four models.</font></i>  <i><font color="gray">(B) Magnetic power of frequency bands during unvoiced (t = 1.4 s) and voiced speech (t = 1.15 s: the gap is shown by dashed lines for the original spectrogram and four reconstructions)</font></i> <br><br>  “This is the same technology that Amazon Echo and Apple Siri use to verbally answer our questions,” <a href="https://zuckermaninstitute.columbia.edu/columbia-engineers-translate-brain-signals-directly-speech">explains</a> Dr. Nima Mesgarani, lead author of the research.  To teach the vocoder to interpret brain activity, specialists found five patients with epilepsy who had already undergone brain surgery.  They were asked to listen to suggestions made by different people, while electrodes measured brain activity, which was processed by four models.  These neural patterns taught vocoder.  Then the researchers asked the same patients to listen to the dynamics of the numbers from 0 to 9, recording brain signals that could be passed through the vocoder.  The sound produced by the vocoder in response to these signals is analyzed and cleaned by several neural networks. <br><br>  As a result of processing at the output of the neural network, a robot voice was received, pronouncing a sequence of numbers.  To test the accuracy of recognition, people were given to listen to sounds synthesized from their own brain activity: “We found that people can understand and repeat sounds in 75% of cases, which is much higher than any previous attempt,” said Dr. Mesgarani. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/091/8fe/670/0918fe670199f931b07d443033b00014.png"><br>  <i><font color="gray">Objective evaluations for different models.</font></i>  <i><font color="gray">(A) The average score for the standard <a href="https://www.researchgate.net/publication/306046797_An_Algorithm_for_Predicting_the_Intelligibility_of_Speech_Masked_by_Modulated_Noise_Maskers">ESTOI score</a> for all subjects for four models.</font></i>  <i><font color="gray">B) The coverage and location of the electrodes and the ESTOI score for each of the five people.</font></i>  <i><font color="gray">Everyone has an ESTOI vocoder rating of DNN higher than other models.</font></i> <br><br>  Now scientists are planning to repeat the experiment with more complex words and sentences.  In addition, the same tests will run for brain signals when a person imagines what he is saying.  Ultimately, they hope the system will become part of an implant that translates the wearer's thoughts directly into words. <br><br>  The scientific article was <a href="https://www.nature.com/articles/s41598-018-37359-z">published</a> on January 29, 2019 in open access in the journal <i>Scientific Reports</i> (doi: 10.1038 / s41598-018-37359-z). <br><br>  The code for conducting phoneme analysis, calculating high-frequency amplitudes and restoring the auditory spectrogram <a href="http://naplab.ee.columbia.edu/naplib.html">is made publicly available</a> . </div>