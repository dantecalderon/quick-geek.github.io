<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural networks and language philosophy</title>
  <meta name="description" content="Why Wittgenstein theories remain the basis of all modern NLP 

 The vector representation of words is perhaps one of the most beautiful and romantic i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-6974184241884155",
      enable_page_level_ads: true
    });
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>Neural networks and language philosophy</h1><div class="post__text post__text-html js-mediator-article">  <font color="gray">Why Wittgenstein theories remain the basis of all modern NLP</font> <br><br>  The vector representation of words is perhaps one of the most beautiful and romantic ideas in the history of artificial intelligence.  The philosophy of language is a section of philosophy that explores the connection between language and reality and how to make speech meaningful and understandable.  And the vector representation of words is a very specific method in modern natural language processing (NLP).  In a sense, it is an empirical proof of the theories of Ludwig Wittgenstein, one of the most relevant philosophers of the last century.  For Wittgenstein, the use of words is a move in a social language <i>game</i> played by community members who understand each other.  The meaning of a word depends only on its usefulness in context; it does not relate one-to-one with an object from the real world. <br><br><blockquote>  For a large class of cases in which we use the word "value", it can be defined as the <b>meaning of a word is its use in language</b> . </blockquote><a name="habracut"></a><br>  Of course, to understand the exact meaning of the word is very difficult.  There are many aspects to consider: <br><br><ul><li>  to which object the word may belong; </li><li>  what is the part of speech; </li><li>  is it an idiomatic expression; </li><li>  all shades of meanings; </li><li>  and so on. </li></ul><br>  All these aspects, in the end, boil down to one thing: to know how to use the word. <br><br>  The concept of <i>meaning</i> and why an ordered set of characters has a certain connotation in the language is not only a philosophical question, but also probably the biggest problem that AI specialists working with NLP face.  It is quite obvious to the Russian-speaking person that the ‚Äúdog‚Äù is an ‚Äúanimal‚Äù and it looks more like a ‚Äúcat‚Äù than a ‚Äúdolphin‚Äù, but this task is far from simple for a systematic solution. <br><br>  By slightly correcting Wittgenstein's theories, we can say that dogs are like cats, because they often appear in the same contexts: dogs and cats are more likely to be associated with the words ‚Äúhouse‚Äù and ‚Äúgarden‚Äù than with the words ‚Äúsea‚Äù and the "ocean".  It is this intuition that underlies <b>Word2Vec</b> , one of the most famous and successful implementations of the vector representation of words.  Today, machines are far from a true <i>understanding of</i> long texts and passages, but the vectorial representation of words is undoubtedly the only method that allowed us to make the biggest step in this direction over the past decade. <br><br><h1>  From BoW to Word2Vec </h1><br>  In many computer problems, the first problem is to present the data in numerical form;  words and sentences are probably the hardest to imagine in this form.  In our setting, <b>D</b> words are selected from the dictionary, and each word can be assigned a numeric index <b>i</b> . <br><br>  For many decades, the classical approach has been adopted to represent each word as a numerical D-dimensional vector of all zeros, except for the one in position i.  As an example, consider a dictionary of three words: "dog", "cat" and "dolphin" (D = 3).  Each word can be represented as a three-dimensional vector: ‚Äúdog‚Äù corresponds to [1,0,0], ‚Äúcat‚Äù - [0,1,0], and ‚Äúdolphin‚Äù, obviously, [0,0,1].  The document can be represented as a D-dimensional vector, where each element counts occurrences of the i-th word in the document.  This model is called Bag-of-words (BoW), and it has been used for decades. <br><br>  Despite its success in the 90s, BoW lacked the only interesting function of words: their meaning.  We know that two very different words can have similar meanings, even if they are completely different from a spelling point of view.  ‚ÄúCat‚Äù and ‚Äúdog‚Äù are both domestic animals, ‚Äúking‚Äù and ‚Äúqueen‚Äù are close to each other, ‚Äúapple‚Äù and ‚Äúcigarette‚Äù are completely unrelated.  We <i>know</i> this, but in the BoW model, all these words are at the same distance in the vector space: 1. <br><br>  The same problem applies to documents: using BoW, one can conclude that documents are similar only if they contain the same word a certain number of times.  And here comes Word2Vec, introducing many philosophical questions into machine learning terms that Wittgenstein talked about in his <i>Philosophical Studies</i> 60 years ago. <br><br>  In a dictionary of size D, where a word is identified by its index, the goal is to calculate the N-dimensional vector representation of each word with N &lt;&lt; D.  Ideally, we want it to be a dense vector representing some semantically specific aspects of meaning.  For example, we ideally want the ‚Äúdog‚Äù and ‚Äúcat‚Äù to have similar ideas, and the ‚Äúapple‚Äù and ‚Äúcigarette‚Äù are very distant in the vector space. <br><br>  We want to perform some basic algebraic operations on vectors, such as <code>–∫–æ—Ä–æ–ª—å+–∂–µ–Ω—â–∏–Ω–∞‚àí–º—É–∂—á–∏–Ω–∞=–∫–æ—Ä–æ–ª–µ–≤–∞</code> .  It would be desirable that the distance between the vectors "actor" and "actress" would largely coincide with the distance between the "prince" and "princess".  Despite the fact that these results are quite utopian, experiments show that Word2Vec vectors exhibit properties that are very close to these. <br><br>  Word2Vec is not directly trained by this presentation, but receives it as a side effect of the classification without a teacher.  The average NLP corpus dataset consists of a set of sentences;  each word from the sentence appears in the context of the surrounding words.  The purpose of the classifier is to predict the target word, taking context words as input.  For the sentence ‚Äúbrown dog plays in the garden,‚Äù the words [brown, plays, in, garden] are provided as input data, and she must predict the word ‚Äúdog‚Äù.  This task is considered as learning without a teacher, since the corpus does not need to be marked with an external source of truth: with a set of sentences, you can always automatically create positive and negative examples.  Considering the ‚Äúbrown dog playing in the garden‚Äù as a positive example, we can create many negative patterns, such as ‚Äúbrown plane playing in the garden‚Äù or ‚Äúbrown grape playing in the garden‚Äù, replacing the target word ‚Äúdog‚Äù with random words from the data set. <br><br>  And now the application of Wittgenstein's theories is quite clear: the context is crucial for the vector representation of words, since in his theories it is important to attach meaning to the word.  If two words have similar meanings, they will have similar representations (a small distance in N-dimensional space) only because they often appear in similar contexts.  Thus, the "cat" and "dog" will eventually have close vectors, because they often appear in the same contexts: it is useful for the model to use similar vector representations for them, because this is the most convenient thing that it can do, to get better results in predicting two words based on their contexts. <br><br>  The original article suggests two different architectures: CBOW and Skip-gram.  In both cases, verbal representations are trained along with the specific classification task, providing the best possible vector representations of words that maximize model performance. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b9/e32/ee7/2b9e32ee767ad0c402e214a566d848a0.png"><br>  <i><font color="gray">Figure 1. Comparison of CBOW and Skip-gram architectures</font></i> <br><br>  <b>CBOW</b> stands for Continuous Bag of Words, and its task is to guess the word taking context into account as input.  Inputs and outputs are represented as D-dimensional vectors, which are projected in N-dimensional space with common weights.  We are only looking for weights to project.  In essence, the vector representation of words is a D √ó N matrix, where each row represents a dictionary word.  All context words are projected into one position, and their vector representations are averaged;  therefore, word order does not affect the result. <br><br>  <b>Skip-gram</b> does the same thing, but vice versa: it tries to predict the context words <b>C</b> , taking the target word as input.  The task of predicting several context words can be reformulated into a set of independent binary classification problems, and now the goal is to predict the presence (or absence) of context words. <br><br>  As a rule, Skip-gram takes more time to learn and often gives slightly better results, but, as usual, different applications have different requirements and it is difficult to predict in advance which of them will show the best result.  Despite the simplicity of the concept, learning architectures of this kind is a real nightmare due to the amount of data and processing power required to optimize the scales.  Fortunately, on the Internet, you can find some pre-trained vector representations of words, and you can study vector space ‚Äî the most interesting ‚Äî with just a few lines of Python code. <br><br><h1>  Possible improvements: GloVe and fastText </h1><br>  Above the classic Word2Vec in recent years, many possible improvements have been proposed.  The two most interesting and frequently used ones are GloVe (Stanford University) and fastText (developed by Facebook).  They are trying to identify and overcome the limitations of the original algorithm. <br><br>  In the <a href="https://nlp.stanford.edu/pubs/glove.pdf">original scientific article, the</a> authors of GloVe emphasize that the training model in a separate local context makes poor use of global statistics of the corpus.  The first step to overcome this limitation is to create a global matrix <b>X</b> , where each element <b>i, j</b> counts the number of references to the word <b>j</b> in the context of the word <b>i</b> .  The second important idea of ‚Äã‚Äãthis document is the understanding that only probabilities are not enough to reliably predict values, but a co-occurrence matrix is ‚Äã‚Äãalso required, from where you can directly extract certain aspects of values. <br><br><blockquote>  Consider two words i and j, which are of particular interest.  For concreteness, we assume that we are interested in the concept of a thermodynamic state, for which we can take <code>i = –ª—ë–¥</code> and <code>j = –ø–∞—Ä</code> .  The connection of these words can be investigated by studying the ratio of their probabilities of joint occurrence with the help of different probing words, k.  For words k associated with ice, but not steam, say <code>k = —Ç–µ–ª–æ</code> [solid body, state of matter], we expect the ratio Pik / Pjk to be greater.  Similarly, for words k associated with steam, but not with ice, say <code>k = –≥–∞–∑</code> , the ratio should be small.  For words like ‚Äúwater‚Äù or ‚Äúfashion,‚Äù which are either equally related to ice and steam, or are not related to them, this relationship should be close to one. </blockquote><br>  This probability ratio becomes the starting point for studying the vector representation of words.  We want to be able to calculate vectors that, in combination with a specific function <b>F,</b> keep this relation constant in the space of a vector representation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4dc/d5d/137/4dcd5d13763ca26ad997565ec2e6e513.jpg"></div><br>  <i><font color="gray">Figure 2. The most common formula for the vector representation of words in the GloVe model</font></i> <br><br>  The function F and the dependence on the word k can be simplified by replacing the exponents and fixed displacements, which results in the minimization of errors by the least squares method <b>J</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e48/049/738/e48049738a71657b998f5630dac792c4.jpg"></div><br>  <i><font color="gray">Figure 3. The final function to calculate the vector representation of words in the GloVe model</font></i> <br><br>  The <b>f</b> function is a counting function that tries to not overburden very frequent and rare matches, while <b>bi</b> and <b>bj</b> represent offsets to restore the symmetry of the function.  In the last paragraphs of the article, it is shown that learning this model is not much different from learning the classic Skip-gram model, although in the empirical tests GloVe surpasses both Word2Vec implementations. <br><br>  On the other hand, <b>fastText</b> corrects a completely different drawback of Word2Vec: if the model begins to be taught by directly encoding a single D-dimensional vector, then the internal structure of the words is ignored.  Instead of directly encoding the encoding of words that study verbal representations, fastText suggests studying the N-grams of characters and representing words as the sum of the N-gram vectors.  For example, when N = 3, the word ‚Äúflower‚Äù is encoded as 6 different 3-grams [&lt;fl, flo, low, Debt, wer, er&gt;] plus a special sequence &lt;flower&gt;.  Notice how the angle brackets are used to mark the beginning and end of the word.  Thus, a word is represented by its index in a word dictionary and a set of N-grams, which it contains, mapped to integers using the hashing function.  This simple improvement allows you to split N-gram representations between words and calculate vector representations of words that were not in the corpus. <br><br><h1>  Experiments and possible applications </h1><br>  As we already said, to <b>use</b> these vector views you need only a few lines of Python code.  I conducted several experiments with the <a href="">50-dimensional GloVe model</a> , trained on 6 billion words from Wikipedia's sentences, and also with the <a href="https://fasttext.cc/docs/en/english-vectors.html">300-dimensional fastText model, trained on Common Crawl</a> (which gave 600 billion tokens).  This paragraph contains references to the results of both experiments only to prove the concepts and give a general understanding of the topic. <br><br>  First of all, I wanted to check out some basic similarities of words, the simplest, but important feature of their vector representation.  As expected, the most similar words with the word "dog" were "cat" (0.92), "dogs" (0.85), "horse" (0.79), "puppy" (0.78) and "pet" (0.77).  Note that the plural form has almost the same meaning as the singular.  Again, for us it is rather trivial to say so, but for a car it is not at all a fact.  Now food: the most similar words for "pizza" are "sandwich" (0.87), "sandwiches" (0.86), "snack" (0.81), "pastries" (0.79), "fries" (0.79) and "burgers" ( 0.78).  It makes sense, the results are satisfactory, and the model behaves quite well. <br><br>  The next step is to perform some basic calculations in the vector space and check how well the model has learned some important properties.  Indeed, as a result of calculating the vectors of a <code>–∂–µ–Ω—â–∏–Ω–∞+–∞–∫—Ç–µ—Ä-–º—É–∂—á–∏–Ω–∞</code> , the result is the ‚Äúactress‚Äù (0.94), and as a result of the calculation of a <code>–º—É–∂—á–∏–Ω–∞+–∫–æ—Ä–æ–ª–µ–≤–∞-–∂–µ–Ω—â–∏–Ω–∞</code> - the word ‚Äúking‚Äù (0.86).  Generally speaking, if the value of <code>a:b=c:d</code> , the word <b>d</b> should be obtained as <code>d=b-a+c</code> .  Moving on to the next level, it is impossible to imagine how these vector operations even describe geographic aspects: we know that Rome is the capital of Italy, since Berlin is the capital of Germany, in fact, <code>–ë–µ—Ä–ª–∏–Ω+–ò—Ç–∞–ª–∏—è-–†–∏–º=–ì–µ—Ä–º–∞–Ω–∏—è (0.88)</code> , and <code>–õ–æ–Ω–¥–æ–Ω+–ì–µ—Ä–º–∞–Ω–∏—è-–ê–Ω–≥–ª–∏—è=–ë–µ—Ä–ª–∏–Ω (0.83)</code> . <br><br>  And now the fun part.  Following the same idea, we will try to add and subtract concepts.  For example, What is the American equivalent of pizza for Italians?  <code>–ø–∏—Ü—Ü–∞+–ê–º–µ—Ä–∏–∫–∞-–ò—Ç–∞–ª–∏—è=–±—É—Ä–≥–µ—Ä—ã (0.60)</code> , then <code>—á–∏–∑–±—É—Ä–≥–µ—Ä—ã (0.59)</code> .  Since I moved to Holland, I always say that this country is a mixture of three things: some American capitalism, Swedish cold and quality of life, and finally a pinch of Neapolitan <i>abundance</i> .  By slightly changing the original theorem, removing a bit of Swiss accuracy, we get Holland (0.68) as a result of the <code>–°–®–ê+–®–≤–µ—Ü–∏—è+–ù–µ–∞–ø–æ–ª—å-–®–≤–µ–π—Ü–∞—Ä–∏—è</code> : quite impressive, to be honest. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fd8/f76/e41/fd8f76e41d78598394d1613652f44f26.png"><br>  <i><font color="gray">Figure 4. To all Dutch readers: accept it as a compliment, okay?</font></i> <br><br>  To use these pre-trained vector representations, good practical resources can be found <a href="http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/">here</a> and <a href="http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/">here</a> .  <b>Gensim</b> is a simple and complete Python library with some ready-to-use algebraic and similarity functions.  These pre-trained vector representations can be used in various (and useful) ways, for example, to improve the performance of mood analyzers or language models.  Whatever the task, the use of N-dimensional vectors will significantly improve the efficiency of the model compared to direct encoding.  Of course, training on vector representations in a specific area will further improve the result, but this may require, perhaps, excessive efforts and time. </div><p>Source: <a href="https://habr.com/ru/post/435984/">https://habr.com/ru/post/435984/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>