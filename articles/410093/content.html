<div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/wt/lo/15/wtlo15dajpz8fdk_4wila3kjdh0.jpeg"></div><br>  <i><font color="gray">Examples of persons who were used to control the multicopter during experiments.</font></i>  <i><font color="gray">The top row shows neutral faces to which the machine does not respond.</font></i>  <i><font color="gray">In the bottom row there are different variants of grimace triggers, corresponding to the Start command.</font></i>  <i><font color="gray">The two right speakers are remote photos from the multicopter.</font></i>  <i><font color="gray">The face recognition program successfully copes with poor image quality, recognizing faces from a distance of several meters</font></i> <br><br>  To control the UAV is usually used or a specialized device, or a special program on a smartphone / tablet.  But in the future, more convenient interfaces may be needed for human interaction with robots.  Students from <a href="http://autonomy.cs.sfu.ca/">the autonomous systems laboratory of the</a> Simon Fraser University School of Computer Science have developed an <a href="http://autonomy.cs.sfu.ca/doc/bruce_crv17.pdf">experimental program for controlling a multicopter using facial expressions</a> .  In principle, there is nothing particularly difficult in such a program, but the idea is interesting. <br><a name="habracut"></a><br>  Theoretically, such control can be more intuitive than the control of a joystick or buttons on a tablet.  Even now, in some situations, the face is easier to manage.  For example, in an experiment with a UAV casting along a parabolic trajectory (a la grenade throwing), it is really convenient to direct the machine by tilting the head. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zm/gx/qo/zmgxqotkyusik6ucwv1gtwfkhre.jpeg"></div><br>  <i><font color="gray">Parabolic trajectories of the UAV are calculated depending on the direction of the face of the operator</font></i> <br><br>  Running a drone using a certain facial expression is also more convenient than getting a remote or opening a program on a tablet.  Imagine: threateningly enough to knit your brows or puff out your cheeks, or make some other facial expression — the device immediately takes off and performs the actions you programmed it in advance (“throwing grenades” or circling a target). <br><br>  In addition to simpler control, there is also no need to spend money on the controller, as is the case with some multicopters.  Everybody has a face. <br><br><img src="https://habrastorage.org/webt/rv/kg/q-/rvkgq-h9zmxh2wa8etjmnhkikd0.jpeg"><br>  <i><font color="gray">Two stages of training.</font></i>  <i><font color="gray">On the left - learning neutral facial expression.</font></i>  <i><font color="gray">Right multicopter remembers grimace trigger</font></i> <br><br>  Expressions of persons to control can be arbitrary.  At the preliminary stage, the facial recognition program is trained.  She should see your neutral face - and grimace for the trigger team.  This is done as follows.  You need to raise the drone to eye level, place it in a horizontal position and give the face a neutral expression.  This expression should be kept until the robot is completely satisfied.  The procedure usually takes less than a minute.  Then the drone rotates 90 degrees - and you make a grimace trigger that is very different from your neutral expression. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/sHkcVIJt2_Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Then comes the “aiming” stage.  After take-off, the multicopter continues to constantly monitor the operator’s face with a video camera.  Even if the operator tries to escape - the robot will follow him and follow the expression on his face.  At this stage, the operator teaches multicopter action, which he will perform on the trigger command.  Currently three types of actions are implemented: <br><br><ul><li>  <b>Ray</b>  Direct movement in a given azimuth and at a given height.  The distance is determined by the size of the user's face during the aiming phase.  This is where the onion string bow analogy works.  The stronger the string is stretched (the farther the face) - the greater the energy of the “shot”. </li><li>  <b>Sling</b> .  Movement along a ballistic trajectory.  It can be useful if the operator wants to send a multicopter to the zone of loss of direct visibility, for an obstacle, for example, for photographing (see the video above).  The developers aptly "threw" the robot at 45 m, but they say that nothing prevents to throw it at hundreds of meters.  And if the UAV can find the target's face after the peak of the ballistic trajectory, then the two operators can be “thrown” by the robot at a distance of more than a kilometer. </li><li>  <b>Boomerang</b>  Movement around a circle of a given radius returning to the starting point.  Here, the trajectory parameters are set at the manual aiming stage, depending on the angle of rotation of the multikopter. </li></ul><br>  The students used a Parrot Bebop quadcopter slightly modified with a strip of LEDs for visual feedback. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ga/0i/1m/ga0i1mkw7jbdfb-4my3sbrkb5qo.png"></div><br>  <i><font color="gray">The results of the experiment "Ray".</font></i>  <i><font color="gray">User marked with an asterisk - expert (system developer)</font></i> <br><br>  In experiments, the system showed itself surprisingly well.  Participants had to send a drone into a hoop with a diameter of 0.8 m at a distance of 8 m - and in most cases coped with the task. <br><br>  In the future, the authors of the project say, people should “interact with robots and AI applications as naturally as they do now with other people and trained animals — as described in science fiction.” <br><br>  They notice that grimacing with robots is fun, and this can be used in entertainment applications - and people will come to grips with the idea that robots are so much fun. </div>