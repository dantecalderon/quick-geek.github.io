<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Infrastructure as a code, we win on a scale (Kirill Vetchinkin, TYME)</title>
  <meta name="description" content="The ‚ÄúInfrastructure as a code (IaC)‚Äù model, which is sometimes called the ‚Äúprogrammable infrastructure‚Äù, is a model in which the process of setting up...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>Infrastructure as a code, we win on a scale (Kirill Vetchinkin, TYME)</h1><div class="post__text post__text-html js-mediator-article"><p>  The ‚ÄúInfrastructure as a code (IaC)‚Äù model, which is sometimes called the ‚Äúprogrammable infrastructure‚Äù, is a model in which the process of setting up an infrastructure is similar to the process of programming software.  In essence, she initiated the elimination of the boundaries between writing applications and creating environments for these applications.  Applications may contain scripts that create and manage their own virtual machines.  This is the foundation of cloud computing and an integral part of DevOps. </p><br><p>  Infrastructure like code allows you to manage virtual machines at the program level.  This eliminates the need for manual configuration and upgrades for individual hardware components.  The infrastructure becomes extremely elastic, i.e. reproducible and scalable.  One operator can deploy and manage both one and 1000 machines using the same set of code.  Among the guaranteed benefits of infrastructure as a code are speed, efficiency and risk reduction. </p><br><p>  About this is the decoding of the report of Kirill Vetchkin on DevOpsDays Moscow 2018. In the report: reuse of Ansible modules, storage in Git, review, reassembly, financial benefits, horizontal scaling in 1 click. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/mElPCgMl_Wg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Who cares, I ask under the cat. </p><a name="habracut"></a><br><p>  Hello.  As already mentioned, I am Kirill Vetchinkin.  I work in the TYME company and today we will talk about infrastructure as a code.  We will also talk about how we have learned to save on this practice, because it is quite expensive.  Writing a lot of code is quite expensive to set up infrastructure. </p><br><p>  Briefly tell about the company.  I work for TYME.  We had a rebranding.  Now we are called PaySystem - as the name implies, we deal with payment systems.  We have both our own solutions - these are processing and custom development.  Custom development is electronic banking, billing and the like.  And as you understand, if this is a custom development, then this is a large number of projects every year.  The project is following the project.  The more projects, the more the same type of infrastructure has to be raised.  Since the projects are often highly loaded, we use microservice architecture.  Therefore, in one project there are still many, many small subprojects. </p><br><p><img src="https://habrastorage.org/webt/e2/pn/or/e2pnoryduehiqs7tpmruibojxng.png"></p><br><p>  Accordingly, it is very difficult to manage all this zoo without full-fledged DevOps.  Therefore, in our company introduced various practices of DevOps.  Naturally, we work on kanban, on SCRUM, we store everything in git.  After zakommitili, there is a continuous integration, tests are run.  Testers write on PyTest end-to-end tests that start every night.  Unit-test starts after each commit.  We use a separate process of assembly and deployment: collected, then deployed to different environments many times.  We were on windows.  On windows we deploy using Octopus deploy, This year we are developing on DotNet Core.  Therefore, we are now able to run software on Linux systems.  We left Octopus and came to Ansible.  Today we will talk about this part, which is a new practice that we have developed this year, something that we did not have before.  When you have tests, when you can build an application well, deploy it somewhere, everything is fine.  But if you have two environments configured differently, then you still fall, and you fall on production.  Therefore, managing configurations is a very important practice.  Here we will talk about it today. </p><br><p><img src="https://habrastorage.org/webt/_u/di/kj/_udikjskq0g3yi48718mjn1hf54.png"><br>  Briefly, I‚Äôll tell you how a product's economy is built in terms of labor costs: <strong>60</strong> percent is spent on <strong>development</strong> , <strong>analytics</strong> takes about <strong>10</strong> percent, <strong>QA</strong> (testing) takes about <strong>20</strong> percent and everything else is spent on configuration.  When systems run as a whole stream, many third-party software runs in them, the operating systems themselves are configured almost equally.  We spend too much time on this, doing essentially the same thing.  The idea was to automate everything and reduce the cost of infrastructure configuration.  Tasks of the same type are automated, well debugged and do not contain manual operations. </p><br><p><img src="https://habrastorage.org/webt/zf/vh/wm/zfvhwmb7pvlufeihfrgj0xk6dnq.png"><br>  Each application works in some environment.  Let's see what it all consists of.  We should at least have an <strong>operating system</strong> , it needs to be configured, there are some <strong>third</strong> - <strong>party applications</strong> that also need to be configured, <strong>the application itself</strong> needs to get configurations, but for the whole product to work, the application itself that runs in the entire system is running.  There is also a <strong>network</strong> that also needs to be configured, but we will not talk about the network today, because we have different customers, different network devices.  We also tried to automate the configuration of the network, but since the devices are different, there was no particular use for it, they spent more resources on it.  But we have automated operating systems, third-party applications, and the transfer of configuration parameters to the applications themselves. </p><br><p><img src="https://habrastorage.org/webt/mp/gx/l0/mpgxl0kr4wzjrc9zymbmjn2pk6c.png"></p><br><p>  Two approaches as you can configure the server: with hands - if you configure them with your hands, you can have such a situation that your production is configured in one way, the test is different, everything is green on the test, the tests are green.  You deploy to production, and there is no framework in place ‚Äî nothing works for you.  Another example: three Application servers are configured by hand.  One Application server is configured in one way, another Application server in another way.  Servers can work in different ways.  Another example: there was a situation when we have one Stage server completely stopped working.  Launched the creation of a new server using and after 30 the server was ready.  Another example: the server just stopped working.  If you set up your hands, then you need to look for a person who knows how to set it up, you need to raise the documentation.  As we know, documentation is hardly relevant.  This is a big problem.  And, most importantly, this is an audit, that is, roughly speaking, you have ten administrators, each of them is setting up something with his hands, it is not particularly clear that they have set it up correctly or incorrectly, and how to understand whether you need to do any then the settings could put something extra, open some unnecessary ports. </p><br><p><img src="https://habrastorage.org/webt/uz/pr/be/uzprbex2cscs4xglphdoy_wsenw.png"></p><br><p> There is an alternative option - this is exactly what we are talking about today - this is the configuration from the code.  That is, we have a git repository in which the entire infrastructure is stored.  All scripts are stored there with the help of which we will customize it.  Since this is all in git, we get all the benefits from managing the code, as in development, that is, we can do review, audit, change history, who did, why did, comments, we can roll back.  To work with the code, you need to use a continuous assembly pipeline ‚Äî a deployment pipeline.  In order for some system to make changes to the servers, that is, it was not the person who did something with his hands, but the system did it exclusively. </p><br><p><img src="https://habrastorage.org/webt/0r/dj/j_/0rdjj_cdepjplmukudcdg5sbnlo.png"></p><br><p>  As a system that makes changes, we use Ansible.  Since we do not have a huge number of servers, it suits us well.  If you have 100-200 servers there, then you will have small problems, because it (ie, Ansible) still connects with each and adjusts them in turn - this is a problem.  It is better to use other means that do not push (push), and bullet (pull).  But for our history, when we have a lot of projects, but no more than 20 servers ‚Äî this suits us perfectly.  Ansible has a big plus - a low threshold of entry.  That is, literally any IT specialist in three weeks can fully master it.  He has many modules.  That is, you can manage the clouds, networks, files, software installation, deployment - absolutely everything.  If there are no modules, you can write your own, you can finally write something using the Ansible shell or command module. </p><br><p><img src="https://habrastorage.org/webt/y6/2j/vq/y62jvqqmsm9jwm0shngvgrp_5_g.png"></p><br><p>  In general, briefly consider how it looks at all, this tool.  Ansible has modules, about which I have already spoken.  That is, they can be delivered, they can write themselves, who do something.  There are inventories - this is where we will roll our changes, that is, these are hosts, their IP addresses, variables specific to these hosts.  And, accordingly, the role.  Roles - this is what we will roll on these servers.  And we also have hosts grouped in groups, that is, in this case, we see that we have two groups: a database server and an application server.  In each group we have three cars.  They are connected via ssh.  Thus, we solve the problems that we talked about earlier, that in our first server we are configured identically, since the same role rolls onto the server.  And in the same way, if we run this role on several machines, then for each one, too, it will work in a similar way. </p><br><p><img src="https://habrastorage.org/webt/h2/hr/-t/h2hr-t11qunwpnaa-9ro7wxhfio.png"></p><br><p>  If we take a deeper look at how the Ansible project works, here we see that here inventories hosts are acceptable for production.  This group is listed and there are two servers in it.  If we go to a specific server, we see that the IP address of this machine is indicated here.  There may also be other parameters - variables that are specific to this environment.  If we look at the roles.  That role contains several tasks (tasks) that will be executed.  In this case, this is the role for installing PostgreSQL.  That is, we install the necessary application, create databases.  Here we use a loop.  There are several of them (databases).  Then we establish the necessary connection - IP addresses, which can be authorized in this database.  And, accordingly, set up at the very end of the firewall.  Settings will be applied to all servers in the group. </p><br><p><img src="https://habrastorage.org/webt/zg/ia/zo/zgiazoqklptftwwsjnxyqtq-1jc.png"></p><br><p>  Just approaching the problem itself: we learned how to configure servers with Ansible perfectly and everything was fine.  But, as I said, we have a lot of projects.  They are almost all the same.  Approximately such systems are involved in each project (k8s, RabbitMQ, Vault, ELK, PostgreSQL, HAProxy).  For each we wrote our role.  We can roll it from the button. </p><br><p><img src="https://habrastorage.org/webt/c3/ob/ez/c3obezyaxpshu6akdh6_qodbhp4.png"></p><br><p>  But we have a lot of projects and in each they essentially intersect.  That is, in one such set, in the second such, in the third such.  We get the intersection point, in which the same roles in different projects. </p><br><p><img src="https://habrastorage.org/webt/zk/fi/ag/zkfiagc0lsac1-a9tnce_rr7cha.png"></p><br><p>  We have a repository with the application, there is a repository with the infrastructure for the project.  The second project is exactly the same.  The continuation of the infrastructure.  And the third.  If we implement the same thing, then in fact we get copy-paste.  We will do the same role in 10 places.  Then, if there is any mistake, we will rule in 10 places. </p><br><p><img src="https://habrastorage.org/webt/g8/qe/ht/g8qehtvllis6iwedgzaasbu9pe8.png"></p><br><p>  What we did: we each role that is common to all projects and all its configurations that come from the outside, we took out to a separate repository and put them into a separate daddy in a guitar - called TYME Infrastructure.  There we have a role for PostgreSQL, for ELK, for deploying Kubernetes clusters.  If we need to put the same PostgreSQL in some project, then we simply enable it as a submodule, rewrite the inventories, that is, roughly speaking, the configuration where to roll this role.  We do not rewrite the role itself: it already exists.  And with us at the click of a button PostgreSQL appears in all new projects.  If you need to raise a cluster of Kubernetes - the same. </p><br><p><img src="https://habrastorage.org/webt/iy/q1/p0/iyq1p0mgtf6vhnk06s4mnirypls.png"></p><br><p>  Thus, it turned out to reduce the cost of writing roles.  That is, once written - 10 times used.  When a project is following a project, it is very convenient.  But since we are now working with the infrastructure as with the code, we naturally need pipelines, about which we spoke.  People commit to git, they can commit some kind of incorrectness - we need to keep track of it all.  Therefore, we have built such a pipeline.  That is, the developer commits ansible scripts in git.  TeamSity tracks them and sends them to Ansible.  Teamcity is needed here for one reason only: firstly, it has a visual interface (there is a free version of Ansible Tower - AWX, which solves the same problem - ed.) As opposed to Ansible free and, in principle, we have Teamcity as one CI.  So in principle, Ansible has a module that git can track itself.  But in this case they did it just in the image and likeness.  And as soon as he tracked him, he sends all the code to Ansible and Ansible, respectively, runs them on the integration server and changes the configuration.  If this process is violated, then we understand what is wrong, why the scripts are not written well. </p><br><p><img src="https://habrastorage.org/webt/rj/hc/wj/rjhcwjfpwdf_xwr8vpk4z2se26c.png"></p><br><p>  The second point is that there is a specific infrastructure, here we have the infrastructure deplots separately, the application deplots separately.  But there is a specific infrastructure for each application, that is, which needs to be deployed before we launch it.  Here, respectively, it is impossible to make it in a different pipeline.  You should have this deployed in the same container as the application itself.  That is, for example, frameworks are a popular thing when you need to install one framework for a new application, and another for another framework.  Here's how with this situation.  Either the caches need to be cleaned.  For example, Ansible can also climb, clear the cache. </p><br><p><img src="https://habrastorage.org/webt/b5/qj/rf/b5qjrf25y_2k4znz0-zidclu7fi.png"></p><br><p>  But here we use docker in combination with Ansible.  That is, the specific infrastructure we are in docker, non-specific in Ansible.  And thus, we sort of share this little delta in docker, everything else, the fundamental - in Ansible. </p><br><p><img src="https://habrastorage.org/webt/xq/6i/21/xq6i21qkn6l18s8f68becyjgt7y.png"></p><br><p>  A very important point is that if you roll the infrastructure through some scripts, through code, then if you still have manual manipulation of the servers, then this is a potential vulnerability.  Because suppose you put java on the test server, wrote the ELK role, rolled it.  Depla in the test was successful.  Deploy in production, but there is no java.  And you did not specify java in the script - it fell into production.  Therefore, you need to take the rights from all servers from administrators so that they do not climb there with their hands and make all the changes via git.  This whole conveyor we ourselves passed.  There is one thing here - no need to tighten the screws too much.  That is, it is necessary to introduce such a process gradually.  Because it is still undated.  In our case, we have left access to all systems at the most important head of administrators in case of unforeseen incidents.  Access is granted on the condition that he will not tune anything by hand. </p><br><p><img src="https://habrastorage.org/webt/za/ka/i0/zakai07gy34i3f7gjx8d-jtvpfy.png"></p><br><p>  How is the development going?  Rollout in staging, production should be without errors.  We can break something.  If the rollout in the integration environment will constantly fall on errors - it will be bad.  This is like debugging applications on a remote machine.  When a developer first develops everything on the machine, compiles.  If everything compiles, then sends it to the repository.  It uses the same approach.  Developers use Visual Studio Code with Ansible, Vagrant, Docker, etc. plugins.  Developers are testing their infrastructure code on a local vagrant.  There rises a clean operating system.  The scripts themselves for raising this machine are also located in this repository with the infrastructure we talked about.  The developer starts to install an FTP server on it.  If something goes wrong, he simply deletes it, reloads it, and tries again to install the necessary software onto it using deployment scripts.  After debugging deployment scripts, it makes a Merge Request to the main branch.  After the Merge Request merge, CI rolls out these changes to the integration server. </p><br><p><img src="https://habrastorage.org/webt/6q/os/qq/6qosqq_jtbghh6dvamxnj1sdvru.png"></p><br><p>  Since all the scripts are code, we can write tests.  Suppose we installed PostgreSQL.  We want to check it works or not.  For this we use the module Ansible assert.  Compare the installed version of PostgreSQL with the version in the scripts.  Thus, we understand that it is installed, it is generally running, it is the version we expected. </p><br><p><img src="https://habrastorage.org/webt/fc/3g/lf/fc3glflk8ozxdmpluhltv2a5mye.png"></p><br><p>  We see that the test has passed.  So our playbook worked correctly.  You can write as many tests as you like.  They are idempotent.  <strong>Idempotency</strong> (operation, which, if applied to any value several times - always produces the same value as in a single application).  If you write the proper script for installation, configuration, then make sure that your scripts always get the same value if you run them several times. </p><br><p><img src="https://habrastorage.org/webt/cn/tx/zd/cntxzd9yeatgui-yml_a61pegsa.png"></p><br><p>  There is another type of test that does not directly relate to testing infrastructure.  But they seem to affect him indirectly.  These are end-to-end tests.  Our infrastructure and the applications themselves are installed on the same server, which testers are testing.  If we have rolled up some kind of incorrect infrastructure, then we simply will not pass complex tests.  That is, our application will work somehow incorrectly.  In this example, we installed a new version on production ‚Äî the application is running.  Then commit was made to the git and end-to-end tests, which take place at night, to track that here we do not have an ftp file.  We analyze this case and see that the problem is in the ftp settings.  We fix the scripts in the code, deploy again and everything turns green.  Same story with code.  Infrastructure code and infrastructure is indirectly tested one way or another.  We can then deploy it to production. </p><br><p><img src="https://habrastorage.org/webt/mx/bo/e3/mxboe3llquz_vb46qalw6yir85m.png"></p><br><p>  When we implemented this approach, CI (Teamcity), which rolled out the changes to the integration server, fell 8 times out of 10. No one paid attention to this, because there was no feedback.  For developers, these processes were introduced a long time ago, and messages did not reach the OPS (system administrators).  Therefore, we added a Dashboard with assemblies of this project to a large monitor in the most prominent place in the office.  On it, various projects are highlighted in <strong>green</strong> - this means that everything is in order with him.  If highlighted in <strong>red</strong> it means that everything is bad with it.  We see that some tests failed.  At the presentation in the left side of the second one from above, we see the result of the infrastructure deployable tasks.  Infrastructure deploynye taskki green.  This means that all tests passed, there were no defects, they successfully assembled.  Alert: Suppose an IT professional has launched a script and has departed.  If kommit broke everything.  He receives an alert in the Slack channel that such an IT specialist broke our project with such a commit. </p><br><p><img src="https://habrastorage.org/webt/3b/bw/l1/3bbwl1jej_cien9satd3epefgqi.png"></p><br><p>  Ok, we just talked about how we develop, how we commit some test environments, how we generally roll it out further to other environments.  We use the trunk based approach.  Therefore, here the Master branch is the main branch of development.  When you commit to the Master, the CI server branch (Teamcity) rolls out the changes to the integration server.  If the task in the CI server is green, then we write to testers that we can test the product on the integration server.  We form a release candidate.  On the test server, this configuration appears.  Its testers can test along with the applications themselves.  If everything is ok, if the end-to-end tests are passed, we can already roll out the configuration itself and the application to the staging environment.  Then we can roll out to production.  Due to such multi-level barriers, we achieve that the staging environment is always green. </p><br><p><img src="https://habrastorage.org/webt/ut/lg/rb/utlgrbwe6gtssxn-ivmxwp3hpuc.png"></p><br><p>  Let's compare the management of the infrastructure from the code and the configuration of the infrastructure by hands.  What kind of economy?  This graph shows how we installed PostgreSQL.  Infrastructure management from the code turned out 5 times more expensive at an early stage.  All scripts must be written, otaladit hands.  This will take 1-2 hours.  Over time, there is an experience of writing these scripts.  Let's compare the installation, configuration of PostgreSQL hands and hours using the deployment script.  Since the deployment scripts and PostgreSQL settings have already been written, it takes 4 minutes to staging, production.  The more environments, the more machines, then you start to win on labor costs in setting up the infrastructure.  And if you have one project and one database, then it‚Äôs cheaper for you to do it by hand.  That is, it is interesting only when you have some large-scale projects. </p><br><p><img src="https://habrastorage.org/webt/ts/28/cl/ts28clrdqnfeocxls1rudzn0k5i.png"></p><br><p>  We have implemented a git submodule allowing the use of the Ansible role several times.  We do not need to write a second time, it is already there.  We add git submodule with the Ansible role we add to the project.  We write in the inventories of the server, where the role deploit.  It takes 30 minutes.  In the case of using git submodule, the effort required to write deployment scripts greatly overtook manual operations. </p><br><p><img src="https://habrastorage.org/webt/qx/y3/_w/qxy3_w7ra6n-35jnlc1oyhy_cew.png"></p><br><p>  About media identity: here I am not ready to say how many mistakes we had before and how much it became later.  But I want to say that, while observing all the rules about which we spoke today, our staging is set up exactly the same as the test.  Because if the test falls apart, we destroy the car, try to re-configure it, and only when it becomes green does the whole thing roll into staging.  How many mistakes your team makes with your hands - you can think for yourself, calculate, each one has a certain threshold. </p><br><p><img src="https://habrastorage.org/webt/mv/lv/nb/mvlvnbmp76dzsemwy9xdkv8nzda.png"></p><br><p>  The graph shows the results for 6 months.  Work - the complexity of a 10-point scale.  The first two months, we just wrote these roles very painfully and for a long time.  Because it was a new system.  When we wrote them, the schedule went down.  Somewhere in the middle, we implemented git submodule when we had already written the roles to reuse different projects.  This led to a sharp decline in labor costs.  If we compare it with the manual adjustment of the project for which this assessment was made, we set up three days by hand, using the infrastructure approach as the code set up for about a month.  In perspective, using the approach of infrastructure as a code is more efficient than setting up servers manually. </p><br><p><img src="https://habrastorage.org/webt/tz/bm/n4/tzbmn4-jneygsmiecrtjudgw5ne.png"><br>  Findings. </p><br><p>  First, we got the documentation, that is, when the manager comes in and asks me: which port does the database listen to, I open the Ansible script in the git repository, say: ‚ÄúLook, look at this and that port‚Äù.  What are our settings here?  All this can be seen in the git repository.  It can be audited, shown to managers and so on.  This is 100% reliable information.  Documentation is becoming obsolete.  And in this case, nothing obsolete. </p><br><p>  The important point about which we did not speak.  About him indirectly say.  There are development teams that need to raise RabbitMQ, ELK locally too in order to test various ideas.  If they do with their hands, then lifting ELK can take more than an hour.  When using the infrastructure approach as a code, the developer raises the virtual machine, presses the button, and ELK is installed on it.  If the developer broke ELK, then he can delete, start ELK installation on the virtual machine again. </p><br><p>  As we have said, this is all cheaper only when you have either many projects, or many environments, or many machines.  If you do not have this, then, accordingly, it is not cheaper, it is more expensive.  In our perspective, in our projects it turned out that it becomes cheaper over time.  Therefore, there are both pros and cons. </p><br><p>  Rapid disaster recovery.  If you fell, you need to look for a person who can set up this machine, who knows how to set it up, who remembers how to set it up.  In this case, all the settings and deployment scripts are in git.  Even if a person quit, moved to another project - everything is in git, everything is in the comments, everything is clear: why he opened such a port, why he put such and such settings and so on.  You can easily restore everything. </p><br><p>  The number of errors decreased because we had different barriers, local tracking environments, development, plus code review.  This is also important, because developers are looking at what they are doing.  Thus, they are still learning.  The complexity has increased, because this is, accordingly, a new system, these are pipelines, people need to be trained.  This is not what they usually used to: they read the article and did it under the article.  Here is a little more.  Nevertheless, over time, it is quite easily mastered.  If you look at the development completely, then it will be about three or four months. </p><br><p>  The question is asked through the application, but the question is not heard. </p><br><p>  Answer: Roles we use the latest version, that is, the role is allocated in a separate git submodule only when it is unique for absolutely all systems.  If any commits occurred on it, then we go to the latest commit.  And the whole configuration comes from inventories.  That is, the role is just what will be performed. –ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥–∏–Ω, –ø–∞—Ä–æ–ª—å –∏ —Ç.–¥. –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å–Ω–∞—Ä—É–∂–∏. –†–æ–ª—å ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º. –í—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏—Ö–æ–¥–∏—Ç –∏–∑ —Å–∞–º–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. </p><br><p> –í–æ–ø—Ä–æ—Å: –ï—Å–ª–∏ —É –≤–∞—Å –∫–∞–∫–∏–µ-–Ω–∏–±—É–¥—å –º–µ–∂–¥—É —Ä–æ–ª—è–º–∏ Ansible —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∫–∞–∫ –≤—ã –≤—ã—Ç—è–≥–∏–≤–∞–µ—Ç–µ –ø—Ä–∏ –¥–µ–ø–ª–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (—Ä–æ–ª—å A –∑–∞–≤–∏—Å–∏—Ç –æ—Ç B, B –æ—Ç C –∏ –≤—ã –∫–æ–≥–¥–∞ –≤—ã–∫–∞—Ç—ã–≤–∞–µ—Ç–µ —Ä–æ–ª—å A, —á—Ç–æ–±—ã –æ–Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤—Å–µ —Å–∞–º–∞ —Ç—è–Ω—É–ª–∞)? –ü–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º –∫–∞–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ —É –≤–∞—Å —Ç–∞–∫–æ–π –µ—Å—Ç—å? </p><br><p> –û—Ç–≤–µ—Ç: –í –¥–∞–Ω–Ω–æ–π –ø–∞—Ä–∞–¥–∏–≥–º–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ç–∞–∫–∏—Ö –Ω–µ—Ç. –£ –Ω–∞—Å –±—ã–≤–∞—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. –¢–æ –µ—Å—Ç—å, –º—ã —Å—Ç–∞–≤–∏–º –∫–∞–∫—É—é-—Ç–æ —Å–∏—Å—Ç–µ–º—É –∏ –∑–Ω–∞–µ–º IP –∞–¥—Ä–µ—Å–∞ —Å–µ—Ä–≤–µ—Ä–æ–≤, –∏ –≤ —Ç–æ –∂–µ —Å–∞–º–æ–µ –µ—Å—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —Ç–µ–ø–µ—Ä—å —Å—Ç–∞–ª, –¥–æ–ª–∂–µ–Ω —Å–µ–±—è –∫–∞–∫ –±—ã –≤–æ–≥–Ω–∞—Ç—å, —á—Ç–æ–±—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —ç—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–∞—à–∏–Ω—É. –≠—Ç–æ –∑–∞ —Å—á–µ—Ç –∫–æ–Ω—Ñ–∏–≥–æ–≤ –∫–∞–∫ –±—ã —Ä–∞–∑—Ä—É–ª–∏–≤–∞–µ—Ç—Å—è. –ù–æ, –µ—Å–ª–∏ –≤—ã –∏–º–µ–µ—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –º–æ–¥—É–ª–µ–π, —Ç–æ –∑–¥–µ—Å—å –Ω–∏—á–µ–≥–æ –Ω–µ—Ç, —É –Ω–∞—Å –æ–¥–∏–Ω —Å–≤–æ–±–æ–¥–Ω—ã–π, –Ω–∏ –æ—Ç —á–µ–≥–æ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç, –æ–Ω —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π. –ò –º—ã –Ω–µ –≤—ã–Ω–æ—Å–∏–º –≤ –æ–±—â–∏–µ —Ä–æ–ª–∏ —Ç–æ, —á—Ç–æ –∏–º–µ–µ—Ç —Ö–æ—Ç—å –∫–∞–∫—É—é-—Ç–æ –Ω–µ –æ–±—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Ç–æ –µ—Å—Ç—å, –¥–æ–ø—É—Å—Ç–∏–º, RabbitMQ –º–æ–≥—É—Ç –∏ –≤ –∞—Ñ—Ä–∏–∫–µ RabbitMQ, –æ–Ω –Ω–∏ –æ—Ç —á–µ–≥–æ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç. –ö–∞–∫—É—é-—Ç–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –º—ã –≤ –¥–æ–∫–µ—Ä–µ –∫–∞–∫ —Ö—Ä–∞–Ω–∏–º, —Ç–µ –∂–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏. </p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PS I suggest that all interested in jointly on github translate into the text interesting reports from conferences. </font><font style="vertical-align: inherit;">You can make a group in github for translation. </font><font style="vertical-align: inherit;">So far, I translate into text in my </font></font><a href="https://github.com/patsevanton/russian-conferences-articles"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github account</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - there you can also send a Pull request to correct the article.</font></font></p></div><p>Source: <a href="https://habr.com/ru/post/438748/">https://habr.com/ru/post/438748/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>