<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About creating budget stereoscopic images on fingers (stereogram, anaglyph, stereoscope)</title>
  <meta name="description" content="Another weekend came, you need to write a couple of dozen lines of code and draw a picture, but better not one. So, last weekend and the day before la...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-6974184241884155",
      enable_page_level_ads: true
    });
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>About creating budget stereoscopic images on fingers (stereogram, anaglyph, stereoscope)</h1><div class="post__text post__text-html js-mediator-article">  Another weekend came, you need to write a couple of dozen lines of code and draw a picture, but better not one.  So, last weekend and the day before last I showed how <a href="https://habr.com/ru/post/436790/">to do ray tracing</a> and even <a href="https://habr.com/ru/post/437714/">blow up anything.</a>  This is surprising to many, but computer graphics is a very simple thing, a couple of hundred lines of bare C ++ is enough to create interesting pictures. <br><br>  The topic of today's conversation is binocular vision, and even today we will not reach even a hundred lines of code.  Knowing how to render three-dimensional scenes, it would be foolish to walk past a stupar, today we will draw something like this: <br><br><img src="https://habrastorage.org/webt/2-/8t/3n/2-8t3n-oonieil_v4f_lntjpvzk.jpeg"><br><a name="habracut"></a><br>  The madness of the <a href="https://en.wikipedia.org/wiki/Magic_Carpet_(video_game)">Magic Carpet</a> developers is haunting me.  For those who did not find this game, it was possible to make a 3D render in both anaglyph and stereograms <b>in the basic settings, just available in the menu!</b>  The brain just blew it up specifically. <br><br><h1>  Parallax </h1><br>  So let's get started.  To begin with, thanks to what does our visual apparatus allow us to perceive depth?  There is such a clever word "parallax".  If on the fingers, then let's focus our eyes on the screen.  Everything that is in the plane of the screen for our brain exists in a single copy.  But if a fly suddenly flies in front of the screen, then (if we don‚Äôt change our eyes!) Our brain will register it in two copies.  And at the same time, the spider on the wall behind the screen also forks, and the direction of the split depends on whether the object is in front of the focal point or behind: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a40/e9b/b9d/a40e9bb9d4a6e0cddcdce4bdfcc5095d.png"><br><br>  Our brain is a very efficient machine for analyzing slightly different images.  It uses <a href="https://en.wikipedia.org/wiki/Binocular_disparity">disparity</a> to obtain depth information from two-dimensional images of the retina for <a href="https://en.wikipedia.org/wiki/Stereopsis">stereopsis</a> .  Well, God bless them, with words, let's better draw pictures! <br><br>  Let's assume that our screen is a window into the virtual world :) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c9a/645/594/c9a645594d957d52f5d4c3e067bd3cb7.png"><br><br>  Our task is to draw two pictures with what will be visible through this ‚Äúwindow‚Äù.  There will be two pictures, one for each eye, in the diagram above I showed them with a red and blue ‚Äúsandwich‚Äù.  Let's not bother yet, how exactly we feed these pictures to the visual apparatus, we just need to save two files.  Specifically, I am interested in how these images can be obtained with the help of <a href="https://habr.com/ru/post/436790/">our ray tracer</a> . <br><br>  Well, let's say, the direction does not change, this is a vector (0,0, -1).  Suppose we can move the camera position to the inter-eye distance, what else?  There is one small subtlety: the cone of a look through our ‚Äúwindow‚Äù is asymmetric.  And our raytracer can only render a symmetric view cone: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e00/5c1/b99/e005c1b996cc7eb9978bc434f6773196.png"><br><br>  What to do?  Read :) <br>  In fact, we can render the pictures wider than we need, and just cut off the extra: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cb8/f49/3d1/cb8f493d14679c297d6510a1a79ef1a3.png"><br><br><h1>  Anaglyph </h1><br>  With the general rendering mechanism it should be clear, now is the time to wonder about the delivery of the image to our brain.  One of the easiest options is red and blue glasses: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cd4/7fb/815/cd47fb815c7a1e80e2d908049e8e4bf0.jpg"><br><br>  We will simply make two pre-renders not in color, but in black and white, the left picture will be recorded in the red channel, and the right one - in the blue one.  It turns out this picture: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9d7/8f6/4f3/9d78f64f371b5960b1d19f5deaff0d9e.jpg"><br><br>  Red glass will cut off one channel, and blue glass will cut off another, so everyone‚Äôs eyes will receive their own picture, and we can look at the world in 3D.  Here are the <a href="https://github.com/ssloy/tinyraytracer/commit/8698c64eec98419c0734f663ab1543c826ed0342">changes to the main commit of the first article</a> , which show both camera settings for both eyes and channel assembly. <br><br>  Anaglyph renders are one of the most ancient ways to view (computer!) Stereo images.  They have many drawbacks, for example, poor color rendering (by the way, try recording the green channel of the right eye in the green channel of the final picture).  One benefit - these glasses are easy to make from scrap materials. <br><br><h1>  Stereoscope </h1><br>  With the mass distribution of smartphones, we remembered what stereoscopes are (which, for a moment, were invented in the 19th century)!  A few years ago, <a href="https://vr.google.com/cardboard/">Google offered to</a> use two penny lenses (unfortunately, they are not made on the knee), a bit of cardboard (lying everywhere) and a smartphone (in your pocket) to get quite tolerable virtual reality glasses: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/328/5d8/98a/3285d898a99110b217a0fbdbc8ddb535.jpg"><br><br>  On the aliexpress, their heaps, a hundred rubles a piece.  Compared to anaglyph, you don‚Äôt need to do anything at all, just take two pictures and make them side by side, <a href="https://github.com/ssloy/tinyraytracer/commit/e966911a467ab480b8cb9ca8bb662d65c66ea463">here‚Äôs a commit</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/82e/563/cfe/82e563cfe69db42b7f5f2f399261d12d.jpg"><br><br>  Strictly speaking, depending on the lens, lens <a href="https://support.google.com/cardboard/manufacturers/answer/6324808%3Fhl%3Den">distortion correction</a> may be needed, but I didn‚Äôt bother at all, and it looks great on my glasses.  But if you really need to apply a barrel-shaped pre-distortion, which compensates for the distortion from the lens, then this is how it looks for my smartphone and for my glasses: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9e2/583/9f5/9e25839f5ea28b1f4e092106f60bebfe.jpg"><br><br><h1>  Stereograms </h1><br>  And what to do if you do not want to use additional devices at all?  Then one option - ooset.  Generally speaking, the previous picture is quite enough to view the stereo, just use the trick to view stereograms.  There are two principles for viewing stereograms: either move your eyes or move apart.  So I drew a diagram on which I show how you can look at the previous picture.  The previous picture is double, two red lines on the diagram show two images on the left retina, two blue ones on the right one. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86e/14a/47b/86e14a47b4e2ae8aa9246becb7155827.png"><br><br>  If we focus our gaze on the screen, then out of the four images we have two.  If we squint to the nose, it is quite possible to show the brain "three" pictures.  Conversely, if you open your eyes, you can also get ‚Äúthree‚Äù pictures.  Overlaying central images will give the brain a stereo effect. <br><br>  These methods are given to different people in different ways, for example, I do not know how to move my eyes at all, but I can easily spread them.  It is important that a stereogram constructed for one method should be viewed in the same way, otherwise an inverted depth map is obtained (see negative and positive parallax).  The problem with this way of viewing stereo is that it is very difficult <b>to</b> move the eyes relatively to the normal state, so you have to be content with small pictures.  And what if you want big?  Let's completely sacrifice color, and want to get only the perception of depth.  Looking ahead, here is a picture that we get at the end of this part: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f7b/e7b/ab2/f7be7bab228dcd5133b2d1ff3a9032e1.jpg"><br><br>  This stereogram is designed for "breeding" the eyes (wall-eyed stereogram).  For those who prefer the reverse way of viewing, <a href="https://habr.com/ru/post/438646/">take a picture here</a> .  If you are not used to stereograms, try different conditions: full-screen picture, small picture, bright light, darkness.  The task is to separate the eyes so that the two adjacent vertix strips coincide.  It is easiest to focus on the top left of the picture, because  she is flat  For example, I am hampered by the surroundings of the habr, I open the picture to full screen.  Do not forget to remove the mouse from it! <br><br>  Do not be satisfied with an inferior 3D effect.  If you are only vaguely aware of rounded shapes in the midst of random points along with some weak 3D effects, this is, of course, an incomplete illusion!  If you look correctly, the balls should obviously go out of the screen plane to the viewer, the effect should be stable and maintained due to the constant and detailed study of each part of the image, both the foreground and background.  The stereopsis has a hysteresis: as soon as you can get a stable image, it becomes clearer the longer you look.  The farther the screen from the eyes, the greater the effect of depth. <br><br>  This stereogram is drawn according to the method proposed by Thimbleby and others a quarter of a century ago in their article " <a href="https://www.cs.waikato.ac.nz/~ihw/papers/94-HWT-SI-IHW-SIRDS-paper.pdf">Displaying 3D Images: Algorithms for Single Random Image Dot Stereograms</a> ". <br><br><h3>  A starting point </h3><br>  The starting point for drawing stereograms is a depth map (we forgot about the color).  <a href="https://github.com/ssloy/tinyraytracer/commit/09bf2208e5fd00395bd4540ff6c2acb3d4e59964">Here is the commit</a> that renders this image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/53c/201/75f/53c20175f5fa2667a7dd7592fee343c4.jpg"><br><br>  The depths in our renderer are clipped to the near and far planes, that is, the furthest point in my map has a depth of 0, the nearest one is 1. <br><br><h3>  The basic principle </h3><br>  Let our eyes be at a distance d from the screen.  Place the (imaginary) far plane (z = 0) at the same distance behind the screen.  Choose a constant Œº, which will determine the position of the near plane (z = 0): it will be at a distance of Œºd from the far.  I chose Œº = 1/3 in my code.  Total, our whole world lives at a distance from d-Œºd to d behind the screen.  Suppose we have defined the distance e between the eyes (in pixels, in my code I chose 400 pixels). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c6/0eb/e87/5c60ebe872837aea90879fa0798ac7e7.png"><br><br>  If we look at the point of our object marked in red in the scheme, then the two pixels marked in green should have the same color in the stereogram.  How to find the distance between these pixels?  Very simple.  If the current projected point has a depth z, then the ratio of the parallax to the distance between the eyes is equal to the ratio of the corresponding depths: p / e = (d-dŒºz) / (2d-dŒºz).  By the way, note that d is shrinking and not participating anywhere else!  That is, p / e = (1-Œºz) / (2-Œºz), which means that the parallax equals p = e * (1-Œºz) / (2-Œºz) pixels. <br><br>  That is, the basic principle of constructing a stereogram: we go through the entire depth map, for each depth value we determine which pixels should have the same color, and write this into our system of constraints.  After that, we start with an arbitrary picture, and try to fulfill all previously imposed restrictions. <br><br><h3>  Prepare the original image </h3><br>  In this stage, we will prepare a picture, which later will impose parallax restrictions. <br>  <a href="https://github.com/ssloy/tinyraytracer/commit/6842f67bb7745a07fd46629a6119b0842f36572e">Here is a commit</a> , he draws just such a picture: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/197/441/685/197441685bf85d56e5416e0af899ace1.jpg"><br><br>  Notice that the overall colors are just random, except that I put rand () * sin in the red channel to provide periodic waves.  These waves are made with a distance of 200 pixels, this (with selected Œº = 1/3 and e = 400) is the maximum parallax value in our world, it is also a far plane.  These waves are optional, but they will facilitate the desired focusing of view. <br><br><h3>  Render the stereogram </h3><br>  Actually, the full code related to the stereogram looks like this: <br><br><pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parallax</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> z)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> eye_separation = <span class="hljs-number"><span class="hljs-number">400.</span></span>; <span class="hljs-comment"><span class="hljs-comment">// interpupillary distance in pixels const float mu = .33; // if the far plane is a distance D behind the screen, then the near plane is a distance mu*D in front of the far plane return static_cast&lt;int&gt;(eye_separation*((1.-z*mu)/(2.-z*mu))+.5); } size_t uf_find(std::vector&lt;size_t&gt; &amp;same, size_t x) { return same[x]==x ? x : uf_find(same, same[x]); } void uf_union(std::vector&lt;size_t&gt; &amp;same, size_t x, size_t y) { if ((x=uf_find(same, x)) != (y=uf_find(same, y))) same[x] = y; } int main() { [...] for (size_t j=0; j&lt;height; j++) { // autostereogram rendering loop std::vector&lt;size_t&gt; same(width); std::iota(same.begin(), same.end(), 0); // initialize the union-find data structure (same[i]=i) for (size_t i=0; i&lt;width; i++) { // put the constraints int par = parallax(zbuffer[i+j*width]); int left = i - par/2; int right = left + par; // works better than i+par/2 for odd values of par if (left&gt;=0 &amp;&amp; right&lt;(int)width) uf_union(same, left, right); // left and right pixels will have the same color } for (size_t i=0; i&lt;width; i++) { // resolve the constraints size_t root = uf_find(same, i); for (size_t c=0; c&lt;3; c++) framebuffer[(i+j*width)*3+c] = framebuffer[(root+j*width)*3+c]; } } [...]</span></span></code> </pre> <br>  If that, then <a href="https://github.com/ssloy/tinyraytracer/commit/7a2599f76ec944059663aacb965255c9c8e34217">commit to take here</a> .  The function int parallax (const float z) gives the distance between pixels of the same color for the current depth value.  We render the stereogram line by line, as the lines are independent of each other (we do not have vertical parallax).  Therefore, the main loop just runs through all the lines;  for each of them, we start with a full unlimited set of pixels on which we will then impose pairwise equality constraints, and as a result we will have a certain number of clusters of (incoherent) pixels of the same color.  For example, a pixel with a left index and a pixel with the right index should end up being the same. <br><br>  How to store this set of restrictions?  The simplest answer is <a href="https://en.wikipedia.org/wiki/Disjoint-set_data_structure">union ‚Äì find data structure</a> .  I will not describe it, it‚Äôs only three lines of code, you can read it in Wikipedia.  The basic idea is that for each cluster we will have some kind of ‚Äúresponsible‚Äù for it, it‚Äôs the root pixel, we‚Äôll leave it the same color as it was in the original image, and repaint all the other pixels in the cluster: <br><br><pre> <code class="cpp hljs"> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">size_t</span></span> i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;width; i++) { <span class="hljs-comment"><span class="hljs-comment">// resolve the constraints size_t root = uf_find(same, i); for (size_t c=0; c&lt;3; c++) framebuffer[(i+j*width)*3+c] = framebuffer[(root+j*width)*3+c]; }</span></span></code> </pre><br><h1>  Conclusion </h1><br>  Well, actually, that's all.  Twenty lines of code - and our stereogram is ready, break eyes and heads, draw pictures!  By the way, just random colors in a stereogram is generally a luxury, in principle, if you try, you can make a partial transfer of the color of our image. <br><br>  Other stereo viewing systems, for example, <a href="https://en.wikipedia.org/wiki/Polarized_3D_system">related to polarization</a> , I brought out of the framework of the discussion, since they go out of the budget of one hundred rubles.  If that is missed, add and correct! </div><p>Source: <a href="https://habr.com/ru/post/438646/">https://habr.com/ru/post/438646/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>