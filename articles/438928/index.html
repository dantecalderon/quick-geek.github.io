<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AI party in Silicon Valley: mayor, billionaire, presidents, geniuses, processor developers and a girl with bright hair</title>
  <meta name="description" content="Last year, a wave of articles about parties in Silicon Valley was held in the Russian and Ukrainian press, with some kind of Hollywood atmosphere, but...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>AI party in Silicon Valley: mayor, billionaire, presidents, geniuses, processor developers and a girl with bright hair</h1><div class="post__text post__text-html js-mediator-article">  Last year, a wave of articles about parties in Silicon Valley was held in the Russian and Ukrainian press, with some kind of Hollywood atmosphere, but without specifying specific names, photographs, and without describing the technologies for developing hardware and writing software related to these names.  This article is different!  In it, too, will be billionaires, geniuses and girls, but with photos, slides, diagrams, and fragments of program code.  So: <br><br>  Recently, the mayor of the city of Campbell, with the Russian surname Paul Resnikoff, cut the ribbon when opening a new office for the startup Wave Computing, which, together with Broadcom, is developing a 7-nm chip to speed up the computation of neural networks.  The office is located in the building of the historic fruit canning factory of the late 19th and early 20th centuries, when Silicon Valley was the largest orchard in the world.  Even then, in the office, they were engaged in innovations, introducing electric motors for conveyors in the apricot-plum industry, for which about 200 employees, mostly women, worked. <br><br>  On the party following the cutting of the ribbon, many well-known people in the industry came to light, in particular Kernigan-Richie's associate and author of the most popular C compiler of the late 70s - early 80s Stephen Johnson, one of the authors of the floating-point standard Jerome Cunen, inventor local bus concepts and chipset developer of the first PC AT Diosdado Banatao, former developers of Sun, DEC, Cyrix, Intel, AMD and Silicon Graphics processors, Qualcomm, Xilinx and Cypress chips, industrial analysts, a girl with red hair and other Californian co-workers  This type of company. <br><br>  At the end of the post we will talk about what books you need to read and do some exercises to join this community. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/g6Ka6je8e48" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Let's start with Jerome Coonen, the innovator of floating point arithmetic and the Apple manager at the time of the first Macintosh. <br><a name="habracut"></a><br>  Not so often there are master's theses that affect computing on billions of devices.  This was the disserver Jeromei Kunen (left) "Contributions for a Floating Point Arithmetic", the results of which were included in IEEE Standard 754 floating-point numbers.  After graduating from Berkeley Graduate School in 1982, Jerome went to work for Apple, where he introduced floating point libraries to the first Macintosh. <br><br>  After 10 years of management at Apple, Kunen advised Hewlett-Packard and Microsoft, and in 2000 he optimized 128-bit arithmetic for AMD's only 64-bit version of x86.  Recently, Jeromy turned his attention to research on floating-point standards for neural networks, in particular, disputes about Unum and Posit.  Unum is the new proposed standard, promoted by the Caltech scholar John Gustafson, the author of The End of Error, the now noisy book, End of Error.  Posit is a version of Unum that can be more effectively (*) than Unum implemented in hardware. <br><br>  (*) More effective in terms of a combination of parameters: clock frequency, number of cycles per operation, conveyor throughput, relative area on a chip, and relative power consumption. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/75f/b62/177/75fb62177ed3fd152c5be36fe0c0423d.jpg"><br><br>  Images from articles (not from Jeromy) <a href="https://code.fb.com/ai-research/floating-point-math/">Making a floating point math</a> <br>  <a href="http://superfri.org/superfri/article/view/137/232">Beating Floating Point at its Own Game: Posit Arithmetic by John L. Gustafson and Isaac Yonemoto</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/60e/fa4/535/60efa4535dd845885e12a421122feb40.png"><br><br>  But at the party Stephen / Steve Johnson is a person on whose compiler the C programming language has become popular.  The first C compiler was written by Denis Ritchie, but the Richie compiler was tightly tied to the PDP-11 architecture.  Steve Johnson, based on the work of Alan Snyder, wrote in the mid-1970s the Portable C Compiler (PCC) compiler, which was easy to remake to generate code for different architectures.  At the same time, the Johnson compiler worked quickly and was optimizing.  How did he achieve this? <br><br>  At the PCC input, Steve Johnson used the LALR (1) parser generated by the YACC (Yet Another Compiler Compiler) program, which was also written by Steve Johnson.  After this, the compilation task was reduced to manipulating trees in recursive functions and generating code using the template table.  Some of these recursive functions were machine-independent, the other part was copied by people who transferred PCC to another machine.  The template table consisted of rule entries of the type ‚Äúif the register is of type A and two of the register of type B, rebuild the tree into a node of type C and generate a code with the string D‚Äù.  The table was machine dependent. <br><br>  Due to the combination of elegance, flexibility and efficiency, the PCC compiler was transferred to more than 200 architectures - from PDP, VAX, IBM / 370, x86 to the Soviet BESM-6 and Orbit 20-700 (the onboard computer in early versions of the MiG-29).  According to Denis Ritchie, almost every C compiler of the early 1980s was based on PCC.  From the BSD Unix world, the PCC was supplanted as the standard-only GNU GCC compiler in 1994. <br><br>  In addition to PCC and Yacc, Steve Johnson is also the author of the original Lint program verification program (see eg <a href="http://www.silicon-russia.com/wp-content/uploads/2019/02/lint_1978_stephen_johnson.pdf">1978 article</a> ).  The names of the programs Yacc and Lint have since become a household name.  In the 2000s, Steve rewrote the MATLAB front-end and wrote MLint.  Now Steve Johnson is busy with the task of parallelizing the algorithms for computing neural networks on devices like CGRA (Coarse-Grained Reconfigurable Architecture), with tens of thousands of processor elements, which are thrown between tensors through a network of tens of thousands of transistors too: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7a4/024/2f7/7a40242f72ebc609b6e64cfb2cca2343.jpg"><br><br>  But with a glass of wine billionaire Diosdado Banatao, the founder of Chips &amp; Technologies, S3 Graphics and an investor in Marvell.  If you programmed the IBM PC in 1985-1988, when they first appeared in the USSR, then you may know that inside most of the AT-NIS with EGA and VGA graphics, there were chipsets from Chips &amp; Technologies, which came out simultaneously with IBM-ovsky.  Early C &amp; T chipsets were designed by Banatao, who studied electronics at Stanford, and before Stanford worked as an engineer in Boeing.  In 1987, Chips &amp; Technologies bought Intel. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c52/6da/200/c526da2001d499259a77e57b09cbf605.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/591/477/731/591477731cfca11f9537d0dd45be0015.jpg"><br><br>  On the left in the picture below is John Bourgoin, President of MIPS Technologies at the time of this company's heyday in the 2000s, when chips with MIPS cores were inside most DVD players, digital cameras and TVs, with chipsets from Zoran, Sigma Design, Realtek, Broadcom and other companies.  Prior to that, John was president of MIPS Silicon Graphics since 1996 when MIPS processors stood inside Silicon Graphics workstations, which were used in Hollywood to shoot the first realistic 3D Jurassic Park films.  Prior to Silicon Graphics, John was one of the vice presidents of AMD, since 1976. <br><br>  Art Swift, right, was the MIPS vice president of marketing in the 2000s, and before that in the 1980s he worked as an engineer at Fairchild Semiconductor (yes, that one), then as vice president of marketing at Sun, DEC, Cirrus Logic, and President of the company Transmet.  Recently, Art was vice-chairman of the marketing committee of RISC-V and is familiar with Russian companies Syntacore and CloudBear in this position.  And now he has become president of Wave's MIPS IP direction: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4d4/466/148/4d44661486839ecda93712a81cb76576.jpg"><br><br>  <a href="http://panchul.com/dropbox/2012_10_27/mips_russia_2012_1_background.pptx">MIPS history presentation</a> slides related to the period when MIPS was managed by John Bourgoin, in the above left image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c27/261/ca5/c27261ca5c7650dbf8028823eb6f8ff5.png"><br><br>  The company Transmet, whose president was Art Swift for a while, in the photo above to the right, released the Crusoe processor in the late 1990s, which could carry out x86 instructions and reached the market in Toshiba Libretto L sub-laptops, NEC and Sharp laptops , the thin client from Compaq.  Their competitive advantage over Intel and AMD was put to controlled low power consumption. <br><br>  Direct implementation and verification of the full set of x86 is a very expensive event, so Transmet went the other way, which resembles the path of the Russian company MCST with the Elbrus processor (the line that began with Elbrus 2000 and is now represented as Elbrus 8C).  Transmet and Elbrus were set up as a basis for a structurally simple processor with a VLIW microarchitecture, and on top of it worked the x86 emulation level using the technology that Transmeta called code morphing. <br><br>  The idea of ‚Äã‚ÄãVLIW (Very Long Instruction Word - Very Long Word Instructions) is quite simple - several processor instructions are explicitly declared by one superinstruction and are executed in parallel.  Unlike superscalar processors, in particular Intels, starting with PentiumPro (1996), in which the processor selects several instructions from memory, and then decides what to execute in parallel and what is sequential, based on automatic analysis of dependencies between instructions. <br><br>  A superscalar processor is much more complicated than a VLIW, because a superscalar has to spend logic on maintaining the illusion of the programmer that all the selected instructions are executed one after another, although there are actually dozens of them inside the processor at different stages of execution.  In the case of VLIW, the burden of maintaining such an illusion falls on the compiler from a high-level language.  Ultimately, the VLIW circuit breaks down when the processor has to work with multi-level cache memory, which has unpredictable delays, which make it difficult for the compiler to schedule tick instructions.  But for mathematical calculations (for example, to put Elbrus on the radar and to calculate the movement of the target) this is the most, especially in the conditions of a shortage of qualified engineering personnel (more people are needed to verify the superscalar). <br><br>  Illustration of the VLIW idea, the Crusoe processor and the Toshiba Libretto L1 sub-laptop: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/876/843/22b/87684322b762cf08eb019374d56fc2a1.png"><br><br>  But in the center in the photo below, Derek Meyer, Derek Meyer, the current CEO of Wave Computing.  Prior to Wave, Derek was the CEO of ARC, the developer of ARC processor cores, which are used in audio chips.  At one time, these cores were <a href="https://www.businesswire.com/news/home/20100901005664/en/Russian-SoC-Leader-MRI-Progress-Licenses-Virage">licensed by the Russian company NIIMA Progress</a> , which later licensed the MIPS cores and <a href="https://habr.com/en/post/395357/">showed the chips based on them at the exhibition in Kazan Innopolis</a> .  Derek Meyer repeatedly traveled to Russia, to St. Petersburg, where the Virage Logic development team was located.  In 2009, ARC bought Virage Logic, and in 2010, ARC was swallowed up by Synopsys, the main chip design manufacturer in the world. <br><br>  Right on the photo - <a href="https://vak.dreamwidth.org/">Sergei Vakulenko</a> , who at the dawn of his career was at the origins of the Runet, worked in the cooperative Demos and the Kurchatov Institute, which brought the Internet to the USSR.  Now Sergey is writing a cycle-accurate model of the Wave processor element for computing neural networks, and earlier he wrote instruction-accurate models of MIPS cores that were used to verify MIPS processor cores I6400 Samurai, Shaolin I7200 and others. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b18/e3c/e1c/b18e3ce1ccb6c280df3e79f5602d021d.jpg"><br><br>  Here is Vadim Antonov and Sergei Vakulenko in 1990, with the first computer in the USSR connected to the Internet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/355/d81/85d/355d8185d7d493c7423db256c2db02d1.jpg"><br><br>  But on the right is Larry Hudepohl (Hudepol is written in Russian?).  Larry began his career at Digital Equipment Corporation (DEC) as a processor designer for MicroVAX.  Then Larry worked in a small company Cyrix, which in the late 1980s defied Intel and made an FPU co-processor that was compatible with Intel 80387 and was 50% faster than it.  Then Larry designed the MIPS chips in Silicon Graphics.  When MIPS Technologies separated from Silicon Graphics, Larry and Ryan Kinter together started the first independent MIPS product - MIPS 4K, which became the basis of the line that dominates the 2000s home electronics (DVD players, cameras, digital TVs).  Then MIPS 5K flew into space - it was used by the Japanese space agency JAXA.  Then Larry in the position of VP Hardware Engineering led the development of the following lines, and is now working on new Wave Accelerator architectures. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3c4/034/18a/3c403418a02c7b913f7503333ff8f467.jpg"><br><br>  The Japanese spacecraft with the proud name of Hayabusa-2 (Sapsan-2), which <a href="https://www.bbc.com/russian/news-45669705">landed on the surface of Ryugu asteroid last year</a> , is controlled by an HR5000 processor based on the MIPS 5Kf processor core licensed by MIPS Technologies for a long time. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/X3ZypNcJPx4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Here is a simple serial pipeline of the 64-bit MIPS 5Kf processor core from its <a href="https://www.digchip.com/datasheets/parts/datasheet/303/MIPS645KF-pdf.php">datasheet</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3d6/e4a/af7/3d6e4aaf717ec14beeffcb819751635c.png"><br><br>  Right on the photo - Darren Jones, Darren Jones.  He was the director of hardware engineering at MIPS in charge of the development of complex cores, with hardware multithreading, and superscalars with the extraordinary execution of instructions.  Then Darren went to Xilinx, where he studied Xilinx Zynq - chips, which are based on a combination of FPGAs and ARM processors.  Darren has now become vice president of engineering at Wave. <br><br>  At MIPS, Darren was the leader of the group, whose members then went to work for Apple and Samsung.  Designer Monica, who went to Samsung, once told me a phrase that I remembered well: ‚ÄúRTL design: development of equipment at the level of register transmissions: a few simple principles, everything else is muhlezh‚Äù) A canonical example of a cache is a cache (the program recorded data and read it, but it will only be remembered sometime later), but this is only a very special case of what Monica was able to do. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/156/d3b/e04/156d3be04866e0fb1b7dab56bb79920a.jpg"><br><br>  Hardware multithreading and an extraordinary superscalar are two different approaches to improving processor performance.  Hardware multithreading allows you to increase throughput without much power consumption, but with non-trivial programming.  The superscalar allows you to perform single-threaded programs roughly twice as fast, but it also spends twice as many watts.  But without programming tricks. <br><br>  Hardware multithreading is finally well explained in the Russian Wikipedia, here is its <a href="https://ru.wikipedia.org/wiki/Temporal_multithreading">temporary multithreading</a> (it is implemented in MIPS interAptiv and MIPS I7200 Shaolin), but <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B4%25D0%25BD%25D0%25BE%25D0%25B2%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%25D0%25BF%25D0%25BE%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">simultaneous multithreading</a> (it was done in the 1990s in DEC Alpha processors, then in SPARC, and then in MIPS I6400 Samurai / I6500 Daimyo). <br><br>  Temporary multithreading exploits the fact that a processor with a regular serial pipeline half the execution time is idle / waiting.  What is he waiting for?  Data that goes through caches from memory.  And it waits a long time - during the waiting time for one cache miss, the processor could execute dozens or even a hundred or two simple arithmetic instructions such as addition. <br><br>  This was not always the case - in the 1960s, arithmetic devices were much slower than memory.  But since about 1980, the speed of processor cores has grown much faster than memory speed, and even the appearance of multi-level caches on processors solved the problem only partially. <br><br>  Processors with temporal multithreading support several sets of registers, one for each thread, and when the current thread waits for data from the memory during a cache slip, the processor switches to another thread.  This happens instantly, in a cycle, without interruptions, and thousands of cycles of the interrupt handler, which is enabled when software (not hardware) multi-threading. <br><br>  Here's the idea of ‚Äã‚Äãmultithreading <a href="http://silicon-russia.com/public_materials/2016_11_04_one_day_mipsfpga_connected_mcu_materials_public_for_the_website/04_present_nanometer_asic_seminar/dna.rus.2016.MIPS.pdf">on slides from the seminars of Charles Danchek</a> , a teacher from the University of California Santa Cruz, Silicon Valley Extension.  Why in Russian?  Because Charles Danchek gave lectures in Moscow MISiS, then in St. Petersburg ITMO and in Kiev KPI: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e61/962/02d/e6196202d6ff12b8259edf30a7560ab5.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b1f/7cf/851/b1f7cf8519a6ad1e77fded74d53db1e7.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/423/ae6/0ff/423ae60ff5eb513c1368d8d35e989164.png"><br><br>  Interestingly, hardware-multithreading can be programmed simply in C.  Here's what it looks like: <br><br><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"mips/m32c0.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"mips/mt.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"mips/mips34k.h"</span></span></span><span class="hljs-meta"> </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">// –≠—Ç–æ –º–∞–∫—Ä–æ –Ω–∞ –°–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GNU—à–Ω—ã–µ —à—Ç—É—á–∫–∏, // –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—Å—Ç–∞–≤–ª—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä—è–º–æ –≤ –∞—Å—Å–µ–º–±–ª–∏—Ä—É–µ–º—ã–π –∫–æ–¥. // –° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –ø–æ—Ç–æ–∫ (thread) –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å —Å–≤–æ–π ID. // –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é FORK. –î–∞, —ç—Ç–æ –æ–¥–Ω–∞ 32-–±–∏—Ç–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è! #define mips_mt_fork_and_pass_param(thread_function, param) \ __extension__ \ ({ \ void * __thread_function = (thread_function); \ unsigned __param = (param); \ \ __asm__ __volatile \ ( \ ".set push; .set mt; fork $4,%0,%z1; .set pop" \ : : "d" (__thread_function), "dJ" (__param) \ ); \ }) void thread (unsigned tc) { // –¢—É—Ç –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ. // –ü–æ—Ç–æ–∫–∏ –º–æ–≥—É—Ç –æ–±–º–µ–Ω–∏–≤–∞—Ç—å—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π // —á–µ—Ä–µ–∑ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ-—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ FIFO –∏ –æ–±—â—É—é –ø–∞–º—è—Ç—å, // –∞ —Ç–∞–∫–∂–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã–º–∏ —Å–µ–º–∞—Ñ–æ—Ä–∞–º–∏. } int main () { // –ú–∞–∫—Ä–æ—Å—ã —á—Ç–æ–±—ã –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∞–ø–ø–∞—Ä–∞—Ç–Ω—É—é –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å for (tc = 0; tc &lt; NUM_TCS; tc++) { mips32_setvpecontrol (VPECONTROL_TE | tc); u = mips32_mt_gettcstatus (); mips32_mt_settcstatus (u | TCSTATUS_DA); mips32_mt_settchalt (0); } mips_mt_emt (); // –ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ—Å–µ–º—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –æ—Å—Ç–∞–µ–º—Å—è –≤ –¥–µ–≤—è—Ç–æ–º for (int tc = 1; tc &lt; NUM_TCS; tc ++) mips_mt_fork_and_pass_param (thread, tc); thread (0); }</span></span></span></span></code> </pre> <br><br>  Here is a Wave device for data centers on the side of the party.  It does not work yet, although the chips are available to some customers as part of a beta program: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/541/022/fe0/541022fe039005728fc94fc043e47d97.jpg"><br><br>  What does this device do?  Can you program on Python?  Here on Python, you can build the so-called Data Flow Graph (DFG) calls to the TensorFlow library.  Neural networks are essentially specialized graphs like these, with matrix operations.  In the Wave software group, part of which Steve Johnson manages, there is a compiler with a subset of the representation of Google‚Äôs TensorFlow in the configuration files for the chips of this device.  After configuration, it can make calculations of such graphs very quickly.  The device is intended for data centers, but the same principle can be applied to small chips, even inside mobile devices, for example, for face recognition: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c39/ce9/c71/c39ce9c71e3145943ffd25c0479ce423.png"><br><br>  Chijioke Anyanwu (left) - has been the custodian of the entire MIPS processor core testing system for many years.  Baldwyn Chieh (center) is the designer of a new generation of processor-like elements in Wave.  Baldwin used to be a senior designer at Qualcomm.  Here are <a href="https://www.hotchips.org/wp-content/uploads/hc_archives/hc29/HC29.22-Tuesday-Pub/HC29.22.60-NeuralNet1-Pub/HC29.22.610-Dataflow-Deep-Nicol-Wave-07012017.pdf">slides about the Wave device from the HotChips conference</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/745/cdf/d34/745cdfd34041fd390d2a783967e98b2f.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a04/730/b9b/a04730b9ba3808736d5328ae1d605ffb.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/3ce/587/752/3ce587752ceaed774217b708a79d3fc0.png"><br><br>  In each nanometer digital innovative AI company in Silicon Valley, there should be a girl with bright hair.  Here is a girl in Wave.  Her name is Athena, she is a sociologist by training, and is engaged in the office office: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/17a/867/013/17a86701329a91caca858bd3aa911f56.jpg"><br><br>  But what an office looks like from the outside, and its more than century-old history from the time when it was an innovative canning factory: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa6/e29/e26/fa6e29e2675c6f16816314521203a486.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ed/8d4/de2/0ed8d4de215a0eb167ca3955c7c64a5d.png"><br><br>  And now the question: how to understand the architecture, micro-architecture, digital circuitry, the principles of design of AI chips and participate in such parties?  The easiest way is to study the textbook ‚ÄúDigital Circuit Design and Computer Architecture‚Äù by David Harris and Sarah Harris, and go to Wave Computing for the summer as an intern (it is planned to hire 15 trainees for the summer).  I hope that this can also be done in Russian microelectronic companies that are engaged in similar developments - ELVIS, Milandr, Baikal Electronics, IVA Technologies and several others.  In Kiev, this can theoretically be done in cooperation with the KPI company Melexis. <br><br>  Recently, a new, finally revised version of the Harris &amp; Harris textbook has been published, which should be here for free at <a href="https://www.mips.com/downloads/digital-design-and-computer-architecture-russian-edition-second-edition-ver3">www.mips.com/downloads/digital-design-and-computer-architecture-russian-edition-second-edition-ver3</a> , but my link does not work, and when it works, I will write a separate post about it.  With questions asked for interviews at Apple, Intel, AMD, and on which pages of this tutorial (and other sources) you can see the answers. </div><p>Source: <a href="https://habr.com/ru/post/438928/">https://habr.com/ru/post/438928/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>