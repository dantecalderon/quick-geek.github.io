<div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/fg/9j/5c/fg9j5ckv7p926poj_vtplcph3lg.jpeg"><br><br>  The earth is actually flat, the Americans never landed on the moon, and the world is ruled by a secret government - the <a href="https://habr.com/ru/post/412101/">most popular conspiracy theories are</a> very easy to find on YouTube.  Because of the <a href="https://habr.com/ru/post/132191/">filter bubble</a> , <a href="https://en.wikipedia.org/wiki/Conservatism_(belief_revision)">psychological conservatism</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25B2%25D0%25BE%25D1%2581%25D0%25BF%25D1%2580%25D0%25B8%25D1%258F%25D1%2582%25D0%25B8%25D0%25B5">selective perception</a> and the <a href="https://en.wikipedia.org/wiki/Illusory_truth_effect">effect of the illusion of truth, a</a> person searches the Internet for information that confirms his point of view, and tends to ignore new information if it contradicts established beliefs. <br><br>  YouTube intends to overcome this unpleasant property of human nature.  On Friday, the largest video service on the Internet <a href="https://youtube.googleblog.com/2019/01/continuing-our-work-to-improve.html">announced</a> that it plans to exclude video with conspiracy theories from the list of recommendations. <br><a name="habracut"></a><br>  YouTube <a href="https://www.wsj.com/articles/how-youtube-drives-viewers-to-the-internets-darkest-corners-1518020478">has been criticized</a> for <a href="https://www.wsj.com/articles/how-youtube-drives-viewers-to-the-internets-darkest-corners-1518020478">many years</a> for recommending videos that spread disinformation.  Now the policy is somewhat relaxed.  On January 25, 2019, YouTube’s official blog announced that it would no longer offer videos with “border content” and which “misinform users in a harmful way” even if the video itself does not contradict the recommendations of the community and cannot be deleted. <br><br>  YouTube says the policy change affected less than 1% of all video clips on the platform.  But with the billions of entries in the YouTube library, that's a really large number. <br><br>  Recently, Facebook, YouTube, Twitter, and other UGC platforms have come across serious criticism for helping to spread misinformation, fakes, conspiracy theories, and other viral information, which by its nature is easily distributed in social networks.  There is a double situation.  On the one hand, censorship of user-generated content contradicts fundamental human rights.  On the other hand, no one forbids people to express their opinions, but the platform also has the right to apply its own rules and at least not to help spread fakes, even if it brings profit (number of views, amount of advertising, etc.). <br><br>  Social media are forced to listen to these requirements.  For example, Facebook recently <a href="https://www.zdnet.com/article/facebook-removes-propaganda-network-linked-to-russian-media-group-sputnik/">banned hundreds of accounts of Sputnik agency employees</a> , who were engaged in propaganda in the Baltic States and Eastern Europe.  Similar actions are taken by Twitter, <a href="https://twitter.com/TwitterSafety/status/1032055161978585088">deleting accounts that are used for coordinated manipulation of public opinion</a> . <br><br>  The main complaint about social media is that they recommend dubious content, even if users clearly do not express interest in it.  For example, recently YouTube, for no apparent reason, recommended millions of users to watch a videotape of the September 11 attacks.  He is also accused of widening the political split in the country, pushing the already biased viewers to more extreme points of view.  Studies show that YouTube’s algorithms really <a href="https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html">systematically recommends to a person more and more extreme videos</a> on a topic he has been interested in before.  Probably, the recommendations of algorithms during machine learning came to this behavior - perhaps this is a really effective strategy that maximizes the number of views.  But this is how YouTube turned into “one of the most powerful tools for radicalizing society in the 21st century,” <a href="https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html%3Fsmid%3Dpl-share%26module%3Dinline">critics write</a> . <br><br>  The new policy is also the latest example of a more aggressive YouTube approach to content that many consider unpleasant, even if it does not violate the <a href="https://www.youtube.com/yt/about/policies/">Community Rules</a> . <br><br>  At the end of 2017, YouTube moved “controversial religious or suprematic” content to a “limited state” (limited state), in which videos are not monetized by advertising, and comments and likes are disabled.  Some videos are accompanied by a short message that they may be inappropriate or offensive. <br><br>  YouTube has named three examples of videos that he will no longer recommend: <br><br><ul><li>  that promote a fake miracle cure for a serious illness; <br></li><li>  who claim that the earth is flat; <br></li><li>  content that makes plainly false statements about historical events, such as the attacks of September 11th. </li></ul><br>  Of course, this is not enough to completely filter misinformation, but with censorship you need to be careful, because it is a double-edged weapon. <br><br>  YouTube will still recommend dubious videos to users who have clearly subscribed to this channel and will leave misinformation in the search results: “We believe that this change sets the balance between maintaining freedom of speech and respecting our responsibility to users,” YouTube wrote in an official blog. </div>