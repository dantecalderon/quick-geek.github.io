<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>BERT is a state-of-the-art language model for 104 languages. BERT launching tutorial locally and on Google Colab</title>
  <meta name="description" content="BERT is a neural network from Google, which showed by a wide margin state-of-the-art results on a number of tasks. With BERT, you can create programs ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-6974184241884155",
      enable_page_level_ads: true
    });
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>BERT is a state-of-the-art language model for 104 languages. BERT launching tutorial locally and on Google Colab</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/getpro/habr/post_images/2bd/0ba/1c4/2bd0ba1c4fb80fe4d771f555168c9ff0.png" alt="image"></p><br><p>  BERT is a neural network from Google, which showed by a wide margin state-of-the-art results on a number of tasks.  With BERT, you can create programs with AI for natural language processing: answer questions posed in an arbitrary form, create chat bots, automatic translators, analyze text, and so on. </p><br><p>  Google has posted pre-trained BERT models, but as is usually the case in Machine Learning, they suffer from a lack of documentation.  Therefore, in this tutorial we will learn how to run the BERT neural network on a local computer, as well as on a free server GPU on Google Colab. </p><a name="habracut"></a><br><h2 id="zachem-eto-voobsche-nuzhno">  Why do you need it </h2><br><p>  To feed the text of the neural network, you need to somehow represent it in the form of numbers.  The easiest way to do this is letter by letter, giving one letter to each input of the neural network.  Then each letter will be encoded with a number from 0 to 32 (plus some margin for punctuation marks).  This is the so-called character-level. </p><br><p>  But much better results are obtained if we present the sentences not in one letter, but by feeding the neural network to each input at once by the whole word (or at least in syllables).  It will already be word-level.  The easiest option is to create a dictionary with all the existing words, and feed the network the number of the word in this dictionary.  For example, if the word "dog" is in this dictionary at 1678 place, then the input of the neural network for this word is given the number 1678. </p><br><p>  That's just in a natural language with the word "dog" in a person many associations immediately pop up: "furry", "evil", "man's friend".  Is it possible to somehow encode this feature of our thinking in a representation for a neural network?  It turns out you can.  To do this, it is enough to re-sort the numbers of words so that the words close in meaning would stand nearby.  Let there be, for example, 1678 for "dog", and 1680 for the word "fluffy". And 9000 for the word "teapot". As you can see, the numbers 1678 and 1680 are much closer to each other than the 9000. </p><br><p>  In practice, each word is assigned not one number, but several - a vector of, say, 32 numbers.  And distances are measured as distances between points pointed to by these vectors in the space of the corresponding dimension (for a vector of 32 numbers in length, this is a space with 32 dimensions, or with 32 axes).  This allows you to match one word at once with several words that are close in meaning (depending on which axis to count).  Moreover, arithmetic operations can be performed with vectors.  A classic example: if from a vector denoting the word "king", subtract the vector "man" and add a vector for the word "woman", you get a certain vector-result.  And it will miraculously correspond to the word "queen".  And indeed, "the king is a man + woman = queen".  Magic!  And this is not an abstract example, but <a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">actually it happens</a> .  Considering that neural networks are well adapted for mathematical transformations over their inputs, apparently this provides such high efficiency of this method. </p><br><p>  This approach is called Embeddings.  All machine learning packages (TensorFlow, PyTorch) allow the first layer of the neural network to put a special Embedding Layer layer that does this automatically.  That is, at the input of the neural network, we give the usual number of a word in the dictionary, and the Embedding Layer, self-learning, translates each word into a vector of the specified length, say, 32 numbers. </p><br><p>  But they quickly realized that it was much more profitable to train in advance such a vector representation of words on some huge corpus of texts, for example, on the whole Wikipedia, and to use ready-made word vectors in specific neural networks, rather than teaching them every time anew. </p><br><p>  There are several ways to represent words by vectors; they gradually evolve: word2vec, GloVe, Elmo. </p><br><p>  In the summer of 2018, <a href="https://blog.openai.com/language-unsupervised/">OpenAI noticed</a> that if you pre-teach a neural network on the <a href="https://arxiv.org/abs/1706.03762">Transformer</a> architecture on large volumes of text, it will unexpectedly and by a large margin show excellent results on a variety of different tasks in the processing of natural language.  In fact, such a neural network at its output creates vector representations for words, and even whole phrases.  And hanging on top of such a language model a small block of a pair of additional layers of neurons, you can train this neural network for any task. </p><br><p>  Google's BERT is an enhanced OpenAI GPT network (bidirectional instead of unidirectional, etc.), also on the Transformer architecture.  At the moment, BERT is a state-of-the-art on almost all popular NLP benchmarks. </p><br><h2 id="kak-oni-eto-sdelali">  How did they do it </h2><br><p>  The idea behind BERT is very simple: let's input phrases to the input of a neural network, in which 15% of words are replaced by [MASK], and teach the neural network to predict these masked words. </p><br><p>  For example, if we feed the phrase ‚ÄúI came to [MASK] and bought [MASK] to the input of the neural network, it should show the words‚Äú store ‚Äùand‚Äú milk ‚Äùat the exit.  This is a simplified example from the official BERT page; on longer sentences, the scatter of possible options becomes less, and the answer of the neural network is unambiguous. </p><br><p>  And in order for the neural network to learn to understand the relationship between different sentences, we will additionally teach it to predict whether the second phrase is a logical continuation of the first.  Or is it some kind of random phrase that has nothing to do with the first. </p><br><p>  So, for two sentences: "I went to the store."  and "And bought milk there.", the neural network should answer that this is logical.  And if the second phrase is "Cruc's sky Pluto," then it must respond that this sentence is not related to the first.  Below we will play around with both of these BERT modes of operation. </p><br><p>  Having thus trained the neural network on the corpus of texts from Wikipedia and the BookCorpus book collection for 4 days on 16 TPU, we got BERT. </p><br><h2 id="ustanovka-i-nastroyka">  Installation and Setup </h2><br><p>  <em><strong>Note</strong> : in this section we will start and play with BERT on the local computer.</em>  <em>To run this neural network on a local GPU, you will need an NVidia GTX 970 with 4 GB of video memory or higher.</em>  <em>If you just want to run BERT in a browser (you don't even need a GPU on your computer for this), go to the Google Colab section.</em> </p><br><p>  First of all, install TensorFlow, if you don‚Äôt have one yet, follow the instructions from <a href="https://www.tensorflow.org/install/">https://www.tensorflow.org/install</a> .  To support GPU, you must first install CUDA Toolkit 9.0, then cuDNN SDK 7.2, and only then itself TensorFlow with GPU support: </p><br><pre><code class="dos hljs">pip install tensorflow-gpu</code> </pre> <br><p>  Basically, this is enough to run BERT.  But there is no instruction as such; you can compose it yourself by <a href="https://github.com/google-research/bert/blob/master/run_classifier.py">sorting</a> out the source code in the <a href="https://github.com/google-research/bert/blob/master/run_classifier.py">run_classifier.py</a> file (the usual situation in Machine Learning, when instead of documentation you have to go into the source code).  But we will do it easier and use the <a href="https://github.com/CyberZHG/keras-bert">Keras BERT</a> shell (it may also come in handy for the fine-tuning network later, because it provides a convenient Keras interface). </p><br><p>  To do this, install Keras himself: </p><br><pre> <code class="dos hljs">pip install keras</code> </pre> <br><p>  And after Keras BERT: </p><br><pre> <code class="dos hljs">pip install keras-bert</code> </pre> <br><p>  We also need the <a href="https://github.com/google-research/bert/blob/master/tokenization.py">tokenization.py</a> file from the original github BERT.  Either click the Raw button and save it to the folder with the future script, or download the entire repository and take the file from there, or take a copy from the repository with this code <a href="https://github.com/blade1780/bert">https://github.com/blade1780/bert</a> . </p><br><p>  Now it's time to download the pre-trained neural network.  There are several options for BERT, all of which are listed on the official page <a href="https://github.com/google-research/bert">github.com/google-research/bert</a> .  We will take the universal multilingual "BERT-Base, Multilingual Cased" for 104 languages.  Download the <a href="">multi_cased_L-12_H-768_A-12.zip</a> file (632 MB) and unzip it in the folder with the future script. </p><br><p>  Everything is ready, create a file BERT.py, then there will be a bit of code. </p><br><p>  Import required libraries and set paths </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding: utf-8 import sys import codecs import numpy as np from keras_bert import load_trained_model_from_checkpoint import tokenization # –ø–∞–ø–∫–∞, –∫—É–¥–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–ª–∏ –ø—Ä–µ–æ–¥–æ–±—É—á–µ–Ω–Ω—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å BERT folder = 'multi_cased_L-12_H-768_A-12' config_path = folder+'/bert_config.json' checkpoint_path = folder+'/bert_model.ckpt' vocab_path = folder+'/vocab.txt'</span></span></code> </pre> <br><p>  Since we have to translate the usual text strings into a special token format, we will create a special object for this.  Pay attention to do_lower_case = False, since we use the Cased BERT model, which is case sensitive. </p><br><pre> <code class="python hljs">tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br><p>  Load the model </p><br><pre> <code class="python hljs">model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.summary()</code> </pre> <br><p>  BERT can work in two modes: guess the words missing in the phrase, or guess whether the second phrase is logically following the first.  We will do both. </p><br><p>  For the first mode, the input to the neural network is to submit a phrase in the format: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>] –Ø –ø—Ä–∏—à–µ–ª –≤ [MASK] –∏ –∫—É–ø–∏–ª [MASK]. [SEP]</code> </pre> <br><p>  The neural network should return the full sentence with filled words on the site of the masks: "I came to the store and bought milk." </p><br><p>  For the second mode, both phrases separated by a separator must be submitted to the input of the neural network: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>] –Ø –ø—Ä–∏—à–µ–ª –≤ –º–∞–≥–∞–∑–∏–Ω. [SEP] –ò –∫—É–ø–∏–ª –º–æ–ª–æ–∫–æ. [SEP]</code> </pre> <br><p>  The neural network must answer whether the second phrase is a logical continuation of the first.  Or is it a random phrase that has nothing to do with the first. </p><br><p>  For BERT to work, you need to prepare three vectors, each with a length of 512 numbers: token_input, seg_input and mask_input. </p><br><p>  The <strong>token_input</strong> will store our source code, translated into tokens using tokenizer.  The phrase in the form of indices in the dictionary will be at the beginning of this vector, and the rest will be filled with zeros. </p><br><p>  In <strong>mask_input,</strong> we have to set 1 for all positions where the [MASK] mask is located, and fill in the rest with zeros. </p><br><p>  In <strong>seg_input,</strong> we must <strong>denote the</strong> first phrase (including the initial CLS and the SEP separator) as 0, the second phrase (including the final SEP) denote as 1, and fill in the rest of the vector with zeros. </p><br><p>  BERT does not use a dictionary of whole words, but rather of the most common syllables.  Although the whole words in it too.  You can open the vocab.txt file in the downloaded neural network and see what words the neural network uses at its input.  There are like whole words, like "France".  But most Russian words need to be broken down into syllables.  So, the word "came" should be broken into "when" and "## went."  To help convert regular text strings to the format required for BERT, we will use the tokenization.py module. </p><br><h2 id="rezhim-1-predskazanie-slov-zakrytyh-tokenom-mask-v-fraze">  Mode 1: prediction of words covered by the token [MASK] in the phrase </h2><br><p>  The input phrase, which is fed to the input of the neural network </p><br><pre> <code class="python hljs">sentence = <span class="hljs-string"><span class="hljs-string">'–Ø –ø—Ä–∏—à–µ–ª –≤ [MASK] –∏ –∫—É–ø–∏–ª [MASK].'</span></span> print(sentence)</code> </pre> <br><p>  Convert it to tokens.  The problem is that tokenizer cannot handle service marks like [CLS] and [MASK], although they are in the vocab.txt dictionary.  Therefore, we will have to manually split our string with [MASK] markers and extract pieces of plain text from it in order to convert it to BERT tokens using tokenizer.  And also add [CLS] to the beginning and [SEP] to the end of the phrase. </p><br><pre> <code class="python hljs">sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">'[MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK]'</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã sentence = sentence.split('[MASK]') # —Ä–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ –º–∞—Å–∫–µ tokens = ['[CLS]'] # —Ñ—Ä–∞–∑–∞ –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–Ω–∞ –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è –Ω–∞ [CLS] # –æ–±—ã—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–æ–∫–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é tokenizer.tokenize(), –≤—Å—Ç–∞–≤–ª—è—è –º–µ–∂–¥—É –Ω–∏–º–∏ [MASK] for i in range(len(sentence)): if i == 0: tokens = tokens + tokenizer.tokenize(sentence[i]) else: tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) tokens = tokens + ['[SEP]'] # —Ñ—Ä–∞–∑–∞ –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–Ω–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –Ω–∞ [SEP]</span></span></code> </pre> <br><p>  In tokens now tokens, which are guaranteed by the dictionary are converted into indices.  Let's do it: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens)</code> </pre> <br><p>  Now in token_input comes a series of numbers (the numbers of words in the vocab.txt dictionary) that need to be fed to the input of the neural network.  It remains only to extend this vector to the length of 512 elements.  Python construct [0] * length creates an array of length length, filled with zeros.  We simply add it to our tokens, which in the python combines two arrays into one. </p><br><pre> <code class="python hljs">token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Now we create a mask with a mask of 512 length, putting everywhere 1, where the number 103 appears in tokens (which corresponds to the marker [MASK] in the vocab.txt dictionary), and filling the rest with 0: </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(mask_input)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token_input[i] == <span class="hljs-number"><span class="hljs-number">103</span></span>: mask_input[i] = <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  For the first BERT mode of operation, seg_input must be completely filled with zeros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  The last step is to convert the python arrays into numpy arrays of c shape (1,512), for which we put them in subarray []: </p><br><pre> <code class="python hljs">token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</code> </pre> <br><p>  Ok, done.  Now we run the neural network prediction! </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">0</span></span>] predicts = np.argmax(predicts, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicts = predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][:len(tokens)] <span class="hljs-comment"><span class="hljs-comment"># –æ—Ç—Ä–µ–∑–∞–µ–º –Ω–∞—á–∞–ª–æ —Ñ—Ä–∞–∑—ã, –¥–ª–∏–Ω–æ–π –∫–∞–∫ –∏—Å—Ö–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞, —á—Ç–æ–±—ã –æ—Ç—Å–µ—á—å —Å–ª—É—á–∞–π–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã —Å—Ä–µ–¥–∏ –Ω—É–ª–µ–π –¥–∞–ª—å—à–µ</span></span></code> </pre> <br><p>  Now format the result from tokens back to the string, separated by spaces. </p><br><pre> <code class="python hljs">out = [] <span class="hljs-comment"><span class="hljs-comment"># –¥–æ–±–∞–≤–ª—è–µ–º –≤ out —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ [MASK], –∫–æ—Ç–æ—Ä—ã–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω—ã —Ü–∏—Ñ—Ä–æ–π 1 –≤ mask_input for i in range(len(mask_input[0])): if mask_input[0][i] == 1: # [0][i], —Ç.–∫. —Å–µ—Ç—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç batch —Å —Ñ–æ—Ä–º–æ–π (1,512), –≥–¥–µ –≤ –ø–µ—Ä–≤–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ –Ω–∞—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç out.append(predicts[i]) out = tokenizer.convert_ids_to_tokens(out) # –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã out = ' '.join(out) # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —Å—Ç—Ä–æ–∫—É —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ out = tokenization.printable_text(out) # –≤ —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç out = out.replace(' ##','') # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–∞–∑—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞: "–ø—Ä–∏ ##—à–µ–ª" -&gt; "–ø—Ä–∏—à–µ–ª"</span></span></code> </pre> <br><p>  And display the result: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Result:'</span></span>, out)</code> </pre> <br><p>  In our example, for the phrase "I came to [MASK] and bought [MASK]."  the neural network gave the result "house" and "it": "I came to the house and bought it."  Well, not so bad for the first time.  Buy a house is definitely better than milk). </p><br><div class="spoiler">  <b class="spoiler_title">Other examples (unsuccessful ones I don‚Äôt cite, there are many more than successful ones. In most cases the network gives just an empty answer)</b> <div class="spoiler_text"><p>  Earth is the third [MASK] from the sun <br>  Result: star </p><br><p>  sandwich best [MASK] with butter <br>  Result: found </p><br><p>  after [MASK] lunch is supposed to sleep <br>  Result: this </p><br><p>  get away from [MASK] <br>  Result: ## oh - is that some kind of curse word?  ) </p><br><p>  [MASK] from the door <br>  Result: view </p><br><p>  With [MASK] a hammer and nails, you can make a cupboard <br>  Result: care </p><br><p>  And if tomorrow is not?  Today, for example, its not [MASK]! <br>  Result: will </p><br><p>  How can annoy you to ignore [MASK]? <br>  Result: her </p><br><p>  There is a household logic, there is a female logic, but nothing is known about the male [MASK] <br>  Result: Philosophy </p><br><p>  For women by the age of thirty, the image of a prince is formed, under which any [MASK] is suitable. <br>  Result: man </p><br><p>  With a majority of votes, Snow White and the Seven Dwarfs voted for [MASK], with one against. <br>  Result: village - the first letter is correct </p><br><p>  Rate the degree of your tediousness on a 10-point scale: [MASK] points <br>  Result: 10 </p><br><p>  Yours [MASK], [MASK] and [MASK]! <br>  Result: love, I - no, BERT, I didn't mean that at all </p></div></div><br><p>  You can also enter English phrases (and any in 104 languages, a list of which <a href="">is here</a> ) </p><br><p>  [MASK] must go on! <br>  Result: I </p><br><h2 id="rezhim-2-proverka-logichnosti-dvuh-fraz">  Mode 2: checking the logic of two phrases </h2><br><p>  We set two consecutive phrases that will be submitted to the input of the neural network. </p><br><pre> <code class="python hljs">sentence_1 = <span class="hljs-string"><span class="hljs-string">'–Ø –ø—Ä–∏—à–µ–ª –≤ –º–∞–≥–∞–∑–∏–Ω.'</span></span> sentence_2 = <span class="hljs-string"><span class="hljs-string">'–ò –∫—É–ø–∏–ª –º–æ–ª–æ–∫–æ.'</span></span> print(sentence_1, <span class="hljs-string"><span class="hljs-string">'-&gt;'</span></span>, sentence_2)</code> </pre> <br><p>  Let's create tokens in the format [CLS] phrase_1 [SEP] phrase_2 [SEP], transforming plain text into tokens using tokenizer: </p><br><pre> <code class="python hljs">tokens_sen_1 = tokenizer.tokenize(sentence_1) tokens_sen_2 = tokenizer.tokenize(sentence_2) tokens = [<span class="hljs-string"><span class="hljs-string">'[CLS]'</span></span>] + tokens_sen_1 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>] + tokens_sen_2 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>]</code> </pre> <br><p>  We convert string tokens into numeric indices (the numbers of words in the vocab.txt dictionary) and extend the vector to 512: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens) token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  The word mask in this case is completely filled with zeros. </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  But you need to fill the mask of sentences with the second phrase (including the final SEP) with units, and everything else with zeros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> len_1 = len(tokens_sen_1) + <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-comment"><span class="hljs-comment"># –¥–ª–∏–Ω–∞ –ø–µ—Ä–≤–æ–π —Ñ—Ä–∞–∑—ã, +2 - –≤–∫–ª—é—á–∞—è –Ω–∞—á–∞–ª—å–Ω—ã–π CLS –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å SEP for i in range(len(tokens_sen_2)+1): # +1, —Ç.–∫. –≤–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–π SEP seg_input[len_1 + i] = 1 # –º–∞—Å–∫–∏—Ä—É–µ–º –≤—Ç–æ—Ä—É—é —Ñ—Ä–∞–∑—É, –≤–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–π SEP, –µ–¥–∏–Ω–∏—Ü–∞–º–∏ # –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –≤ —Ñ–æ—Ä–º—É (1,) -&gt; (1,512) token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</span></span></code> </pre> <br><p>  Pass the phrases through the neural network (this time the result is in [1], and not in [0], as it was above) </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br><p>  And we derive the probability that the second phrase is normal, and not a random set of words </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Sentence is okey:'</span></span>, int(round(predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>)), <span class="hljs-string"><span class="hljs-string">'%'</span></span>)</code> </pre> <br><p>  In two phrases: </p><br><p>  I came to the store.  -&gt; And bought milk. </p><br><p>  Neural network response: </p><br><p>  Sentence is okey: 99% </p><br><p>  And if the second phrase is "Cruc's sky Pluto", the answer will be: </p><br><p>  Sentence is okey: 4% </p><br><h2 id="google-colab">  Google colab </h2><br><p>  Google provides a free server GPU Tesla K80 with 12 Gb of video memory (TPU is also available now, but their configuration is a bit more complicated).  All code for Colab should be designed as jupyter notebook.  To launch BERT in a browser, simply open the link. </p><br><p>  <a href="http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb">http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb</a> </p><br><p>  In the <strong>Runtime</strong> menu, select <strong>Run All</strong> , so that all cells start for the first time, download the model and connect the necessary libraries.  Agree to reset all runtime, if required. </p><br><div class="spoiler">  <b class="spoiler_title">If something went wrong ...</b> <div class="spoiler_text"><p>  Make sure GPU and Python 3 are selected in the Runtime -&gt; Change runtime type menu </p><br><p>  If the connect button is not active, click it to become Connected. </p></div></div><br><p>  Now change the input lines <strong>sentence</strong> , <strong>sentence_1</strong> and <strong>sentence_2</strong> , and click the Play icon on the left to launch only the current cell.  Run the entire notebook is no longer necessary. </p><br><p>  You can launch BERT in Google Colab even from a smartphone, but if it does not open, you may need to enable the Full version checkbox in your browser settings. </p><br><h2 id="chto-dalshe">  What's next? </h2><br><p>  In order to train BERT for a specific task, it is necessary to add one or two layers of a simple Feed Forward network on top of it, and to train it only without touching the main BERT network.  This can be done either on the bare TensorFlow, or through the shell Keras BERT.  Such additional training for a specific domain occurs very quickly and is completely analogous to Fine Tuning in convolutional networks.  So, for the SQuAD task, you can train a neural network on a single TPU in just 30 minutes (compared to 4 days on 16 TPU to train BERT itself). </p><br><p>  To do this, you will have to study how the last layers are presented in BERT, and also have a suitable dataset.  On the official BERT page <a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a> there are several examples for different tasks, as well as instructions on how to run additional training on cloud TPU.  And everything else will have to look at the source in the files <a href="https://github.com/google-research/bert/blob/master/run_classifier.py">run_classifier.py</a> and <a href="https://github.com/google-research/bert/blob/master/extract_features.py">extract_features.py</a> . </p><br><h3 id="ps">  PS </h3><br><p>  The code presented here and jupyter notebook for Google Colab <a href="https://github.com/blade1780/bert"><strong>are located in the repository</strong></a> . </p><br><p>  Miracles should not wait.  Do not expect BERT to speak as a person.  The status of state-of-the-art does not mean that progress in NLP has reached an acceptable level.  This only means that BERT is better than previous models, which were even worse.  A strong conversational AI is still very far away.  In addition, BERT is primarily a language model, and not a ready-made chat bot, so it shows good results only after additional training for a specific task. </p></div><p>Source: <a href="https://habr.com/ru/post/436878/">https://habr.com/ru/post/436878/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>