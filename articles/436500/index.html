<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to render a frame of Rise of the Tomb Raider</title>
  <meta name="description" content="Rise of the Tomb Raider (2015) is a sequel to the excellent restart of Tomb Raider (2013). Personally, I find both parts interesting because they move...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-6974184241884155",
      enable_page_level_ads: true
    });
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>How to render a frame of Rise of the Tomb Raider</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/1k/5p/s6/1k5ps6qnv2soku_ymxsemz7wnxa.gif"></div><br>  Rise of the Tomb Raider (2015) is a sequel to the excellent restart of Tomb Raider (2013).  Personally, I find both parts interesting because they moved away from the stagnant original series and told the story of Lara again.  In this game, as in the prequel, the plot is central, it provides fascinating mechanics of crafting, hunting and climbing / exploration. <br><br>  In Tomb Raider, the Crystal Engine developed by Crystal Dynamics was used, also used in <a href="http://www.adriancourreges.com/blog/2015/03/10/deus-ex-human-revolution-graphics-study/">Deus Ex: Human Revolution</a> .  The sequel used a new engine called Foundation, previously developed for Lara Croft and the Temple of Osiris (2014).  Its rendering can be generally described as a tile engine with a preliminary light pass, and later we will find out what this means.  The engine allows you to choose between DX11 and DX12 renderers;  I chose the latter, for the reasons we discuss below.  <a href="https://renderdoc.org/">Renderdoc</a> 1.2 on Geforce 980 Ti was used to capture the frame, the game included all the functions and decorations. <br><br><h2>  Analyzed frame </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/254/df9/0bb/254df90bbbfd5498710f37624e12d1f4.jpg"></div><br>  To avoid spoilers, I‚Äôll say that in this frame the bad guys pursue Lara, because she is looking for an artifact that they are searching for.  This conflict of interest does not resolve without weapons.  Lara made her way to the enemy base at night.  I chose a frame with atmospheric and contrasting lighting, in which the engine can show itself. <a name="habracut"></a><br><br><h4>  Preliminary depth pass </h4><br>  This is where the usual for many games optimization is performed - a small preliminary depth pass (approximately 100 draw calls).  The game renders the largest objects (and not those that take up more space on the screen) to take advantage of the Early-Z video processors.  Read more about it in <a href="https://software.intel.com/en-us/articles/early-z-rejection-sample">an Intel article</a> .  In short, GPUs are able to avoid executing a pixel shader, if they can determine that it is covered by the previous pixel.  This is a fairly low-cost pass, pre-filling the Z-buffer with depth values. <br><br>  At this stage, I discovered an interesting technique of level of detail (LOD) called ‚Äúfizzle‚Äù or ‚Äúcheckerboard‚Äù.  This is a common way of gradually displaying or hiding objects at a distance in order to later either replace them with a lower-quality mesh, or completely hide them.  Look at this truck.  It seems that it is rendered twice, but in fact it is rendered with a high LOD and a low LOD in the same position.  Each of the levels renders those pixels that the other did not render.  The first LOD has 182226 vertices, and the second LOD has 47250. They are indistinguishable at a great distance, but one of them is three times less expensive.  In this frame, LOD 0 almost disappears, and LOD 1 is rendered almost completely.  After the complete disappearance of LOD 0, only LOD 1 will be rendered. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c31/d8d/24e/c31d8d24e58229c27fb4edf6f635974d.png"></div><br>  <i>LOD 0</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7bf/3b4/5bb/7bf3b45bbe13521b4bae010f72a702a7.png"></div><br>  <i>LOD 1</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/7r/it/jy7rit_bwqm4n0lwpzzlydyx5rs.gif"></div><br>  The pseudo-random texture and coefficient of probability allows us to discard pixels that do not pass the threshold value.  This texture is used in the ROTR.  One may wonder why not use alpha blending.  Alpha blending has many drawbacks compared to fizzle fading. <br><br><ol><li>  <strong>Convenience for the preliminary passage of the depths:</strong> thanks to the rendering of an opaque object with holes made in it, we can render in the preliminary passage and use early-z.  Objects with alpha blending at this early stage are not rendered to the depth buffer due to sorting problems. </li><li>  <strong>The need for additional shaders</strong> : if a deferred renderer is used, then the shader of opaque objects does not contain any lighting.  If you need to replace an opaque object with a transparent one, then you need a separate option that has lighting.  In addition to increasing the amount of memory required and the complexity due to at least one additional shader for all non-transparent objects, they must be accurate in order to avoid objects from protruding.  This is difficult for many reasons, but it all comes down to the fact that rendering is now performed along a different code path. </li><li>  <strong>More redrawing</strong> : alpha blending can create a large redraw and, at a certain level of object complexity, a large portion of the bandwidth may be required to shade the LOD. </li><li>  <strong>Z-conflicts</strong> : Z-conflicts are a <a href="https://youtu.be/9AcCrF_nX-I">blink effect</a> , when two polygons are rendered at a very close depth to each other.  In this case, the inaccuracy of floating point calculations causes them to be rendered in turn.  If we render two consecutive LODs, gradually hiding one and showing the second, they can cause a z-conflict, because they are very close to each other.  There are always ways to get around this, for example, preferring one polygon to another, but such a system is difficult. </li><li>  <strong>Z-Buffer Effects</strong> : Many effects like SSAO use only a depth buffer.  If we render transparent objects at the end of the pipeline, when ambient occlusion is already done, we could not take it into account. </li></ol><br>  The disadvantage of this technique is that it looks worse than alpha blending, but a good noise pattern, blurring after a fizzle or temporal anti-aliasing can almost completely hide it.  In this regard, ROTR does not do anything particularly unusual. <br><br><h4>  Normal pass </h4><br>  Crystal Dynamics uses a rather unusual lighting scheme in its games, which we will look at in the lighting aisle.  For now, suffice it to say that the engine does not have a G-buffer pass;  at least to the extent that is common in other games.  On this pass, objects pass only depth and normal information to the output.  Normals are written to the RGBA16_SNORM format render target in global space.  It is curious that this engine uses the Z-up scheme, and not Y-up (the Z axis is directed upward, not Y), which is more often used in other engines / modeling packages.  The alpha channel contains glossiness (glossiness), which is further unpacked as <code>exp2(glossiness * 12 + 1.0)</code> .  The glossiness value can also be negative, because the sign is used as a flag indicating whether the surface is metallic.  This can be seen independently, because all the dark colors in the alpha channel refer to metal objects. <br><br><table><tbody><tr><td>  <strong><font color="#ff0000">R</font></strong> </td><td>  <strong><font color="#00ff00">G</font></strong> </td><td>  <strong><font color="0000ff">B</font></strong> </td><td></td></tr><tr><td>  <strong>Normal.x</strong> </td><td>  <strong>Normal.y</strong> </td><td>  <strong>Normal.z</strong> </td><td>  <b>Glossiness + Metalness</b> </td></tr></tbody></table><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1e8/209/484/1e8209484fe730d178c4afffef4c8f93.jpg"></div><br>  <i>Normals</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8fb/dd3/b2d/8fbdd3b2d2ddbce3111a4c017e7daef3.jpg"></div><br>  <i>Glossiness / Metalness</i> <br><br>  <strong>Benefits of Pre-Pass Depths</strong> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aa1/b15/52b/aa1b1552b0f81c2122d094441e531f3f.jpg"></div><br>  Remember that in the ‚ÄúPre-Pass Depths‚Äù section, we talked about saving costs per pixel?  I‚Äôll go back a little to illustrate it.  Take the following image.  This is a rendering of the detailed part of the mountain to the normal buffer.  Renderdoc kindly selected the pixels that passed the depth test, in green, and those that did not pass it - in red (they are not rendered).  The total number of pixels that would be rendered without this preliminary pass is approximately 104518 (calculated in Photoshop).  The total number of pixels actually rendered is 23858 (calculated by Renderdoc).  Savings of about 77%!  As we can see, with clever use, this preliminary pass can give a big win, but it requires only about a hundred draw calls. <br><br>  <strong>Record multithreaded commands</strong> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/919/5d0/623/9195d06236fd6247cea6e6065f05e9e8.png"></div><br>  It is worth noting one interesting aspect, because of which I chose the DX12 renderer - the recording of multi-threaded commands.  In previous APIs, such as DX11, rendering is usually performed in a single thread.  The graphics driver received rendering commands from the game and constantly transmitted requests to the GPU, but the game did not know when this would happen.  This leads to inefficiency, because the driver must somehow guess what the application is trying to do and does not scale to multiple threads.  Newer APIs, such as the DX12, take control of the developer, who can decide how to write commands and when to send them.  Although Renderdoc cannot show how the recording is performed, you will see that there are seven color passes marked as Color Pass N, and each one is wrapped in a pair of ExecuteCommandList: Reset / Close.  It marks the beginning and end of the list of commands.  The list has about 100-200 draw calls.  This does not mean that they were recorded using multiple streams, but hints at it. <br><br>  <strong>Footprints in the snow</strong> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca6/d21/2e8/ca6d212e89b61f52738dfbd01363e366.jpg"></div><br>  If you look at Lara, you can see that when moving before the screenshot, she left traces in the snow.  In each frame, a compute shader (compute shader) is executed, recording deformations in certain areas and applying them based on the type and height of the surface.  Here, only the normal map is applied to the snow (i.e., the geometry does not change), but in some areas where the snow thickness is greater, the deformation is actually performed!  You can also see how the snow ‚Äúfalls‚Äù into place and fills the traces left by Lara.  Much more detail this technique is described in <a href="http://gpupro.blogspot.com/2016/01/gpu-pro-7-table-of-content.html">GPU Pro 7</a> .  The snow deformation texture is a kind of height map that tracks Lara's movements and is glued along the edges so that the sampling shader can take advantage of this folding. <br><br><h4>  Shadow Atlas </h4><br>  When creating Shadow mapping, a fairly common approach is used - packing as many shadow maps as possible into the overall shadow texture.  This shadow atlas is actually a huge 16-bit texture of 16384 √ó 8196.  This makes it very flexible to reuse and scale the shadow maps that are in the atlas.  In the frame we analyzed, 8 shadow maps were recorded in the atlas.  Four of them belong to the main source of directional lighting (the moon, because it happens at night), because they use cascading shadow maps ‚Äî a fairly standard long-distance shadow technique for directional lighting, which I explained a little <a href="https://habr.com/ru/post/430518/">earlier</a> .  More interestingly, several spotlights and spotlights are also included in the capture of this frame.  The fact that 8 shadow maps are recorded in this frame does not mean that there are only 8 sources of the cast shadow of lighting.  A game can cache shadow calculations, that is, lighting that has not changed either the position of the source or the geometry in the area of ‚Äã‚Äãoperation should not update its shadow map. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f00/8d9/169/f008d9169bd3490c26f923bcc4ea8e85.jpg"></div><br>  It seems that the rendering of shadow maps also benefits from writing multi-threaded commands to the list, and in this case, for the rendering of shadow maps, there are as many as 19 lists of commands. <br><br>  <strong>Shadows from directional lighting</strong> <br><br>  The shadows from the directional illumination are calculated before the passage of the illumination and later sampled.  I do not know what would happen if there were several sources of directional lighting in the scene. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cef/209/845/cef209845628db5e68588f07880542eb.jpg"></div><br><h4>  Ambient occlusion </h4><br>  For ambient occlusion, ROTR allows you to use either HBAO or its HBAO + variant (this technique was originally published by NVIDIA).  There are several variations of this algorithm, so I‚Äôll look at the one I found in ROTR.  First, the depth buffer is divided into 16 textures, each of which contains 1/16 of all depth values.  Separation is performed in such a way that each texture contains one value from the 4 √ó 4 block of the original texture shown in the figure below.  The first texture contains all the values ‚Äã‚Äãmarked in red (1), the second - the values ‚Äã‚Äãmarked in blue (2), and so on.  If you want to know more about this technique, then here is the <a href="https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf">article by</a> <a href="https://twitter.com/louisbavoil">Louis Bavoil</a> , who was also one of the authors of the HBAO article. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c9/2dc/60f/1c92dc60fe369caed766efe18dd2739d.png"></div><br>  The next stage calculates for each texture ambient occlusion, which gives us 16 AO textures.  The ambient occlusion is generated as follows: the depth buffer is sampled several times, recreating the position and accumulating the result of the calculations for each of the samples.  Each ambient occlusion texture is calculated using different sampling coordinates, that is, in a 4 √ó 4 pixel block, each pixel tells its own part of the story.  This is done for performance reasons.  Each pixel already samples the depth buffer 32 times, and the full effect will require 16 √ó 32 = 512 samples, which is a brute force even for the most powerful GPUs.  Then they are recombined into one full-screen texture, which turns out to be quite noisy, therefore, to smooth the results immediately after that, a passage of full-screen blur is performed.  We saw a very similar solution in the <a href="https://habr.com/ru/post/430518/">Shadow of Mordor</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/25f/e57/dd8/25fe57dd8001f561e9a971b36b57085e.png" alt="image"></div><br>  <i>HBAO parts</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/e1b/7d0/113e1b7d065a25b8f4dfb8dce29c808c.png" alt="image"></div><br>  <i>Full HBAO with noise</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/063/bfe/cd1/063bfecd15cdf8d62a0b6221fddfe4ee.png" alt="image"></div><br>  <i>Full HBAO with horizontal blur</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/51d/af9/8e2/51daf98e295b549a57089b4d17df1975.png" alt="image"></div><br>  <i>Ready HBAO</i> <br><br><h4>  Tile pre-lighting </h4><br>  Light Prepass is a rather unusual technique.  Most development teams use a combination of deferred + direct lighting calculations (with variations, for example, with tile, clustered) or completely direct for some screen space effects.  Technique pre-pass lighting is so unusual that it deserves an explanation.  If the concept of traditional deferred lighting is to separate the properties of materials from the lighting, then the idea of ‚Äã‚Äãseparating the lighting from the properties of materials is at the heart of the preliminary lighting pass.  Although this wording looks a bit silly, the difference from traditional deferred lighting is that we store all the properties of materials (such as albedo, specular color, roughness, metalness, micro-occlusion, emissive) in a huge G-buffer, and use it later as input data for subsequent light passes.  Traditional deferred lighting may represent a large bandwidth load;  the more complex the materials, the more information and operations are needed in the G-buffer.  However, in the pre-pass lighting, we first accumulate all the lighting separately using the minimum amount of data, and then apply them in subsequent passes to the materials.  In this case, illumination is sufficient only for normals, roughness and metalness.  Shaders (two passes are used here) output data to three RGBA16F render targets.  One contains diffuse illumination, the second is specular illumination, and the third is ambient illumination.  At this stage, all shadow data is taken into account.  It is curious that in the first pass (diffuse + specular illumination) for a full-screen passage a quadrilateral (quad) of two triangles is used, and in other effects one full-screen triangle is used (you can find out why this is important <a href="https://michaldrobot.com/2014/04/01/gcn-execution-patterns-in-full-screen-passes/">here</a> ).  From this point of view, the entire frame is not complete. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/41d/6ba/a3b/41d6baa3b45cc3e0df1423eaf015d416.jpg" alt="image"></div><br>  <i>Diffuse lighting</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9cd/0c5/137/9cd0c513782787daa220b7627b11b28e.jpg" alt="image"></div><br>  <i>Mirror lighting</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/33d/ed9/cbc/33ded9cbc62470319b7729b542b7e830.jpg" alt="image"></div><br>  <i>Ambient lighting</i> <br><br>  <strong>Tile optimization</strong> <br><br>  Tile lighting is an optimization technique designed to render a large number of lighting sources.  ROTR splits the screen into 16 √ó 16 tiles, and then stores information about which sources intersect each tile, that is, lighting calculations will be performed only for those sources that relate to tiles.  At the beginning of the frame, a sequence of computational shaders is launched, determining which sources are related to tiles.  At the lighting stage, each pixel determines which tile it is in and loops around each light source in the tile, performing all light calculations.  If the binding of sources to tiles is done qualitatively, then you can save a lot of calculations and much of the bandwidth, as well as improve performance. <br><br>  <strong>Scale up to depth</strong> <br><br>  Depth based sampling is an interesting technique that is useful on this and subsequent passes.  Sometimes computationally expensive algorithms cannot be rendered at full resolution, so they are rendered with a lower resolution, and then increase in scale.  In our case, the ambient lighting is calculated at half the resolution, that is, after the calculations, the lighting must be correctly recreated.  In its simplest form, 4 pixels of low resolution are taken and interpolated to get something resembling the original image.  It works for smooth transitions, but it looks bad on discontinuities of surfaces, because there we mix quantities that are not related to each other, which may be adjacent in the screen space, but distant from each other in the global space.  In solving this problem, several samples of the depth buffer are usually taken and compared with the sample of depths that we want to recreate.  If the sample is too far, then we do not take it into account when recreating.  Such a scheme works well, but it means that the reconstruction shader heavily loads the bandwidth. <br><br>  ROTR makes a tricky move with the early dropping of stencil.  After the passage of the normals, the depth buffer is completely filled, so the engine performs a full-screen passage that marks all discontinuous pixels in the stencil buffer.  When it comes time to recreate the ambient lighting buffer, the slider uses two shaders: one is very simple for areas without depth gaps, the other is more complex for pixels with gaps.  An early stencil discards pixels if they do not belong to the corresponding area, that is, there are costs only in the desired areas.  The following images are much clearer: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/747/912/729/747912729abfecffd281c72e8ca4e258.jpg" alt="image"></div><br>  <i>Ambient lighting at half resolution</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f0d/e46/b7e/f0de46b7e19ace552377d1a39d86ed22.png" alt="image"></div><br>  <i>Increase the depth of the internal parts</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d64/354/63b/d6435463b209e7361831b279abcf1ae1.jpg" alt="image"></div><br>  <i>Ambient lighting in full resolution, without edges</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/db5/e12/702/db5e1270220121c5ca9c2a9277d519ed.png" alt="image"></div><br>  <i>Edge Scale Increase</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ea/971/5cf/3ea9715cf0c57785d926b0711808b256.jpg" alt="image"></div><br>  <i>Ready ambient lighting</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4a5/5d2/128/4a55d2128ab191fb120ca3c1c4310c95.jpg" alt="image"></div><br>  <i>Approximate half resolution view</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cd4/eaa/a07/cd4eaaa0719a527bcdd1add09f00597b.jpg" alt="image"></div><br>  <i>Approximate view of the recreated image</i> <br><br>  After the preliminary passage of the lighting, the geometry is transferred to the pipeline, only this time each object samples the lighting textures, the ambient occlusion texture and other material properties that we did not write to the G-buffer from the very beginning.  This is good, because throughput is greatly saved here due to the fact that you don‚Äôt need to read a bunch of textures to write them into a large G-buffer, and then read them / decode them again.  The obvious disadvantage of this approach is that all the geometry needs to be transferred anew, and the pre-pass lighting textures themselves represent a large load on throughput.  I wondered why not use a lighter format for pre-pass lighting textures, for example R11G11B10F, but there is additional information in the alpha channel, so that would be impossible.  Anyway, this is an interesting technical solution.  At this stage, all opaque geometry is already rendered and lit.  Note that light emitting objects such as the sky and laptop screen are included. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2eb/3cb/0a3/2eb3cb0a3f58c521079fc2bbd1b3fd67.jpg"></div><br><h4>  Reflections </h4><br>  This scene is not a particularly good example for showing reflections, so I chose another.  The reflection shader is a fairly complex combination of cycles that can be reduced to two parts: one samples the cubic maps, and the other performs SSR (Screen space reflection - calculating reflections in the screen space);  All this is done in a single pass and at the end is mixed, taking into account the coefficient that determines whether the SSR detected reflections (probably, the coefficient is not binary, but is a value in the interval [0, 1]).  SSR works in a standard way for many games - repeatedly traces the depth buffer, trying to find the best intersection between the ray reflected by the shaded surface and another surface in some part of the screen.  SSR works with the previously-scaled mip chain of the current HDR buffer, rather than the entire buffer. <br><br>  There are also such adjustment factors as reflection brightness, as well as a kind of Fresnel texture, which was calculated before this passage, based on normals and roughness.  I‚Äôm not completely sure, but after examining the assembler code, it seems to me that ROTR can calculate SSR only for smooth surfaces.  There is no mip chain of blur in the engine after the SSR stage that exists in other <a href="https://www.guerrilla-games.com/read/taking-killzone-shadow-fall-image-quality-into-the-next-generation-1">engines</a> , and there is not even anything like tracing the depth buffer using rays, which <a href="https://www.ea.com/frostbite/news/stochastic-screen-space-reflections">varies based on roughness</a> .  In general, rougher surfaces get reflections from cubic maps, or not at all.  However, where SSR works, its quality is very high and stable, given that it does not accumulate over time and spatial blur is not performed for it.  Alpha data also supports SSR (in some temples you can see very beautiful reflections in the water) and this is a good addition that you will not often see. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5af/b6f/093/5afb6f0937fe9b172b8e38d3f0459b9b.jpg" alt="image"></div><br>  <i>Reflections to</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ecc/66d/c6c/ecc66dc6ca98108593b9a5ec4695dc63.jpg" alt="image"></div><br>  <i>Reflection buffer</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec9/6ff/e57/ec96ffe5730e775b678a47ae23906b73.jpg" alt="image"></div><br>  <i>Reflections after</i> <br><br><h4>  Illuminated fog </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/578/565/d2a/578565d2af38eac19d8aff86b82cf710.jpg"></div><br>  In our scene, fog is represented poorly because it darkens the background, and therefore is created by particles, so that we again use the reflection example.  The fog is relatively simple, but quite effective.  There are two modes: global, the overall color of the fog, and the color of the scatter inward, obtained from the cube map.  Perhaps, the cubic map was again taken from the cubic reflection maps, or perhaps it was created anew.  In both modes, the rarefaction of the fog is taken from the global rarefaction texture, in which the rarefaction curves are packed for several effects.  In such a scheme, it is remarkable that it gives a very low-cost illuminated fog, i.e.  Inward scattering changes in space, creating the illusion of mist interaction with distant illumination.  This approach can also be used for atmospheric dispersion inside the sky. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e33/e0e/c3a/e33e0ec3aee95a3cd5a669b332946083.jpg" alt="image"></div><br>  <i>Fog up</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/acb/5ed/c78/acb5edc78ab74eb97b9350c1e7aba261.jpg" alt="image"></div><br>  <i>Fog after</i> <br><br><h4>  Volumetric lighting </h4><br>  In the early stages of the frame, several operations take place to prepare for volumetric lighting.  Two buffers are copied from the CPU to the GPU: the indices of the light sources and the data of the light sources.  Both are read by a compute shader that outputs a 40x23x16 3D texture from a camera containing the number of light sources crossing this area.  The texture is 40 √ó 23 in size because each tile is 32 √ó 32 pixels (1280/32 = 40, 720/32 = 22.5), and 16 is the number of pixels in depth.  Not all light sources are included in the texture, but only those that are marked as voluminous (there are three in our scene).  As we will see below, there are other fake volume effects created by flat textures.  The displayed texture has a higher resolution - 160x90x64.  After determining the number of sources of illumination on the tile and their index, three computational shaders are run sequentially, performing the following operations: <br><br><ol><li>  The first pass determines the amount of light entering the cell within the volume in the form of a pyramid of visibility.  Each cell accumulates the influence of all light sources, as if they have suspended particles that react to light and return part of it to the camera. </li><li>  The second pass blurs the lighting with a small radius.  This is probably necessary to avoid flickering when moving the camera, because the resolution is very low. </li><li>  The third pass bypasses the volume texture from front to back, incrementally adding the influence of each source and giving the finished texture.  In fact, it simulates the total amount of incoming light along the beam to a given distance.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Since each cell contains a part of the world reflected by particles in the direction of the camera, in each of them we will get the joint contribution of all the cells that were previously traversed. </font><font style="vertical-align: inherit;">This pass also performs a blur.</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When all this is completed, we get a 3D texture that tells how much light a particular position gets relative to the camera. </font><font style="vertical-align: inherit;">All that remains is to make a full-screen passage - to determine this position, find the corresponding voxel texture and add it to the HDR buffer. </font><font style="vertical-align: inherit;">The lighting shader itself is very simple and contains only about 16 instructions.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b58/939/e8a/b58939e8ae5abf2320c519abc51f6e8a.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Volumetric lighting up to</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ab/006/e9d/1ab006e9d0b62e0ea0470751185ffe41.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bulk lighting after</font></font></i> <br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hair rendering </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If the PureHair function is not turned on, then standard layers of hair are rendered on top of each other. </font><font style="vertical-align: inherit;">This solution still looks great, but I would like to focus on the most advanced technologies. </font><font style="vertical-align: inherit;">If the function is on, the frame starts with a simulation of Lara's hair by a sequence of computational heads. </font><font style="vertical-align: inherit;">The first part of the Tomb Raider used a technology called TressFX, and in the sequel Crystal Dynamics implemented an improved technology. </font><font style="vertical-align: inherit;">After the initial computation, as many as 7 buffers are obtained. </font><font style="vertical-align: inherit;">All of them are used to control Lara's hair. </font><font style="vertical-align: inherit;">The process is as follows:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Run a compute shader to calculate motion values ‚Äã‚Äãbased on previous and current positions (for motion blur) </font></font></li><li> –ó–∞–ø—É—Å–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ —à–µ–π–¥–µ—Ä–∞ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∫—É–±–∏—á–µ—Å–∫–æ–π –∫–∞—Ä—Ç—ã —Å–≤–µ—Ç–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–æ–º 1√ó1 –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–æ–Ω–¥–∞ –æ—Ç—Ä–∞–∂–µ–Ω–∏–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–≤–µ—Ç–∏–º–æ—Å—Ç–∏ (–æ—Å–≤–µ—â–µ–Ω–∏—è) </li><li> –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ 122 —Ç—ã—Å—è—á –≤–µ—Ä—à–∏–Ω –≤ —Ä–µ–∂–∏–º–µ –ø–æ–ª–æ—Å —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ (Triangle Strip) (–∫–∞–∂–¥–∞—è –ø—Ä—è–¥—å –≤–æ–ª–æ—Å ‚Äî —ç—Ç–æ –ø–æ–ª–æ—Å–∞). –ó–¥–µ—Å—å –Ω–µ—Ç –Ω–∏–∫–∞–∫–æ–≥–æ –±—É—Ñ–µ—Ä–∞ –≤–µ—Ä—à–∏–Ω, –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –æ–∂–∏–¥–∞—Ç—å –ø—Ä–∏ –æ–±—ã—á–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö –æ—Ç—Ä–∏—Å–æ–≤–∫–∏. –í–º–µ—Å—Ç–æ –Ω–∏—Ö –µ—Å—Ç—å 7 –±—É—Ñ–µ—Ä–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤–æ–ª–æ—Å. –ü–∏–∫—Å–µ–ª—å–Ω—ã–π —à–µ–π–¥–µ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä—É—á–Ω—É—é –æ–±—Ä–µ–∑–∫—É, –µ—Å–ª–∏ –ø–∏–∫—Å–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–∫–Ω–∞, —Ç–æ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è. –≠—Ç–æ—Ç –ø—Ä–æ—Ö–æ–¥ –ø–æ–º–µ—á–∞–µ—Ç —Å—Ç–µ–Ω—Å–∏–ª –∫–∞–∫ ¬´—Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤–æ–ª–æ—Å—ã¬ª. </li><li> –ü—Ä–æ—Ö–æ–¥ –æ—Å–≤–µ—â–µ–Ω–∏—è/—Ç—É–º–∞–Ω–∞ —Ä–µ–Ω–¥–µ—Ä–∏—Ç –ø–æ–ª–Ω–æ—ç–∫—Ä–∞–Ω–Ω—ã–π quad —Å–æ –≤–∫–ª—é—á–µ–Ω–Ω—ã–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å—Ç–µ–Ω—Å–∏–ª–∞, —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª—è–ª–∏—Å—å —Ç–æ–ª—å–∫–æ —Ç–µ –ø–∏–∫—Å–µ–ª–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∏–¥–Ω—ã –≤–æ–ª–æ—Å—ã. –ü–æ —Å—É—Ç–∏, –æ–Ω —Å—á–∏—Ç–∞–µ—Ç –≤–æ–ª–æ—Å—ã –Ω–µ–ø—Ä–æ–∑—Ä–∞—á–Ω–º–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∑–∞—Ç–µ–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Ç–µ–º–∏ –ø—Ä—è–¥—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤–∏–¥–Ω—ã –Ω–∞ —ç–∫—Ä–∞–Ω–µ. </li><li> –¢–∞–∫–∂–µ –µ—Å—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –Ω–∞–ø–æ–¥–æ–±–∏–µ —ç—Ç–∞–ø–∞ 4, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–≤–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ –≥–ª—É–±–∏–Ω—É –≤–æ–ª–æ—Å (–≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç—É—Ä—ã ¬´–≥–ª—É–±–∏–Ω—ã –≤–æ–ª–æ—Å¬ª) </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you are interested in learning more about this, then AMD has a lot of </font></font><a href="http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Augmented-Hair-in-Deus-Ex-Universe-Projects-TressFX-3.0.ppsx"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">resources</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="http://32ipi028l5q82yhj72224m8j.wpengine.netdna-cdn.com/wp-content/uploads/2017/03/GDC2017-Real-Time-Finite-Element-Method-And-TressFX-4.0.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">presentations</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , because it is a </font></font><a href="https://gpuopen.com/gaming-product/tressfx/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">public library</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> created by the company </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">I was confused by the stage before stage 1, in which the same draw call is performed as in stage 3, while it says that it renders only depth values, but in fact the content is not rendered, and that is curious; </font><font style="vertical-align: inherit;">maybe Renderdoc keeps back on me. </font><font style="vertical-align: inherit;">I suspect that he may have tried to perform a conditional rendering request, but I do not see any prediction calls.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/21f/127/76f/21f12776f41f0893c37a677deca12529.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hair up</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/f49/ecd/832f49ecd96406db87ee99f3b1089de0.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visible hair pixels</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/289/33e/030/28933e0309107cfcb3120564d55fd412.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hair shading</font></font></i> <br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tile rendering of alpha data and particles </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transparent objects again use the tile classification of light sources calculated for the tile pre-aisle lighting. Each transparent object calculates its own illumination in one pass, that is, the number of instructions and cycles becomes quite frightening (that is why a preliminary pass of illumination was used for opaque objects). Transparent objects can even perform reflections in screen space if they are on! Each object is rendered in the sort order from back to front, directly into the HDR buffer, including glass, flame, water in the tracks, etc. The alpha passage also renders edge highlighting when Lara focuses on an object (for example, on a bottle with a combustible mixture on a box on the left).</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/445/4a8/311/4454a83115f4118ab092fc1ff3bcaaf1.jpg"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, the particles are rendered into a half-resolution buffer to smooth out the huge load on the bandwidth created by their redrawing, especially when many large, screen-covering particles are used to create fog, haze, flames, etc. Therefore, the HDR buffer and the depth buffer are halved on each side, after which the particles begin to be rendered. Particles create a huge amount of redrawing, some pixels are shaded about 40 times. On the heat map you can see what I mean. Since the particles were rendered in half resolution, the same clever scaling trick is used here as in ambient lighting (gaps are marked in the stencil, the first pass renders the internal pixels, the second recreates the edges). You may notice that the particles are rendered before some other alpha effects, such as flame,shine, etc. This is necessary so that the alpha can be properly sorted for, for example, smoke. You can also notice that there are "volumetric" rays of light coming from security spotlights. They are added here, rather than being created at the stage of volumetric lighting. This is a low-cost but realistic way to create them at a great distance.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e78/0f1/c77/e780f1c77d8c9031e10b1ddc6faf6b20.jpg" alt="image"></div><br> <i>–¢–æ–ª—å–∫–æ –Ω–µ–ø—Ä–æ–∑—Ä–∞—á–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1db/409/09c/1db40909cea712bec3b9f1d4eab109d7.jpg" alt="image"></div><br> <i>–ü–µ—Ä–≤—ã–π –∞–ª—å—Ñ–∞-–ø—Ä–æ—Ö–æ–¥</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2c1/8f2/80a/2c18f280ab65f272470ff711def4b89e.jpg" alt="image"></div><br> <i>–ß–∞—Å—Ç–∏—Ü—ã –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è 1</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/098/e29/66c/098e2966cf5d8bdc62a7d76bfdc2344e.jpg" alt="image"></div><br> <i>–ß–∞—Å—Ç–∏—Ü—ã –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è 2</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e0b/c73/f5a/e0bc73f5ab35af88840eebe45f175074.jpg" alt="image"></div><br> <i>–ß–∞—Å—Ç–∏—Ü—ã –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è 3</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb0/6ef/e9e/eb06efe9eb6fe1db3e749e6b1a6c1c9c.jpg" alt="image"><br> <i>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —á–∞—Å—Ç–µ–π</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/56e/263/ba6/56e263ba63ae60ee5af108c172711410.jpg" alt="image"></div><br> <i>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –∫—Ä–∞—ë–≤</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e8f/564/288/e8f564288ae2310272be7ac9899409d9.jpg" alt="image"></div><br> <i>–ü–æ–ª–Ω—ã–µ –∞–ª—å—Ñ–∞-–¥–∞–Ω–Ω—ã–µ</i> <br><br><h4> –í—ã–¥–µ—Ä–∂–∫–∞ –∏ —Ç–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ROTR performs shutter speed and tone correction in a single pass. However, although we usually assume that gamma correction occurs during tone correction, this is not the case here. There are many ways to implement excerpts, as we have seen in </font></font><a href="http://www.elopezr.com/castlevania-lords-of-shadow-2-graphics-study/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">other </font></font></a> <a href="https://habr.com/ru/post/430518/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">games</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . The luminance calculation in ROTR is very interesting and requires almost no intermediate data or passes, so we will explain this process in more detail. The entire screen is divided into 64 √ó 64 tiles, after which the computation of groups (20, 12, 1) of 256 streams each starts to fill the entire screen. Each thread essentially performs the following task (below is pseudocode):</font></font><br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; <span class="hljs-number"><span class="hljs-number">16</span></span>; ++i) { uint2 iCoord = CalculateCoord(threadID, i, j); <span class="hljs-comment"><span class="hljs-comment">// Obtain coordinate float3 hdrValue = Load(hdrTexture, iCoord.xyz); // Read HDR float maxHDRValue = max3(hdrValue); // Find max component float minHDRValue = min3(hdrValue); // Find min component float clampedAverage = max(0.0, (maxHDRValue + minHDRValue) / 2.0); float logAverage = log(clampedAverage); // Natural logarithm sumLogAverage += logAverage; }</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Each group calculates the logarithmic sum of all 64 pixels (256 streams, each of which processes 16 values). Instead of storing an average value, it saves the sum and the number of actually processed pixels (not all groups will process exactly 64 √ó 64 pixels, because, for example, they can go beyond the edges of the screen). The shader wisely uses local stream storage to separate the amount; Each stream first works with 16 horizontal values, and then the individual streams summarize all these values ‚Äã‚Äãvertically, and finally the control flow of this group (stream 0) adds the result and stores them all into the buffer. This buffer contains 240 elements, essentially giving us the average brightness of many areas of the screen. The subsequent command starts 64 threads that bypass all these values ‚Äã‚Äãin the loop and add them,to get the final screen brightness. It also goes back from logarithm to linear units.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I do not have much experience with exposure techniques, but reading </font></font><a href="https://knarkowicz.wordpress.com/2016/01/09/automatic-exposure/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this post by</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Krzysztof Narkovic has clarified some things. Saving to an array of 64 elements is necessary for calculating the moving average, at which you can view the previous calculated values ‚Äã‚Äãand smooth the curve in order to avoid very sharp changes in brightness, which create sharp changes in the shutter speed. This is a very complex shader and I have not yet figured out all its details, but the end result is the shutter value corresponding to the current frame. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After finding adequate shutter speeds, one pass performs the final shutter speed plus tonal correction. ROTR seems to use </font></font><a href="http://www.cmap.polytechnique.fr/~peyre/cours/x2005signal/hdr_photographic.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">photographic tonal correction (Photographic Tonemapping)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, which explains the use of logarithmic averages instead of the usual averages. </font><font style="vertical-align: inherit;">The tonal correction formula in the shader (after exposure) can be expanded as follows:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bz/yv/-k/bzyv-kjhv-hs6lj3mh4yfrh427w.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/id/zn/1d/idzn1d3rn-3qs7vrokrn94d9z1m.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A brief explanation can be found </font></font><a href="https://expf.wordpress.com/2010/05/04/reinhards_tone_mapping_operator/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">I could not figure out why an additional division by Lm is needed, because it cancels the effect of multiplication. </font><font style="vertical-align: inherit;">In any case, whitePoint is 1.0, so the process does not do very much in this frame, only the shutter speed changes the image. </font><font style="vertical-align: inherit;">There is not even a limitation of the values ‚Äã‚Äãwithin the LDR interval! </font><font style="vertical-align: inherit;">It occurs during color correction, when the color cube indirectly limits values ‚Äã‚Äãgreater than 1.0.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e1a/92f/2e8/e1a92f2e852892aa137c170a56718f8e.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exposure to</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/788/c4f/905/788c4f905eecffaa8d5dbd12d1c8972d.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exposure after</font></font></i> <br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Lens flare </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The lens flare (Lens Flares) is rendered in an interesting way. A small preliminary pass calculates a 1xN texture (where N is the total number of glare elements that will be rendered as lens flare, in our case there are 28). This texture contains an alpha value for a particle and some other unused information, but instead of calculating it from a visibility query or something like that, the engine calculates it by analyzing the depth buffer around the particle in the circle. For this, information about vertices is stored in a buffer accessible to the pixel shader.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8a0/604/e96/8a0604e96d5a4fe4ebf691719f03d75f.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Each element is then rendered as simple planes aligned with the screen, emitted from light sources. </font><font style="vertical-align: inherit;">If the alpha value is less than 0.01, then the position is assigned the value NaN so that this particle is not rasterized. </font><font style="vertical-align: inherit;">They are a bit like the bloom effect and add glow, but this effect itself is created later.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/526/da9/a18/526da9a184f303dc8e3bfd82cd16034a.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lens flares up</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/465/440/335/4654403353e5b94dc7bae2a68b95ed88.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lens Flare Elements</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5c1/f10/c43/5c1f10c4396cbd6a8502a28b5a53fe0f.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lens flares after</font></font></i> <br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bloom </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bloom uses a standard approach: downsampling of the HDR buffer is performed, bright pixels are isolated, and then their scale increases sequentially with blurring to expand their area of ‚Äã‚Äãinfluence. The result is increased to the screen resolution and compositing is superimposed on top of it. There are a couple of interesting points worth exploring. The whole process is performed using 7 computational shaders: 2 for downsampling, 1 for simple blur, 4 for zooming in.</font></font><br><br><ol><li> –ü–µ—Ä–≤—ã–π –¥–∞—É–Ω—Å—ç–º–ø–ª–∏–Ω–≥ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –≤ –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–∏–∫—Å–µ–ª–∏ —è—Ä—á–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∏—Ç –∏—Ö –≤ target –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (mip 1). –¢–∞–∫–∂–µ –æ–Ω –ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞–∑–º—ã—Ç–∏–µ. –ú–æ–∂–Ω–æ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –ø–µ—Ä–≤–∞—è mip-—Ç–µ–∫—Å—Ç—É—Ä–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–µ–º–Ω–æ–≥–æ —Ç–µ–º–Ω–µ–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ –º—ã –æ—Ç–±—Ä–æ—Å–∏–ª–∏ –ø–∏–∫—Å–µ–ª–∏ —Å –¥–æ–≤–æ–ª—å–Ω–æ –Ω–∏–∑–∫–∏–º –ø–æ—Ä–æ–≥–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º 0.02. </li><li> –°–ª–µ–¥—É—é—â–∏–π —à–µ–π–¥–µ—Ä –¥–∞—É–Ω—Å—ç–º–ø–ª–∏–Ω–≥–∞ –±–µ—Ä—ë—Ç mip –∏ —Å–æ–∑–¥–∞—ë—Ç –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ mip 2, 3, 4 –∏ 5. </li><li> –°–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ—Ö–æ–¥ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ —Ä–∞–∑–º—ã–≤–∞–µ—Ç mip 5. –í–æ –≤—Å—ë–º —ç—Ç–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –Ω–µ—Ç –æ—Ç–¥–µ–ª–∏–º—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Ä–∞–∑–º—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –∏–Ω–æ–≥–¥–∞ –≤—Å—Ç—Ä–µ—á–∞–ª–∏. –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–º—ã—Ç–∏—è –ø–æ–ª—å–∑—É—é—Ç—Å—è –≥—Ä—É–ø–ø–æ–≤–æ–π –æ–±—â–µ–π –ø–∞–º—è—Ç—å—é, —á—Ç–æ–±—ã —à–µ–π–¥–µ—Ä –±—Ä–∞–ª –∫–∞–∫ –º–æ–∂–Ω–æ –º–µ–Ω—å—à–µ —Å—ç–º–ø–ª–æ–≤ –∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∞–Ω–Ω—ã–µ —Å–≤–æ–∏—Ö —Å–æ—Å–µ–¥–µ–π. </li><li> –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ ‚Äî —Ç–æ–∂–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å. –≠—Ç–∏ 3 –ø—Ä–æ—Ö–æ–¥–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —à–µ–π–¥–µ—Ä –∏ –±–µ—Ä—É—Ç –¥–≤–µ —Ç–µ–∫—Å—Ç—É—Ä—ã, —Ä–∞–Ω–µ–µ —Ä–∞–∑–º—ã—Ç—É—é mip N –∏ –Ω–µ —Ä–∞–∑–º—ã—Ç—É—é mip N + 1, —Å–º–µ—à–∏–≤–∞—è –∏—Ö –≤–º–µ—Å—Ç–µ —Å –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–º —Å–Ω–∞—Ä—É–∂–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º, –≤ —Ç–æ–∂–µ –≤—Ä–µ–º—è —Ä–∞–∑–º—ã–≤–∞—è –∏—Ö. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –≤ bloom –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –ø–æ–¥—Å–≤–µ—Ç–∫–∏ –≤–º–µ—Å—Ç–æ —Ç–µ—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∏—Å—á–µ–∑–Ω—É—Ç—å –ø—Ä–∏ —Ä–∞–∑–º—ã—Ç–∏–∏. </li><li> –§–∏–Ω–∞–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç mip 1 –∏ –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç –µ–≥–æ –∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É HDR-–±—É—Ñ–µ—Ä—É, —É–º–Ω–æ–∂–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é —Å–∏–ª—É bloom. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b0c/478/bc7/b0c478bc7ccc0c4774cea8b2aaa4ec8b.jpg" alt="image"></div><br> <i>Bloom –¥–æ</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f77/b74/99b/f77b7499b451e0f232b06d183ccc04f4.jpg"></div><br> <i>MIP 1 —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c65/ae2/875/c65ae2875487b8a9b441a9ce0b137430.jpg"></div><br> <i>MIP 2 —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55b/9a4/757/55b9a4757f48867201dc76df90ea0dd1.jpg"></div><br> <i>MIP 3 —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ba5/17d/a6f/ba517da6f179cca1067a08115579c155.jpg"></div><br> <i>MIP 4 —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/33f/91f/44d/33f91f44dcc1f9a395375fce8535ac2d.jpg" alt="image"></div><br> <i>MIP 5 —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ecb/561/33d/ecb56133d1965bcc93024c5cded95ec7.jpg"></div><br> <i>–†–∞–∑–º—ã—Ç–∏–µ MIP 5 —ç—Ñ—Ñ–µ–∫—Ç–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/235/0ed/474/2350ed47490101cda12baffbd61fc8f0.jpg"></div><br> <i>MIP 4 —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ff0/cfb/4df/ff0cfb4dfeea995b8666d4b8aec057a1.jpg"></div><br> <i>MIP 3 —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/980/7ca/a31/9807caa31251495050af2e2fd32b5e90.jpg" alt="image"></div><br> <i>MIP 2 —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/857/4be/854/8574be854f2526857ef9b6854d6f7235.jpg"></div><br> <i>MIP 1 —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ Bloom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9be/aa8/8a1/9beaa88a1bf00876801bb103ef85dd16.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bloom after The</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> curious aspect is that the reduced-scale textures change the aspect ratio. </font><font style="vertical-align: inherit;">For the sake of visualization, I corrected them, and I can only guess at the reasons for this; </font><font style="vertical-align: inherit;">Perhaps this is done so that texture sizes are multiple of 16. Another interesting point: since these shaders are usually very limited in bandwidth, the values ‚Äã‚Äãstored in the group shared memory are converted from float32 to float16! </font><font style="vertical-align: inherit;">This allows the shader to exchange math operations for doubling free memory and bandwidth. </font><font style="vertical-align: inherit;">For this to become a problem, the range of values ‚Äã‚Äãmust be quite large.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> FXAA </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ROTR supports a wide range of various anti-aliasing techniques, such as </font></font><a href="https://developer.download.nvidia.com/assets/gamedev/files/sdk/11/FXAA_WhitePaper.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FXAA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Fast Approximate AA) and SSAA (Super Sampling AA). </font><font style="vertical-align: inherit;">Notable is the absence of the option to enable temporal AA, because for most modern AAA games it becomes standard. </font><font style="vertical-align: inherit;">Be that as it may, FXAA copes with its task remarkably, SSAA also works well, this is a rather ‚Äúhard‚Äù option if the game lacks performance.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Motion blur </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur seems to use an approach very similar to the </font><a href="https://habr.com/ru/post/430518/"><font style="vertical-align: inherit;">Shadows of Mordor</font></a><font style="vertical-align: inherit;"> solution.</font></font><a href="https://habr.com/ru/post/430518/"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. After rendering the volumetric illumination, a separate rendering pass outputs motion vectors from animated objects to the motion buffer. This buffer is then combined with the motion caused by the camera, and the final motion buffer becomes the input to the blur pass, which blurs in the direction indicated by the motion vectors of the screen space. To estimate the blur radius for several passes, the texture of motion vectors is calculated on a reduced scale, so that each pixel has an approximate idea of ‚Äã‚Äãwhat movement occurs next to it. Blur is performed in several passes at half resolution and, as we have seen, later its scale is enlarged in two passes using stencil. Multiple passes are performed for two reasons: first,to reduce the amount of texture reading required to create a blur with a potentially very large radius, and secondly, because different types of blur are performed. It depends on whether there is an animated character on the current pixels.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3f4/91d/df6/3f491ddf60c6a1a6b9caeaec316eab4f.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur up</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2e5/eb5/d42/2e5eb5d42d4e0c36c4c96e764538f8c8.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Speed ‚Äã‚Äãmotion blur</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/edf/3ed/2e7/edf3ed2e7144ec27ba21b3b16e606c7f.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur pass 1</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7c5/c65/931/7c5c6593183900a85a3d0372fce9eb03.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur pass 2</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8a3/bc5/bcc/8a3bc5bcc85c92d698fc081e49c0f318.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur, pass 3</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04f/5e1/a7e/04f5e1a7e722d9818c8a23ef5759c172.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur, pass 4</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f9d/98f/579/f9d98f579e02acec8e3ef99e90bdd0c0.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur, pass 5</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/99e/3ca/b5c/99e3cab5cbf5d25b2eea0d3311eae7cf.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur, passage 6</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/042/af0/5bd/042af05bd488751c53bd97c73cb037fc.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion Blur, Zooming Internal Parts</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e10/26d/c47/e1026dc47d9035e8713422ad8210e6a7.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motion blur, zooming in on edges</font></font></i> <br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Additional features and details </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There are a few more things worth mentioning without much detail. </font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Camera freezing: in cold weather it adds snowflakes and frost to the camera </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dirty camera: adds dirt to the camera </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Color Correction: A small color correction is performed at the end of the frame, using a fairly standard color cube to perform color correction, as described above, and also adds noise to give some scenes a severity. </font></font></li></ol><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ui </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The UI is implemented a bit unusual - it renders all elements in a linear space. </font><font style="vertical-align: inherit;">Usually, by the time of rendering, the UI has already completed tonal correction and gamma correction. </font><font style="vertical-align: inherit;">However, ROTR uses linear space right up to the very end of the frame. </font><font style="vertical-align: inherit;">This makes sense, because the game uses a reminiscent 3D UI; </font><font style="vertical-align: inherit;">however, before writing sRGB images to HDR buffer, they need to be converted to linear space so that the most recent operation (gamma correction) does not distort colors.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Let's sum up </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I hope you enjoyed reading this analysis just as I created it. </font><font style="vertical-align: inherit;">Personally, I definitely learned a lot from him. </font><font style="vertical-align: inherit;">Congratulations to the talented developers from </font></font><a href="https://en.wikipedia.org/wiki/Crystal_Dynamics"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Crystal Dynamics</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for the fantastic work done to create this engine. </font><font style="vertical-align: inherit;">I also want to thank Baldur Karlsson for his fantastic work on Renderdoc. </font><font style="vertical-align: inherit;">His work made debugging graphics on a PC a much more convenient process. </font><font style="vertical-align: inherit;">I think the only thing that was a bit difficult in this analysis was tracking the shader launches themselves, because at the time of this writing, this feature is not available for DX12. </font><font style="vertical-align: inherit;">I hope, in time, she will appear and we will all be very pleased.</font></font></div><p>Source: <a href="https://habr.com/ru/post/436500/">https://habr.com/ru/post/436500/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>