<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Writing XGBoost from scratch - part 2: gradient boosting</title>
  <meta name="description" content="Hello! 

 In the last article we figured out how the decisive trees are arranged, and implemented it from scratch. 
 construction algorithm, simultane...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>Writing XGBoost from scratch - part 2: gradient boosting</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/ll/lb/oe/lllboex0ltvh9n7kpkfomxfz_p8.jpeg"></div><br>  Hello! <br><br>  In the <a href="https://habr.com/ru/company/mailru/blog/438560/">last article</a> we figured out how the decisive trees are arranged, and implemented it from scratch. <br>  construction algorithm, simultaneously optimizing and improving it.  In this article, we implement the gradient boost algorithm and at the end create our own XGBoost.  The narration will follow the same pattern: we write an algorithm, describe it, summarize the results, comparing the results of work with analogues from Sklearn. <br><br>  In this article, the emphasis will also be made on the implementation in the code, so the whole theory is better read in another together (for example, <a href="https://github.com/Yorko/mlcourse.ai/tree/master/jupyter_russian/topic10_boosting">in the ODS course</a> ), and already with the knowledge of the theory, you can move on to this article, since the topic is quite complicated. <br><br><img src="https://habrastorage.org/webt/l0/vy/cx/l0vycx4dsrrohxafyliapuq7nxi.png"><br><a name="habracut"></a><br>  What is gradient boosting?  A picture with a golfer best describes the main idea.  In order to drive the ball into the hole, the golfer makes every next strike, taking into account the experience of previous shots - for him it is a necessary condition to drive the ball into the hole.  If it‚Äôs very rough (I‚Äôm not a master of the game of golf :)), then with each new strike the first thing a golfer looks at is the distance between the ball and the hole after the previous strike.  And the main task is to reduce this distance with the next blow. <br><br>  Boosting is built in a very similar way.  First, we need to introduce the definition of "hole", that is, the goal to which we will strive.  Secondly, we need to learn to understand which way to hit with a stick in order to get closer to the goal.  Thirdly, taking into account all these rules, you need to come up with the correct sequence of blows, so that each subsequent one shortens the distance between the ball and the hole. <br><br>  Now we give a slightly more rigorous definition.  We introduce a weighted voting model: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">h</span><span class="MJXp-mo" id="MJXp-Span-3" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">x</span><span class="MJXp-mo" id="MJXp-Span-5" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-6" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-7">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-9">u</span><span class="MJXp-msubsup" id="MJXp-Span-10"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-11" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16">n</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-12"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-13">i</span><span class="MJXp-mo" id="MJXp-Span-14">=</span><span class="MJXp-mn" id="MJXp-Span-15">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-17"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-19" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-msubsup" id="MJXp-Span-20"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-21" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-22" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-23" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24">x</span><span class="MJXp-mtext" id="MJXp-Span-25">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-28">X</span><span class="MJXp-mo" id="MJXp-Span-29" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-30"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-32" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mtext" id="MJXp-Span-33">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-34">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-35">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36">R</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="33.839ex" height="2.901ex" viewBox="0 -832 14569.5 1249" role="img" focusable="false" style="vertical-align: -0.969ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-68" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMAIN-28" x="576" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-78" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMAIN-29" x="1538" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMAIN-3D" x="2205" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-73" x="3512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-75" x="3981" y="0"></use><g transform="translate(4554,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-6E" x="1242" y="499"></use><g transform="translate(878,-308)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-69" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMAIN-3D" x="345" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMAIN-31" x="1124" y="0"></use></g></g><g transform="translate(6681,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-62" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-69" x="607" y="-213"></use></g><g transform="translate(7455,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-69" x="748" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMAIN-2C" x="8328" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-78" x="8774" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-69" x="9596" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-6E" x="9942" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-58" x="10542" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMAIN-2C" x="11395" y="0"></use><g transform="translate(11840,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-62" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-69" x="607" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-69" x="12864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-6E" x="13209" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/438562/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifCDBs1Tm9x9ZtrMDKHAfUnAfX5A#MJMATHI-52" x="13810" y="0"></use></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> h (x) = \ sum_ {i = 1} ^ nb_ia_i, x \ in X, b_i \ in R </script></p><br>  Here <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-37"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38">X</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-2"> X </script>  Is the space from which we take objects <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-39"><span class="MJXp-msubsup" id="MJXp-Span-40"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-42" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-43" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-44"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-46" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-3"> b_i, a_i </script>  - this is the coefficient in front of the model and the model itself, that is, the decision tree.  Suppose that already at some step, using the described rules, we managed to add to the composition <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-47"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-48">T</span><span class="MJXp-mo" id="MJXp-Span-49" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mn" id="MJXp-Span-50">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-4"> T-1 </script>  weak algorithm.  To learn to understand what exactly the algorithm should be on the step <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-51"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52">T</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-5"> T </script>  , we introduce the error function: <br><br><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-53"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56">r</span><span class="MJXp-mo" id="MJXp-Span-57" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58">h</span><span class="MJXp-mo" id="MJXp-Span-59" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-60" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-61">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-62">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-63">u</span><span class="MJXp-msubsup" id="MJXp-Span-64"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-65" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70">N</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-66"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67">j</span><span class="MJXp-mo" id="MJXp-Span-68">=</span><span class="MJXp-mn" id="MJXp-Span-69">1</span></span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-71">L</span><span class="MJXp-mo" id="MJXp-Span-72" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-73">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-74">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-75">u</span><span class="MJXp-msubsup" id="MJXp-Span-76"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-77" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-82"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-83">T</span><span class="MJXp-mo" id="MJXp-Span-84">‚àí</span><span class="MJXp-mn" id="MJXp-Span-85">1</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-78"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-79">i</span><span class="MJXp-mo" id="MJXp-Span-80">=</span><span class="MJXp-mn" id="MJXp-Span-81">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-86"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-88" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-msubsup" id="MJXp-Span-89"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-90" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-91" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-92" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-93"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-94" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-95" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-96" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-97" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-98"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-99" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-100" style="vertical-align: -0.4em;">T</span></span><span class="MJXp-msubsup" id="MJXp-Span-101"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-102" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-103" style="vertical-align: -0.4em;">T</span></span><span class="MJXp-mo" id="MJXp-Span-104" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-105"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-106" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-107" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-108" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-109" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-110">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-111">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-112">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-113">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-114">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-115">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-116">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-117">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-118">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-120">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-121">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-122">i</span><span class="MJXp-msubsup" id="MJXp-Span-123"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-124" style="margin-right: 0.05em;">n</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-125" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-126"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-127" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-128" style="vertical-align: -0.4em;">T</span></span><span class="MJXp-mo" id="MJXp-Span-129">,</span><span class="MJXp-msubsup" id="MJXp-Span-130"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-131" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-132" style="vertical-align: -0.4em;">T</span></span></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-6"> err (h) = \ sum_ {j = 1} ^ N L (\ sum_ {i = 1} ^ {T-1} a_ib_i (x_j) + b_Ta_T (x_j)) \ rightarrow min_ {a_T, b_T} </script><br><br>  It turns out that the best algorithm is the one that can minimize the error obtained at previous iterations.  And since the boosting is gradient, then this error function must necessarily have an anti-gradient vector along which you can move in search of a minimum.  Everything! <br><br>  Immediately before the implementation I will insert a few words about how exactly everything will be arranged.  As in the previous article, let's take MSE as a loss.  Calculate its gradient: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-133"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-135">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-136">e</span><span class="MJXp-mo" id="MJXp-Span-137" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-138">y</span><span class="MJXp-mo" id="MJXp-Span-139" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-140">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-141">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-142">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-143">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-144">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-145">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-146">t</span><span class="MJXp-mo" id="MJXp-Span-147" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-148" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-149" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-150">y</span><span class="MJXp-mo" id="MJXp-Span-151" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-152">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-153">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-154">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-156">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-157">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-158">t</span><span class="MJXp-msubsup" id="MJXp-Span-159"><span class="MJXp-mo" id="MJXp-Span-160" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-161" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mspace" id="MJXp-Span-162" style="width: 0em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-163">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-165">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-167">l</span><span class="MJXp-msubsup" id="MJXp-Span-168"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-169" style="margin-right: 0.05em;">a</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-170" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-171">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-172">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-173">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-174">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-175">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-177">t</span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-178">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-180">e</span><span class="MJXp-mo" id="MJXp-Span-181" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-182">y</span><span class="MJXp-mo" id="MJXp-Span-183" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-184">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-185">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-186">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-187">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-188">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-189">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190">t</span><span class="MJXp-mo" id="MJXp-Span-191" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-192" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-194">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-196">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-198">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-199">t</span><span class="MJXp-mo" id="MJXp-Span-200" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-201">y</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-7"> mse (y, predict) = (y - predict) ^ 2 \\ \ nabla_ {predict} mse (y, predict) = predict - y </script></p><br>  Thus, the antigradient vector will be equal to <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-202"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-203">y</span><span class="MJXp-mo" id="MJXp-Span-204" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-205">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-206">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-207">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-208">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-209">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-210">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-211">t</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-8"> y - predict </script>  .  On the move <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-212"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-213">i</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-9"> i </script>  we consider the errors of the algorithm obtained at past iterations.  Next, we train our new algorithm on these errors, and then with a minus sign and add some coefficient to our ensemble. <br><br>  Now we will start implementation. <br><br><h3>  1. Implement the usual gradient boost class </h3><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm_notebook <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datasets <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mean_squared_error <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mse <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.tree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DecisionTreeRegressor <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> itertools %matplotlib inline %load_ext Cython %%cython -a <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> itertools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np cimport numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> itertools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * cdef <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RegressionTreeFastMse</span></span></span><span class="hljs-class">:</span></span> cdef public int max_depth cdef public int feature_idx cdef public int min_size cdef public int averages cdef public np.float64_t feature_threshold cdef public np.float64_t value cpdef RegressionTreeFastMse left cpdef RegressionTreeFastMse right <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, max_depth=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">3</span></span></span></span><span class="hljs-function"><span class="hljs-params">, min_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">, averages=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.max_depth = max_depth self.min_size = min_size self.value = <span class="hljs-number"><span class="hljs-number">0</span></span> self.feature_idx = <span class="hljs-number"><span class="hljs-number">-1</span></span> self.feature_threshold = <span class="hljs-number"><span class="hljs-number">0</span></span> self.left = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.right = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, np.ndarray[np.float64_t, ndim=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">2</span></span></span></span><span class="hljs-function"><span class="hljs-params">] X, np.ndarray[np.float64_t, ndim=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">] y)</span></span></span><span class="hljs-function">:</span></span> cpdef np.float64_t mean1 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef np.float64_t mean2 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef long N = X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] cpdef long N1 = X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] cpdef long N2 = <span class="hljs-number"><span class="hljs-number">0</span></span> cpdef np.float64_t delta1 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef np.float64_t delta2 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef np.float64_t sm1 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef np.float64_t sm2 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef list index_tuples cpdef list stuff cpdef long idx = <span class="hljs-number"><span class="hljs-number">0</span></span> cpdef np.float64_t prev_error1 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef np.float64_t prev_error2 = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef long thres = <span class="hljs-number"><span class="hljs-number">0</span></span> cpdef np.float64_t error = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef np.ndarray[long, ndim=<span class="hljs-number"><span class="hljs-number">1</span></span>] idxs cpdef np.float64_t x = <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-comment"><span class="hljs-comment"># –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ y self.value = y.mean() # –Ω–∞—á–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ - mse –º–µ–∂–¥—É –∑–Ω–∞—á–µ–Ω–∏–µ–º –≤ –ª–∏—Å—Ç–µ base_error = ((y - self.value) ** 2).sum() error = base_error flag = 0 # –ø—Ä–∏—à–ª–∏ –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É if self.max_depth &lt;= 1: return dim_shape = X.shape[1] left_value, right_value = 0, 0 for feat in range(dim_shape): prev_error1, prev_error2 = base_error, 0 idxs = np.argsort(X[:, feat]) # –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–µ–±—Ä–æ—Å–∞ —Å—É–º–º—ã mean1, mean2 = y.mean(), 0 sm1, sm2 = y.sum(), 0 N = X.shape[0] N1, N2 = N, 0 thres = 1 while thres &lt; N - 1: N1 -= 1 N2 += 1 idx = idxs[thres] x = X[idx, feat] # –≤—ã—á–∏—Å–ª—è–µ–º –¥–µ–ª—å—Ç—ã - –ø–æ –Ω–∏–º, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º, –±—É–¥–µ—Ç –¥–µ–ª–∞—Ç—å—Å—è –ø–µ—Ä–µ–±—Ä–æ—Å delta1 = (sm1 - y[idx]) * 1.0 / N1 - mean1 delta2 = (sm2 + y[idx]) * 1.0 / N2 - mean2 # —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—É–º–º—ã sm1 -= y[idx] sm2 += y[idx] # –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ –∑–∞ O(1) prev_error1 += (delta1**2) * N1 prev_error1 -= (y[idx] - mean1)**2 prev_error1 -= 2 * delta1 * (sm1 - mean1 * N1) mean1 = sm1/N1 prev_error2 += (delta2**2) * N2 prev_error2 += (y[idx] - mean2)**2 prev_error2 -= 2 * delta2 * (sm2 - mean2 * N2) mean2 = sm2/N2 # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–ª–∏–∑–∫–∏–µ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É –∑–Ω–∞—á–µ–Ω–∏—è if thres &lt; N - 1 and np.abs(x - X[idxs[thres + 1], feat]) &lt; 1e-5: thres += 1 continue if (prev_error1 + prev_error2 &lt; error): if (min(N1,N2) &gt; self.min_size): # –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∞–º—ã–π –ª—É—á—à–∏–π –ø—Ä–∏–∑–Ω–∞–∫ –∏ –≥—Ä–∞–Ω–∏—Ü—É –ø–æ –Ω–µ–º—É self.feature_idx, self.feature_threshold = feat, x # –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ª–∏—Å—Ç–∞—Ö left_value, right_value = mean1, mean2 # —Ñ–ª–∞–≥ - –∑–Ω–∞—á–∏—Ç —Å–¥–µ–ª–∞–ª–∏ —Ö–æ—Ä–æ—à–∏–π —Å–ø–ª–∏—Ç flag = 1 error = prev_error1 + prev_error2 thres += 1 # –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–∑–¥–µ–ª–∏–ª–∏, –≤—ã—Ö–æ–¥–∏–º if self.feature_idx == -1: return # –≤—ã–∑—ã–≤–∞–µ–º –ø–æ—Ç–æ–º–∫–æ–≤ –¥–µ—Ä–µ–≤–∞ self.left = RegressionTreeFastMse(self.max_depth - 1) self.left.value = left_value self.right = RegressionTreeFastMse(self.max_depth - 1) self.right.value = right_value # –Ω–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Ç–æ–º–∫–æ–≤ idxs_l = (X[:, self.feature_idx] &gt; self.feature_threshold) idxs_r = (X[:, self.feature_idx] &lt;= self.feature_threshold) # –æ–±—É—á–µ–Ω–∏–µ –ø–æ—Ç–æ–º–∫–æ–≤ self.left.fit(X[idxs_l, :], y[idxs_l]) self.right.fit(X[idxs_r, :], y[idxs_r]) def __predict(self, np.ndarray[np.float64_t, ndim=1] x): if self.feature_idx == -1: return self.value if x[self.feature_idx] &gt; self.feature_threshold: return self.left.__predict(x) else: return self.right.__predict(x) def predict(self, np.ndarray[np.float64_t, ndim=2] X): y = np.zeros(X.shape[0]) for i in range(X.shape[0]): y[i] = self.__predict(X[i]) return y</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GradientBoosting</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">()</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, n_estimators=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">100</span></span></span></span><span class="hljs-function"><span class="hljs-params">, learning_rate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, max_depth=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">3</span></span></span></span><span class="hljs-function"><span class="hljs-params">, random_state=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">17</span></span></span></span><span class="hljs-function"><span class="hljs-params">, n_samples = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">15</span></span></span></span><span class="hljs-function"><span class="hljs-params">, min_size = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params">, base_tree=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Bagging'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.n_estimators = n_estimators self.max_depth = max_depth self.learning_rate = learning_rate self.initialization = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> y: np.mean(y) * np.ones([y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]]) self.min_size = min_size self.loss_by_iter = [] self.trees_ = [] self.loss_by_iter_test = [] self.n_samples = n_samples self.base_tree = base_tree <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, X, y)</span></span></span><span class="hljs-function">:</span></span> self.X = X self.y = y b = self.initialization(y) prediction = b.copy() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm_notebook(range(self.n_estimators)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> t == <span class="hljs-number"><span class="hljs-number">0</span></span>: resid = y <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-comment"><span class="hljs-comment"># —Å—Ä–∞–∑—É –ø–∏—à–µ–º –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç resid = (y - prediction) # –≤—ã–±–∏—Ä–∞–µ–º –±–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º if self.base_tree == 'Bagging': tree = Bagging(max_depth=self.max_depth, min_size = self.min_size) if self.base_tree == 'Tree': tree = RegressionTreeFastMse(max_depth=self.max_depth, min_size = self.min_size) # –æ–±—É—á–∞–µ–º—Å—è –Ω–∞ –≤–µ–∫—Ç–æ—Ä–µ –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ tree.fit(X, resid) # –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º –∫ –∞–Ω—Å–∞–º–±–ª—é b = tree.predict(X).reshape([X.shape[0]]) self.trees_.append(tree) prediction += self.learning_rate * b # –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–≤–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è if t &gt; 0: self.loss_by_iter.append(mse(y,prediction)) return self def predict(self, X): # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥–Ω–æ–∑ ‚Äì —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –≤–µ–∫—Ç–æ—Ä –∏–∑ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ pred = np.ones([X.shape[0]]) * np.mean(self.y) # –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–µ—Ä–µ–≤—å–µ–≤ for t in range(self.n_estimators): pred += self.learning_rate * self.trees_[t].predict(X).reshape([X.shape[0]]) return pred</span></span></code> </pre><br>  We now construct the loss curve on the training set to make sure that with each iteration we actually have its decrease. <br><br><pre> <code class="python hljs">GDB = GradientBoosting(n_estimators=<span class="hljs-number"><span class="hljs-number">50</span></span>) GDB.fit(X,y) x = GDB.predict(X) plt.grid() plt.title(<span class="hljs-string"><span class="hljs-string">'Loss by iterations'</span></span>) plt.plot(GDB.loss_by_iter)</code> </pre> <br><img src="https://habrastorage.org/webt/je/2u/dp/je2udpxa0zdsvsmjqk4mmwmprlo.png"><br><br><h3>  2. Bagging over decisive trees </h3><br>  Okay, before comparing the results, let's talk more about the procedure of <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%2591%25D1%258D%25D0%25B3%25D0%25B3%25D0%25B8%25D0%25BD%25D0%25B3">begging</a> over trees. <br><br>  Everything is simple here: we want to protect ourselves from retraining, and therefore, with the help of return samples, we will average our predictions in order not to accidentally run into emissions (why it works like this - read the link better). <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Bagging</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">()</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">''' –ö–ª–∞—Å—Å Bagging - –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –±—É—Å—Ç—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π. '''</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, max_depth = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">3</span></span></span></span><span class="hljs-function"><span class="hljs-params">, min_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">, n_samples = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#super(CART, self).__init__() self.max_depth = max_depth self.min_size = min_size self.n_samples = n_samples self.subsample_size = None self.list_of_Carts = [RegressionTreeFastMse(max_depth=self.max_depth, min_size=self.min_size) for _ in range(self.n_samples)] def get_bootstrap_samples(self, data_train, y_train): # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤—ã–±–æ—Ä–æ–∫ —Å –≤–æ–∑—Ä–∞—â–µ–Ω–∏–µ–º indices = np.random.randint(0, len(data_train), (self.n_samples, self.subsample_size)) samples_train = data_train[indices] samples_y = y_train[indices] return samples_train, samples_y def fit(self, data_train, y_train): # –æ–±—É—á–∞–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å self.subsample_size = int(data_train.shape[0]) samples_train, samples_y = self.get_bootstrap_samples(data_train, y_train) for i in range(self.n_samples): self.list_of_Carts[i].fit(samples_train[i], samples_y[i].reshape(-1)) return self def predict(self, test_data): # –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –±–µ—Ä—ë–º –µ–≥–æ —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–µ–¥–∏–∫—Ç num_samples = test_data.shape[0] pred = [] for i in range(self.n_samples): pred.append(self.list_of_Carts[i].predict(test_data)) pred = np.array(pred).T return np.array([np.mean(pred[i]) for i in range(num_samples)])</span></span></code> </pre><br>  Great, now we can use not one tree as the base algorithm, but tree begging - so, again, we will defend against retraining. <br><br><h3>  3. Results </h3><br>  Compare the results of our algorithms. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KFold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GradientBoostingRegressor <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> GDBSklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> copy <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_metrics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X,y,n_folds=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">2</span></span></span></span><span class="hljs-function"><span class="hljs-params">, model=None)</span></span></span><span class="hljs-function">:</span></span> kf = KFold(n_splits=n_folds, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) kf.get_n_splits(X) er_list = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> train_index, test_index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm_notebook(kf.split(X)): X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] model.fit(X_train,y_train) predict = model.predict(X_test) er_list.append(mse(y_test, predict)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> er_list data = datasets.fetch_california_housing() X = np.array(data.data) y = np.array(data.target) er_boosting = get_metrics(X,y,<span class="hljs-number"><span class="hljs-number">30</span></span>,GradientBoosting(max_depth=<span class="hljs-number"><span class="hljs-number">3</span></span>, n_estimators=<span class="hljs-number"><span class="hljs-number">40</span></span>, base_tree=<span class="hljs-string"><span class="hljs-string">'Tree'</span></span> )) er_boobagg = get_metrics(X,y,<span class="hljs-number"><span class="hljs-number">30</span></span>,GradientBoosting(max_depth=<span class="hljs-number"><span class="hljs-number">3</span></span>, n_estimators=<span class="hljs-number"><span class="hljs-number">40</span></span>, base_tree=<span class="hljs-string"><span class="hljs-string">'Bagging'</span></span> )) er_sklearn_boosting = get_metrics(X,y,<span class="hljs-number"><span class="hljs-number">30</span></span>,GDBSklearn(max_depth=<span class="hljs-number"><span class="hljs-number">3</span></span>,n_estimators=<span class="hljs-number"><span class="hljs-number">40</span></span>, learning_rate=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)) %matplotlib inline data = [er_sklearn_boosting, er_boosting, er_boobagg] fig7, ax7 = plt.subplots() ax7.set_title(<span class="hljs-string"><span class="hljs-string">''</span></span>) ax7.boxplot(data, labels=[<span class="hljs-string"><span class="hljs-string">'Sklearn Boosting'</span></span>, <span class="hljs-string"><span class="hljs-string">'Boosting'</span></span>, <span class="hljs-string"><span class="hljs-string">'BooBag'</span></span>]) plt.grid() plt.show()</code> </pre> <br>  Got: <br><br><img src="https://habrastorage.org/webt/rd/ih/nb/rdihnby6vylrk7azxytowqvb5uu.png"><br><br>  We cannot yet defeat the analog from Sklearn, because again we do not take into account a lot of parameters that are used in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html">this method</a> .  However, we see that bagging helps a bit. <br><br>  Let's not despair, and move on to writing XGBoost'a. <br><br><h3>  4. XGBoost </h3><br>  Before reading further, I strongly advise you first to familiarize yourself with the following <a href="https://www.youtube.com/watch%3Fv%3DyFXXi_edUS0">video</a> , the theory is very well explained in it. <br><br>  Recall what error we minimize in the usual boosting: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-214"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-216">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-217">r</span><span class="MJXp-mo" id="MJXp-Span-218" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-219">h</span><span class="MJXp-mo" id="MJXp-Span-220" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-221" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-222">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-224">u</span><span class="MJXp-msubsup" id="MJXp-Span-225"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-226" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-231">N</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-227"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-228">j</span><span class="MJXp-mo" id="MJXp-Span-229">=</span><span class="MJXp-mn" id="MJXp-Span-230">1</span></span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-232">L</span><span class="MJXp-mo" id="MJXp-Span-233" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-234">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-235">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-236">u</span><span class="MJXp-msubsup" id="MJXp-Span-237"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-238" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-243"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-244">T</span><span class="MJXp-mo" id="MJXp-Span-245">‚àí</span><span class="MJXp-mn" id="MJXp-Span-246">1</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-239"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-240">i</span><span class="MJXp-mo" id="MJXp-Span-241">=</span><span class="MJXp-mn" id="MJXp-Span-242">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-247"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-249" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-msubsup" id="MJXp-Span-250"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-251" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-252" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-253" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-254"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-255" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-256" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-257" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-258" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-259"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-260" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-261" style="vertical-align: -0.4em;">T</span></span><span class="MJXp-msubsup" id="MJXp-Span-262"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-263" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-264" style="vertical-align: -0.4em;">T</span></span><span class="MJXp-mo" id="MJXp-Span-265" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-266"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-267" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-268" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-269" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-270" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-10"> err (h) = \ sum_ {j = 1} ^ N L (\ sum_ {i = 1} ^ {T-1} a_ib_i (x_j) + b_Ta_T (x_j)) </script></p><br>  XGBoost explicitly adds regularization to this error functionality: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-271"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-272">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-273">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-274">r</span><span class="MJXp-mo" id="MJXp-Span-275" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-276">h</span><span class="MJXp-mo" id="MJXp-Span-277" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-278" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-279">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-281">u</span><span class="MJXp-msubsup" id="MJXp-Span-282"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-283" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">N</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-284"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-285">j</span><span class="MJXp-mo" id="MJXp-Span-286">=</span><span class="MJXp-mn" id="MJXp-Span-287">1</span></span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">L</span><span class="MJXp-mo" id="MJXp-Span-290" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-291">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-293">u</span><span class="MJXp-msubsup" id="MJXp-Span-294"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-295" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-300"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">T</span><span class="MJXp-mo" id="MJXp-Span-302">‚àí</span><span class="MJXp-mn" id="MJXp-Span-303">1</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-296"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-297">i</span><span class="MJXp-mo" id="MJXp-Span-298">=</span><span class="MJXp-mn" id="MJXp-Span-299">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-304"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-305" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-306" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-msubsup" id="MJXp-Span-307"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-308" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-309" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-310" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-311"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-312" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-313" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-314" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-315" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-316"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-317" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-318" style="vertical-align: -0.4em;">T</span></span><span class="MJXp-msubsup" id="MJXp-Span-319"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-321" style="vertical-align: -0.4em;">T</span></span><span class="MJXp-mo" id="MJXp-Span-322" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-323"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-324" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-325" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-326" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-327" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-328" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-329">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-330">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-331">u</span><span class="MJXp-msubsup" id="MJXp-Span-332"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-338">T</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-334"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-335">i</span><span class="MJXp-mo" id="MJXp-Span-336">=</span><span class="MJXp-mn" id="MJXp-Span-337">1</span></span></span></span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-339">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-340">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-341">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-342">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-343">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-344">a</span><span class="MJXp-mo" id="MJXp-Span-345" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-346"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-347" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-348" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-349" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-11"> err (h) = \ sum_ {j = 1} ^ NL (\ sum_ {i = 1} ^ {T-1} a_ib_i (x_j) + b_Ta_T (x_j)) + \ sum_ {i = 1} ^ T \ omega (a_i) </script></p><br>  How to count this functionality?  First, we bring it closer with the help of the second-order Taylor series, where the new algorithm is considered as an increment along which we will move, and we will write further on depending on the loss we have: <br><br><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-402"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-403">f</span><span class="MJXp-mo" id="MJXp-Span-404" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-405">x</span><span class="MJXp-mo" id="MJXp-Span-406" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-407">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-409">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-410">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-411">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-412">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-413">x</span><span class="MJXp-mo" id="MJXp-Span-414" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-415">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-416">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-417">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-418">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-419">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-420">k</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-421">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-422">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-423">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-424">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-425">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-426">x</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-427">f</span><span class="MJXp-mo" id="MJXp-Span-428" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-429">x</span><span class="MJXp-mo" id="MJXp-Span-430" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-431" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-432">f</span><span class="MJXp-mo" id="MJXp-Span-433" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-434">x</span><span class="MJXp-msup" id="MJXp-Span-435"><span class="MJXp-mo" id="MJXp-Span-436" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-437" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mtext" id="MJXp-Span-438">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-439">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-440">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-441">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-442">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-443">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-444">x</span><span class="MJXp-mo" id="MJXp-Span-445" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mn" id="MJXp-Span-446">0.5</span><span class="MJXp-mo" id="MJXp-Span-447" style="margin-left: 0.267em; margin-right: 0.267em;">‚àó</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-448">f</span><span class="MJXp-mo" id="MJXp-Span-449" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-450">x</span><span class="MJXp-msup" id="MJXp-Span-451"><span class="MJXp-mo" id="MJXp-Span-452" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-453" style="vertical-align: 0.5em;">‚Ä≥</span></span><span class="MJXp-mo" id="MJXp-Span-454" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-455">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-456">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-457">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-458">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-459">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-460">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-461">x</span><span class="MJXp-msubsup" id="MJXp-Span-462"><span class="MJXp-mo" id="MJXp-Span-463" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-464" style="vertical-align: 0.5em;">2</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-12"> f (x + \ delta x) \ thickapprox f (x) + f (x) '\ delta x + 0.5 * f (x)' '(\ delta x) ^ 2 </script><br><br>  It is necessary to determine which tree we will consider bad and which one to be good. <br><br>  Recall the principle on which the <a href="https://craftappmobile.com/l1-vs-l2-regularization/">regression</a> with <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-465"><span class="MJXp-msubsup" id="MJXp-Span-466"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-467" style="margin-right: 0.05em;">L</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-468" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-13"> L_2 </script>  -regularization - the more normal values ‚Äã‚Äãof the coefficients before regression, the worse, so you need to be as small as possible. <br><br>  In XGBoost, the idea is very similar: a tree is penalized if the sum of the norm values ‚Äã‚Äãin the leaves in it is very large.  Therefore, the complexity of the tree is introduced here as follows: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-469"><span class="MJXp-mtext" id="MJXp-Span-470">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-471">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-472">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-473">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-474">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-475">a</span><span class="MJXp-mo" id="MJXp-Span-476" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-477">a</span><span class="MJXp-mo" id="MJXp-Span-478" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-479" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-480">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-481">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-482">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-483">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-484">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-485">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-486">Z</span><span class="MJXp-mo" id="MJXp-Span-487" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mn" id="MJXp-Span-488">0.5</span><span class="MJXp-mo" id="MJXp-Span-489" style="margin-left: 0.267em; margin-right: 0.267em;">‚àó</span><span class="MJXp-mtext" id="MJXp-Span-490">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-491">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-492">u</span><span class="MJXp-msubsup" id="MJXp-Span-493"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-494" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-499"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-500">Z</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-495"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-496">i</span><span class="MJXp-mo" id="MJXp-Span-497">=</span><span class="MJXp-mn" id="MJXp-Span-498">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-501"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-502" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-504">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-503">i</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-14"> \ omega (a) = \ gamma Z + 0.5 * \ sum_ {i = 1} ^ {Z} w_i ^ 2 </script></p><br><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-505"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-506">w</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-15"> w </script>  - values ‚Äã‚Äãin the leaves, <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-507"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-508">Z</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-16"> Z </script>  - the number of leaves. <br><br>  There are transitional formulas in the video, we will not display them here.  It all comes down to the fact that we will choose the new partition, maximizing the gain: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-509"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-510">G</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-511">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-512">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-513">n</span><span class="MJXp-mo" id="MJXp-Span-514" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-515">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-516">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-517">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-518">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-519">c</span><span class="MJXp-mrow" id="MJXp-Span-520"><span class="MJXp-msubsup" id="MJXp-Span-521"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-522" style="margin-right: 0.05em;">G</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-524">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-523">l</span></span></span></span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-525"><span class="MJXp-msubsup" id="MJXp-Span-526"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-527" style="margin-right: 0.05em;">S</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-529">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-528">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-530" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-531">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-532">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-533">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-534">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-535">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-536">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-537">a</span></span><span class="MJXp-mo" id="MJXp-Span-538" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-539">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-540">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-541">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-542">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-543">c</span><span class="MJXp-mrow" id="MJXp-Span-544"><span class="MJXp-msubsup" id="MJXp-Span-545"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-546" style="margin-right: 0.05em;">G</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-548">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-547">r</span></span></span></span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-549"><span class="MJXp-msubsup" id="MJXp-Span-550"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-551" style="margin-right: 0.05em;">S</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-553">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-552">r</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-554" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-555">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-556">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-557">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-558">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-559">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-560">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-561">a</span></span><span class="MJXp-mo" id="MJXp-Span-562" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-563">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-564">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-565">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-566">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-567">c</span><span class="MJXp-mrow" id="MJXp-Span-568"><span class="MJXp-mo" id="MJXp-Span-569" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-570"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-571" style="margin-right: 0.05em;">G</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-572" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-573" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-574"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-575" style="margin-right: 0.05em;">G</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-576" style="vertical-align: -0.4em;">r</span></span><span class="MJXp-msubsup" id="MJXp-Span-577"><span class="MJXp-mo" id="MJXp-Span-578" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-579" style="vertical-align: 0.5em;">2</span></span></span><span class="MJXp-mrow" id="MJXp-Span-580"><span class="MJXp-msubsup" id="MJXp-Span-581"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-582" style="margin-right: 0.05em;">S</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-584">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-583">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-585" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-586"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-587" style="margin-right: 0.05em;">S</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-589">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-588">r</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-590" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-591">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-592">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-593">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-594">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-595">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-596">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-597">a</span></span><span class="MJXp-mo" id="MJXp-Span-598" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-599">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-600">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-601">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-602">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-603">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-604">a</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-17"> Gain = \ frac {G_l ^ 2} {S_l ^ 2 + \ lambda} + \ frac {G_r ^ 2} {S_r ^ 2 + \ lambda} - \ frac {(G_l + G_r) ^ 2} {S_l ^ 2 + S_r ^ 2 + \ lambda} - \ gamma </script></p><br>  Here <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-605"><span class="MJXp-mtext" id="MJXp-Span-606">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-607">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-608">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-609">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-610">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-611">a</span><span class="MJXp-mo" id="MJXp-Span-612" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mtext" id="MJXp-Span-613">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-614">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-615">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-616">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-617">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-618">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-619">a</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-18"> \ gamma, \ lambda </script>  Are the numerical parameters of the regularization, and <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-620"><span class="MJXp-msubsup" id="MJXp-Span-621"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-622" style="margin-right: 0.05em;">G</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-623" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-624" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-625"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-626" style="margin-right: 0.05em;">S</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-627" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-19"> G_i, S_i </script>  - the corresponding amounts of the first and second derivatives for this partition. <br><br>  Everything, the theory is very briefly stated, the links are given, now let's talk about what the derivatives will be if we work with MSE.  It's simple: <br><br><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-628"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-629">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-630">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-631">e</span><span class="MJXp-mo" id="MJXp-Span-632" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-633">y</span><span class="MJXp-mo" id="MJXp-Span-634" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-635">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-636">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-637">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-638">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-639">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-640">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-641">t</span><span class="MJXp-mo" id="MJXp-Span-642" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-643" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-644" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-645">y</span><span class="MJXp-mo" id="MJXp-Span-646" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-647">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-648">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-649">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-650">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-651">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-652">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-653">t</span><span class="MJXp-msubsup" id="MJXp-Span-654"><span class="MJXp-mo" id="MJXp-Span-655" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-656" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mspace" id="MJXp-Span-657" style="width: 0em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-658">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-659">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-660">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-661">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-662">l</span><span class="MJXp-msubsup" id="MJXp-Span-663"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-664" style="margin-right: 0.05em;">a</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-665" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-666">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-667">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-668">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-669">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-670">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-671">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-672">t</span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-673">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-674">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-675">e</span><span class="MJXp-mo" id="MJXp-Span-676" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-677">y</span><span class="MJXp-mo" id="MJXp-Span-678" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-679">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-680">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-681">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-682">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-683">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-684">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-685">t</span><span class="MJXp-mo" id="MJXp-Span-686" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-687" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-688">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-689">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-690">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-691">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-692">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-693">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-694">t</span><span class="MJXp-mo" id="MJXp-Span-695" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-696">y</span><span class="MJXp-mspace" id="MJXp-Span-697" style="width: 0em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-698">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-699">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-700">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-701">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-702">l</span><span class="MJXp-msubsup" id="MJXp-Span-703"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-704" style="margin-right: 0.05em;">a</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-713">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-705"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-706">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-707">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-708">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-709">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-710">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-711">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-712">t</span></span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-714">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-715">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-716">e</span><span class="MJXp-mo" id="MJXp-Span-717" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-718">y</span><span class="MJXp-mo" id="MJXp-Span-719" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-720">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-721">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-722">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-723">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-724">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-725">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-726">t</span><span class="MJXp-mo" id="MJXp-Span-727" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-728" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-729">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-20"> mse (y, predict) = (y - predict) ^ 2 \\ \ nabla_ {predict} mse (y, predict) = predict - y \\ \ nabla_ {predict} ^ 2 mse (y, predict) = 1 </script><br><br>  When will we take the amount <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-730"><span class="MJXp-msubsup" id="MJXp-Span-731"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-732" style="margin-right: 0.05em;">G</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-733" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-734" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-735"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-736" style="margin-right: 0.05em;">S</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-737" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-21"> G_i, S_i </script>  , just add to the first <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-738"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-739">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-740">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-741">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-742">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-743">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-744">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-745">t</span><span class="MJXp-mo" id="MJXp-Span-746" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-747">y</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-22"> predict - y </script>  , and the second - just the amount. <br><br><pre> <code class="python hljs">%%cython -a <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np cimport numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np cdef <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RegressionTreeGain</span></span></span><span class="hljs-class">:</span></span> cdef public int max_depth cdef public np.float64_t gain cdef public np.float64_t lmd cdef public np.float64_t gmm cdef public int feature_idx cdef public int min_size cdef public np.float64_t feature_threshold cdef public np.float64_t value cpdef public RegressionTreeGain left cpdef public RegressionTreeGain right <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, int max_depth=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">3</span></span></span></span><span class="hljs-function"><span class="hljs-params">, np.float64_t lmd=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, np.float64_t gmm=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, min_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.max_depth = max_depth self.gmm = gmm self.lmd = lmd self.left = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.right = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.feature_idx = <span class="hljs-number"><span class="hljs-number">-1</span></span> self.feature_threshold = <span class="hljs-number"><span class="hljs-number">0</span></span> self.value = <span class="hljs-number"><span class="hljs-number">-1e9</span></span> self.min_size = min_size <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, np.ndarray[np.float64_t, ndim=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">2</span></span></span></span><span class="hljs-function"><span class="hljs-params">] X, np.ndarray[np.float64_t, ndim=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">] y)</span></span></span><span class="hljs-function">:</span></span> cpdef long N = X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] cpdef long N1 = X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] cpdef long N2 = <span class="hljs-number"><span class="hljs-number">0</span></span> cpdef long idx = <span class="hljs-number"><span class="hljs-number">0</span></span> cpdef long thres = <span class="hljs-number"><span class="hljs-number">0</span></span> cpdef np.float64_t gl, gr, gn cpdef np.ndarray[long, ndim=<span class="hljs-number"><span class="hljs-number">1</span></span>] idxs cpdef np.float64_t x = <span class="hljs-number"><span class="hljs-number">0.0</span></span> cpdef np.float64_t best_gain = -self.gmm <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.value == <span class="hljs-number"><span class="hljs-number">-1e9</span></span>: self.value = y.mean() base_error = ((y - self.value) ** <span class="hljs-number"><span class="hljs-number">2</span></span>).sum() error = base_error flag = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.max_depth &lt;= <span class="hljs-number"><span class="hljs-number">1</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dim_shape = X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] left_value = <span class="hljs-number"><span class="hljs-number">0</span></span> right_value = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-comment"><span class="hljs-comment"># –Ω–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è # —á—É—Ç—å-—á—É—Ç—å –º–∞—Ç–∞–Ω–∞ - —É –Ω–∞—Å mse, L = (y - pred)**2 # dL/dpred = pred - y, —ç—Ç—É —Ä–∞–∑–Ω–∏—Ü—É –º—ã –≤ –±—É—Å—Ç–∏–Ω–≥–µ –±—É–¥–µ–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Å–æ –∑–Ω–∞–∫–æ–º - # dL^2/d^2pred = 1 - –ø–æ–ª—É—á–∞–µ—Ç—Å—è, —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ª–∏—Å—Ç–µ for feat in range(dim_shape): idxs = np.argsort(X[:, feat]) gl,gr = y.sum(),0.0 N1, N2, thres = N, 0, 0 while thres &lt; N - 1: N1 -= 1 N2 += 1 idx = idxs[thres] x = X[idx, feat] gl -= y[idx] gr += y[idx] # —Å—á–∏—Ç–∞–µ–º –≥–µ–π–Ω gn = (gl**2) / (N1 + self.lmd) + (gr**2) / (N2 + self.lmd) gn -= ((gl + gr)**2) / (N1 + N2 + self.lmd) + self.gmm if thres &lt; N - 1 and x == X[idxs[thres + 1], feat]: thres += 1 continue # –ø—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏—è –Ω–∞ –≥–µ–π–Ω if (gn &gt; best_gain) and (min(N1,N2) &gt; self.min_size): flag = 1 best_gain = gn left_value = -gl / (N1 + self.lmd) right_value = -gr / (N2 + self.lmd) self.feature_idx = feat self.feature_threshold = x thres += 1 self.gain = best_gain if self.feature_idx == -1: return self.left = RegressionTreeGain(max_depth=self.max_depth - 1, gmm=self.gmm, lmd=self.lmd) self.left.value = left_value self.right = RegressionTreeGain(max_depth=self.max_depth - 1, gmm=self.gmm, lmd=self.lmd) self.right.value = right_value idxs_l = (X[:, self.feature_idx] &gt; self.feature_threshold) idxs_r = (X[:, self.feature_idx] &lt;= self.feature_threshold) self.left.fit(X[idxs_l, :], y[idxs_l]) self.right.fit(X[idxs_r, :], y[idxs_r]) # –ø–æ–¥—Ä—É–±–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –≥–µ–π–Ω if (self.left.left == None or self.right.left == None): if self.gain &lt; 0.0: self.left = None self.right = None self.feature_idx = -1 def __predict(self, np.ndarray[np.float64_t, ndim=1] x): if self.feature_idx == -1: return self.value if x[self.feature_idx] &gt; self.feature_threshold: return self.left.__predict(x) else: return self.right.__predict(x) def predict(self, np.ndarray[np.float64_t, ndim=2] X): y = np.zeros(X.shape[0]) for i in range(X.shape[0]): y[i] = self.__predict(X[i]) return y</span></span></code> </pre> <br>  A small clarification: so that the formulas in the trees with a gain were more beautiful, in the boosting we train the target with a minus sign. <br><br>  Slightly modify our boosting, make some parameters adaptive.  For example, if we notice that the loss has begun to emerge on a plateau, then we decrease the learning rate and increase the max_depth for the following estimators.  We will also add a new bagging - now we will make a boosting over the baggings from trees with a gain: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Bagging</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">()</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, max_depth = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">3</span></span></span></span><span class="hljs-function"><span class="hljs-params">, min_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params">, n_samples = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.max_depth = max_depth self.min_size = min_size self.n_samples = n_samples self.subsample_size = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.list_of_Carts = [RegressionTreeGain(max_depth=self.max_depth, min_size=self.min_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(self.n_samples)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_bootstrap_samples</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, data_train, y_train)</span></span></span><span class="hljs-function">:</span></span> indices = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, len(data_train), (self.n_samples, self.subsample_size)) samples_train = data_train[indices] samples_y = y_train[indices] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> samples_train, samples_y <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, data_train, y_train)</span></span></span><span class="hljs-function">:</span></span> self.subsample_size = int(data_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) samples_train, samples_y = self.get_bootstrap_samples(data_train, y_train) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(self.n_samples): self.list_of_Carts[i].fit(samples_train[i], samples_y[i].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predict</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, test_data)</span></span></span><span class="hljs-function">:</span></span> num_samples = test_data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] pred = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(self.n_samples): pred.append(self.list_of_Carts[i].predict(test_data)) pred = np.array(pred).T <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array([np.mean(pred[i]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(num_samples)])</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GradientBoosting</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">()</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, n_estimators=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">100</span></span></span></span><span class="hljs-function"><span class="hljs-params">, learning_rate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">, max_depth=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">3</span></span></span></span><span class="hljs-function"><span class="hljs-params">, random_state=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">17</span></span></span></span><span class="hljs-function"><span class="hljs-params">, n_samples = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">15</span></span></span></span><span class="hljs-function"><span class="hljs-params">, min_size = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params">, base_tree=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Bagging'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.n_estimators = n_estimators self.max_depth = max_depth self.learning_rate = learning_rate self.initialization = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> y: np.mean(y) * np.ones([y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]]) self.min_size = min_size self.loss_by_iter = [] self.trees_ = [] self.loss_by_iter_test = [] self.n_samples = n_samples self.base_tree = base_tree <span class="hljs-comment"><span class="hljs-comment"># —Ö–æ—Ç–∏–º –∫–∞–∫-—Ç–æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –Ω–∞ –ø–æ–∑–¥–Ω–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏—è—Ö # –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞—Å—Ç—Ä—è–ª–∞, —Ç–æ —É–º–µ–Ω—å—à–∞–µ–º lr –∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º max_depth self.add_to_max_depth = 1 self.init_mse_board = 1.5 def fit(self, X, y): print (self.base_tree) self.X = X self.y = y b = self.initialization(y) prediction = b.copy() for t in tqdm_notebook(range(self.n_estimators)): if t == 0: resid = y else: resid = (y - prediction) if (mse(temp_resid,resid) &lt; self.init_mse_board): self.init_mse_board /= 1.5 self.add_to_max_depth += 1 self.learning_rate /= 1.1 # print ('Alert!', t, self.add_to_max_depth) if self.base_tree == 'Bagging': tree = Bagging(max_depth=self.max_depth+self.add_to_max_depth, min_size = self.min_size) resid = -resid if self.base_tree == 'Tree': tree = RegressionTreeFastMse(max_depth=self.max_depth+self.add_to_max_depth, min_size = self.min_size) if self.base_tree == 'XGBoost': tree = RegressionTreeGain(max_depth=self.max_depth+self.add_to_max_depth, min_size = self.min_size) resid = -resid tree.fit(X, resid) b = tree.predict(X).reshape([X.shape[0]]) # print (b.shape) self.trees_.append(tree) prediction += self.learning_rate * b temp_resid = resid return self def predict(self, X): # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥–Ω–æ–∑ ‚Äì —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –≤–µ–∫—Ç–æ—Ä –∏–∑ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ pred = np.ones([X.shape[0]]) * np.mean(self.y) # –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–µ—Ä–µ–≤—å–µ–≤ for t in range(self.n_estimators): pred += self.learning_rate * self.trees_[t].predict(X).reshape([X.shape[0]]) return pred</span></span></code> </pre> <br><h3>  5. Results </h3><br>  By tradition, compare the results: <br><br><pre> <code class="python hljs">data = datasets.fetch_california_housing() X = np.array(data.data) y = np.array(data.target) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GradientBoostingRegressor <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> GDBSklearn er_boosting_bagging = get_metrics(X,y,<span class="hljs-number"><span class="hljs-number">30</span></span>,GradientBoosting(max_depth=<span class="hljs-number"><span class="hljs-number">3</span></span>, n_estimators=<span class="hljs-number"><span class="hljs-number">150</span></span>,base_tree=<span class="hljs-string"><span class="hljs-string">'Bagging'</span></span>)) er_boosting_xgb = get_metrics(X,y,<span class="hljs-number"><span class="hljs-number">30</span></span>,GradientBoosting(max_depth=<span class="hljs-number"><span class="hljs-number">3</span></span>, n_estimators=<span class="hljs-number"><span class="hljs-number">150</span></span>,base_tree=<span class="hljs-string"><span class="hljs-string">'XGBoost'</span></span>)) er_sklearn_boosting = get_metrics(X,y,<span class="hljs-number"><span class="hljs-number">30</span></span>,GDBSklearn(max_depth=<span class="hljs-number"><span class="hljs-number">3</span></span>,n_estimators=<span class="hljs-number"><span class="hljs-number">150</span></span>,learning_rate=<span class="hljs-number"><span class="hljs-number">0.2</span></span>)) %matplotlib inline data = [er_sklearn_boosting, er_boosting_xgb, er_boosting_bagging] fig7, ax7 = plt.subplots() ax7.set_title(<span class="hljs-string"><span class="hljs-string">''</span></span>) ax7.boxplot(data, labels=[<span class="hljs-string"><span class="hljs-string">'GdbSklearn'</span></span>, <span class="hljs-string"><span class="hljs-string">'Xgboost'</span></span>, <span class="hljs-string"><span class="hljs-string">'XGBooBag'</span></span>]) plt.grid() plt.show()</code> </pre> <br>  The picture will be as follows: <br><br><img src="https://habrastorage.org/webt/jh/at/2u/jhat2uvy0yetcxkbzkaaix06wki.png"><br><br>  The lowest error is in XGBoost, but in XGBooBag the error is more crowded, which is definitely better: the algorithm is more stable. <br><br>  That's all.  I really hope that the material presented in two articles was useful, and you were able to learn something new for yourself.  I am especially grateful to Dmitry for the comprehensive feedback and source codes, to Anton for the advice, to Vladimir for the difficult tasks of study. <br><br>  Successes to all! </div><p>Source: <a href="https://habr.com/ru/post/438562/">https://habr.com/ru/post/438562/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>