<div class="post__text post__text-html js-mediator-article"><p>  You can use kaniko to collect Docker images in a container while doing without Docker.  Let's learn how to run kaniko locally and in the Kubernetes cluster. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/699/1ac/23b/6991ac23b41241f34ce0ab774a64124d.jpg" alt="image"><br>  <em>Next will be a lot of</em> </p><a name="habracut"></a><br><p>  Suppose you decide to collect Docker images in a Kubernetes cluster (well, that's necessary).  What is convenient, consider the real example, so clearer. </p><br><p>  We will also talk about Docker-in-Docker and its alternative - kaniko, with which you can collect Docker images without using Docker.  Finally, we will learn how to set up image assembly in a Kubernetes cluster. </p><br><p>  A general description of Kubernetes is in the book <a href="https://www.thenativeweb.io/blog/2018-04-05-15-17-recommended-reading-kubernetes-in-action/">"Kubernetes in Action" ("Kubernetes in action")</a> . </p><br><h3 id="realnyy-primer">  Real example </h3><br><p>  We in the native web have quite a few Docker private images that need to be stored somewhere.  So we implemented a private <a href="https://hub.docker.com/">Docker Hub</a> .  In the public <a href="https://hub.docker.com/">Docker Hub,</a> there are two functions that we are especially interested in. </p><br><p>  First, we wanted to create a queue that would asynchronously collect Docker images at Kubernetes.  Secondly, to implement sending the collected images to <a href="https://docs.docker.com/registry/">the Docker</a> private <a href="https://docs.docker.com/registry/">registry</a> . </p><br><p>  Typically, the Docker CLI is used directly to implement these functions: </p><br><pre><code class="plaintext hljs">$ docker build ... $ docker push ...</code> </pre> <br><p>  But in the Kubernetes cluster, we host containers based on small and elementary Linux images, in which the Docker is not included by default.  If now we want to use Docker (for example, <code>docker build...</code> ) in a container, we need something like Docker-in-Docker. </p><br><h3 id="chto-ne-tak-s-docker-in-docker">  What is wrong with Docker-in-Docker? </h3><br><p>  To collect container images in Docker, we need a running Docker daemon in the container, that is, Docker-in-Docker.  A docker daemon is a virtualized environment, and the container in Kubernetes is virtualized by itself.  That is, if you want to run the Docker-daemon in a container, you need to use nested virtualization.  To do this, run the container in privileged mode - to gain access to the host system.  But at the same time there are problems with security: for example, you have to work with different file systems (host and container) or use the build cache from the host system.  That’s why we didn’t want to touch Docker-in-Docker. </p><br><h3 id="znakomstvo-s-kaniko">  Meet kaniko </h3><br><p>  Not a Docker-in-Docker by one ... There is another solution - <a href="https://github.com/GoogleContainerTools/kaniko">kaniko</a> .  This tool, written in <a href="https://golang.org/">Go</a> , it collects images of containers from a Dockerfile without a Docker.  Then sends them to the specified <a href="https://docs.docker.com/registry/">Docker registry</a> .  It is recommended to configure kaniko - use a ready <a href="https://console.cloud.google.com/gcr/images/kaniko-project/GLOBAL/executor%3Fpli%3D1">-executor image</a> , which can be run as a Docker container or a container in Kubernetes. </p><br><p>  Just keep in mind that kaniko is still in development and does not support all the Dockerfile commands, for example - <code>--chownflag</code> for the <code>COPY</code> . </p><br><h3 id="zapusk-kaniko">  Running kaniko </h3><br><p>  If you want to run kaniko, you need to specify several arguments for the kaniko container.  First insert the Dockerfile with all its dependencies into the kaniko container.  Locally (in Docker) the <code>-v &lt;путь_в_хосте&gt;:&lt;путь_в_контейнере&gt;</code> parameter is used for this <code>-v &lt;путь_в_хосте&gt;:&lt;путь_в_контейнере&gt;</code> , and Kubernetes has <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volyums</a> . </p><br><p>  Having inserted the Dockerfile with dependencies into the kaniko container, add the argument <code>--context</code> , it will indicate the path to the attached directory (inside the container).  The next argument is <code>--dockerfile</code> .  It indicates the path to the Dockerfile (including the name).  Another important argument is <code>--destination</code> with the full URL to the Docker registry (including the name and image tag). </p><br><h3 id="lokalnyy-zapusk">  Local launch </h3><br><p>  Kaniko is launched in several ways.  For example, on a local computer using Docker (so as not to mess around with the Kubernetes cluster).  Run kaniko with the following command: </p><br><pre> <code class="plaintext hljs">$ docker run \ -v $(pwd):/workspace \ gcr.io/kaniko-project/executor:latest \ --dockerfile=&lt;path-to-dockerfile&gt; \ --context=/workspace \ --destination=&lt;repo-url-with-image-name&gt;:&lt;tag&gt;</code> </pre> <br><p>  If authentication is enabled in the Docker registry, kaniko must first log in.  To do this, connect the local Docker <code>config.jsonfile</code> file with credentials for the Docker registry to the kaniko container using the following command: </p><br><pre> <code class="plaintext hljs">$ docker run \ -v $(pwd):/workspace \ -v ~/.docker/config.json:/kaniko/.docker/config.json \ gcr.io/kaniko-project/executor:latest \ --dockerfile=&lt;path-to-dockerfile&gt; \ --context=/workspace \ --destination=&lt;repo-url-with-image-name&gt;:&lt;tag&gt;</code> </pre> <br><h3 id="zapusk-v-kubernetes">  Run in Kubernetes </h3><br><p>  In the example we wanted to run kaniko in the Kubernetes cluster.  And we also needed something like a queue to build images.  If there is a crash when building or uploading an image to the Docker registry, it would be nice if the process starts automatically again.  For this, there is a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Job</a> in Kubernetes.  Configure <code>backoffLimit</code> by specifying how often the process should retry. </p><br><p>  The easiest way is to embed a Dockerfile with dependencies into the kaniko container using the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a> object (in our example, it is called the <code>kaniko-workspace</code> ).  It will be tied to the container as a directory, and all the data should already be in the <code>kaniko-workspace</code> .  For example, in another container, there is already a Dockerfile with dependencies in the <code>/my-build</code> <code>kaniko-workspace</code> in the <code>kaniko-workspace</code> . </p><br><p>  Don't forget that AWS is in trouble with PersistentVolumeClaim.  If you create PersistentVolumeClaim in AWS, it appears on only one node in the AWS cluster and will only be available there.  (upd: in fact, when creating a PVC, RDS volyam will be created in the random availability zone of your cluster. Accordingly, this volum will be available to all machines in this zone. Kubernetes itself controls that under this PVC will be launched on the node in the accessibility zone RDS volyum. - approx. Per.) So, if you run Job kaniko and this task is on another node, it will not start, because PersistentVolumeClaim is not available.  Hopefully, soon Amazon Elastic File System will be available in Kubernetes and the problem will disappear.  (upd: Kubernetes supports EFS using <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs">storage provisioner</a> .) </p><br><p>  The job resource for building Docker images usually looks like this: </p><br><pre> <code class="plaintext hljs">apiVersion: batch/v1 kind: Job metadata: name: build-image spec: template: spec: containers: - name: build-image image: gcr.io/kaniko-project/executor:latest args: - "--context=/workspace/my-build" - "--dockerfile=/workspace/my-build/Dockerfile" - "--destination=&lt;repo-url-with-image-name&gt;:&lt;tag&gt;" volumeMounts: - name: workspace mountPath: /workspace volumes: - name: workspace persistentVolumeClaim: claimName: kaniko-workspace restartPolicy: Never backoffLimit: 3</code> </pre> <br><p>  If the target Docker registry requires authentication, pass the <code>config.json</code> file with credentials to the kaniko container.  The easiest way to connect <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a> to a container where there is already a <code>config.json</code> file.  Here PersistentVolumeClaim will not be mounted as a directory, but rather as a file in the path <code>/kaniko/.docker/config.json</code> in the kaniko container: </p><br><pre> <code class="plaintext hljs">apiVersion: batch/v1 kind: Job metadata: name: build-image spec: template: spec: containers: - name: build-image image: gcr.io/kaniko-project/executor:latest args: - "--context=/workspace/my-build" - "--dockerfile=/workspace/my-build/Dockerfile" - "--destination=&lt;repo-url-with-image-name&gt;:&lt;tag&gt;" volumeMounts: - name: config-json mountPath: /kaniko/.docker/config.json subPath: config.json - name: workspace mountPath: /workspace volumes: - name: config-json persistentVolumeClaim: claimName: kaniko-credentials - name: workspace persistentVolumeClaim: claimName: kaniko-workspace restartPolicy: Never backoffLimit: 3</code> </pre> <br><p>  If you want to check the status of an <code>kubectl</code> build job, use <code>kubectl</code> .  To filter the status by <code>stdout</code> , run the command: </p><br><pre> <code class="plaintext hljs">$ kubectl get job build-image -o go-template='{{(index .status.conditions 0).type}}'</code> </pre> <br><h3 id="itogi">  Results </h3><br><p>  From the article, you learned when Docker-in-Docker is not suitable for building Docker images in Kubernetes.  They got an idea about kaniko - an alternative to Docker-in-Docker, which is used to create Docker images without Docker.  And they also learned how to write Job resources to collect Docker images in Kubernetes.  And, finally, they saw how to find out the status of the task in progress. </p></div>