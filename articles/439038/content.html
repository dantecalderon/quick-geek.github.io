<div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br>  In this small note I will talk about two pitfalls, which are easy to collide with and easily broken about. <br><br>  It will be about creating a trivial neural network on Keras, with which we will predict the arithmetic mean of two numbers. <br><br>  It would seem that it could be easier.  And indeed, nothing complicated, but there are nuances. <br><br>  To whom the topic is interesting, welcome under the cat, there will not be long boring descriptions here, just a short code and comments to it. <br><a name="habracut"></a><br>  The solution looks something like this: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Dense, Lambda <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras.backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-comment"><span class="hljs-comment"># генератор данных def train_iterator(batch_size=64): x = np.zeros((batch_size, 2)) while True: for i in range(batch_size): x[i][0] = np.random.randint(0, 100) x[i][1] = np.random.randint(0, 100) x_mean = (x[::,0] + x[::,1]) / 2 x_mean_ex = np.expand_dims(x_mean, -1) yield [x], [x_mean_ex] # модель def create_model(): x = Input(name = 'x', shape=(2,)) x_mean = Dense(1)(x) model = Model(inputs=x, outputs=x_mean) return model # создаем и учим model = create_model() model.compile(loss=['mse'], optimizer = 'rmsprop') model.fit_generator(train_iterator(), steps_per_epoch = 1000, epochs = 100, verbose = 1) # предсказываем x, x_mean = next(train_iterator(1)) print(x, x_mean, model.predict(x))</span></span></code> </pre> <br>  We are trying to learn ... but nothing comes out.  And in this place you can arrange dances with a tambourine and lose a lot of time. <br><br><pre> <code class="plaintext hljs">Epoch 1/100 1000/1000 [==============================] - 2s 2ms/step - loss: 1044.0806 Epoch 2/100 1000/1000 [==============================] - 2s 2ms/step - loss: 713.5198 Epoch 3/100 1000/1000 [==============================] - 3s 3ms/step - loss: 708.1110 ... Epoch 98/100 1000/1000 [==============================] - 2s 2ms/step - loss: 415.0479 Epoch 99/100 1000/1000 [==============================] - 2s 2ms/step - loss: 416.6932 Epoch 100/100 1000/1000 [==============================] - 2s 2ms/step - loss: 417.2400 [array([[73., 57.]])] [array([[65.]])] [[49.650894]]</code> </pre><br>  Predicted 49, which is far from 65. <br><br>  But if we alter the generator a little, everything starts working immediately. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_iterator_1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">64</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> x = np.zeros((batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>)) x_mean = np.zeros((batch_size,)) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(batch_size): x[i][<span class="hljs-number"><span class="hljs-number">0</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x[i][<span class="hljs-number"><span class="hljs-number">1</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x_mean[::] = (x[::,<span class="hljs-number"><span class="hljs-number">0</span></span>] + x[::,<span class="hljs-number"><span class="hljs-number">1</span></span>]) / <span class="hljs-number"><span class="hljs-number">2</span></span> x_mean_ex = np.expand_dims(x_mean, <span class="hljs-number"><span class="hljs-number">-1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> [x], [x_mean_ex]</code> </pre><br>  And it is clear that the network is already converging literally in the third era. <br><br><pre> <code class="plaintext hljs">Epoch 1/5 1000/1000 [==============================] - 2s 2ms/step - loss: 648.9184 Epoch 2/5 1000/1000 [==============================] - 2s 2ms/step - loss: 0.0177 Epoch 3/5 1000/1000 [==============================] - 2s 2ms/step - loss: 0.0030</code> </pre><br>  The main difference is that in the first case, the x_mean object is created in memory each time, and in the second it appears when the generator is created and then it is only reused. <br><br>  We understand further whether everything is true in this generator.  It turns out that not really. <br>  The following example shows that something is wrong. <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_iterator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> x = np.zeros((batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(batch_size): x[i][<span class="hljs-number"><span class="hljs-number">0</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x[i][<span class="hljs-number"><span class="hljs-number">1</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x_mean = (x[::,<span class="hljs-number"><span class="hljs-number">0</span></span>] + x[::,<span class="hljs-number"><span class="hljs-number">1</span></span>]) / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> x, x_mean it = train_iterator() print(next(it), next(it))</code> </pre><br> <code>(array([[44., 2.]]), array([10.])) (array([[44., 2.]]), array([23.])) <br></code> <br>  The average value in the first iterator call does not match the numbers on the basis of which it is calculated.  In fact, the average value was calculated correctly, but since  the array was passed by reference, then when the iterator was invoked for the second time, the values ​​in the array were overwritten, and the print () function returned, which was in the array, and not what we expected. <br><br>  There are two ways to fix this.  Both costly, but correct. <br>  1. Move the creation of the variable x inside the while loop so that the array at each yield creates a new one. <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_iterator_1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: x = np.zeros((batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(batch_size): x[i][<span class="hljs-number"><span class="hljs-number">0</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x[i][<span class="hljs-number"><span class="hljs-number">1</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x_mean = (x[::,<span class="hljs-number"><span class="hljs-number">0</span></span>] + x[::,<span class="hljs-number"><span class="hljs-number">1</span></span>]) / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> x, x_mean it_1 = train_iterator_1() print(next(it_1), next(it_1))</code> </pre><br> <code>(array([[82., 4.]]), array([43.])) (array([[77., 34.]]), array([55.5])) <br></code> <br><br>  2. Return a copy of the array. <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_iterator_2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> x = np.zeros((batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: x = np.zeros((batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(batch_size): x[i][<span class="hljs-number"><span class="hljs-number">0</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x[i][<span class="hljs-number"><span class="hljs-number">1</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x_mean = (x[::,<span class="hljs-number"><span class="hljs-number">0</span></span>] + x[::,<span class="hljs-number"><span class="hljs-number">1</span></span>]) / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> np.copy(x), x_mean it_2 = train_iterator_2() print(next(it_2), next(it_2))</code> </pre><br> <code>(array([[63., 31.]]), array([47.])) (array([[94., 25.]]), array([59.5])) <br></code> <br><br>  Now everything is fine.  Go ahead. <br><br>  Do I need to expand_dims?  Let's try to remove this line and the new code will be like this: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_iterator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">64</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: x = np.zeros((batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(batch_size): x[i][<span class="hljs-number"><span class="hljs-number">0</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x[i][<span class="hljs-number"><span class="hljs-number">1</span></span>] = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) x_mean = (x[::,<span class="hljs-number"><span class="hljs-number">0</span></span>] + x[::,<span class="hljs-number"><span class="hljs-number">1</span></span>]) / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> [x], [x_mean]</code> </pre><br>  Everything is great at learning, although the returned data has a different shape. <br><br>  For example, it was [[49.]], and it became [49.], but inside Keras, this seems to be correctly reduced to the desired dimension. <br><br>  So, we know what the correct data generator should look like, now let's play with the lambda function, and look at the behavior of expand_dims there. <br><br>  We will not predict anything, just consider the correct value inside lambda. <br><br>  The code is as follows: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calc_mean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> res = (x[::,<span class="hljs-number"><span class="hljs-number">0</span></span>] + x[::,<span class="hljs-number"><span class="hljs-number">1</span></span>]) / <span class="hljs-number"><span class="hljs-number">2</span></span> res = K.expand_dims(res, <span class="hljs-number"><span class="hljs-number">-1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> res <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> x = Input(name = <span class="hljs-string"><span class="hljs-string">'x'</span></span>, shape=(<span class="hljs-number"><span class="hljs-number">2</span></span>,)) x_mean = Lambda(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: calc_mean(x), output_shape=(<span class="hljs-number"><span class="hljs-number">1</span></span>,))(x) model = Model(inputs=x, outputs=x_mean) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre><br>  We start and see that everything is fine: <br><br><pre> <code class="plaintext hljs">Epoch 1/5 100/100 [==============================] - 0s 3ms/step - loss: 0.0000e+00 Epoch 2/5 100/100 [==============================] - 0s 2ms/step - loss: 0.0000e+00 Epoch 3/5 100/100 [==============================] - 0s 3ms/step - loss: 0.0000e+00</code> </pre><br>  Now let's try a little change our lambda function and remove the expand_dims. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calc_mean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> res = (x[::,<span class="hljs-number"><span class="hljs-number">0</span></span>] + x[::,<span class="hljs-number"><span class="hljs-number">1</span></span>]) / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> res</code> </pre><br>  When compiling the model, there were no errors on the dimension, but the result is different, the loss is considered incomprehensible as.  Thus, here expand_dims needs to be done, nothing will automatically happen. <br><br><pre> <code class="plaintext hljs">Epoch 1/5 100/100 [==============================] - 0s 3ms/step - loss: 871.6299 Epoch 2/5 100/100 [==============================] - 0s 3ms/step - loss: 830.2568 Epoch 3/5 100/100 [==============================] - 0s 2ms/step - loss: 830.8041</code> </pre><br>  And if you look at the returned result of predict (), you can see that the dimension is wrong, the output is [46.], and it is expected [[46.]]. <br><br>  Something like this.  Thanks to everyone who read it.  And be careful in the details, the effect of them can be significant. </div>