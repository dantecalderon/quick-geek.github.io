<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The book "Machine learning and TensorFlow"</title>
  <meta name="description" content="Acquaintance with machine learning and the TensorFlow library is similar to the first lessons in a driving school, when you are suffering with paralle...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>The book "Machine learning and TensorFlow"</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habr.com/ru/company/piter/blog/437964/"><img src="https://habrastorage.org/webt/cy/lg/n3/cylgn38lpjyhq7gveb9cmmt_ml8.jpeg" align="left" alt="image"></a>  Acquaintance with machine learning and the TensorFlow library is similar to the first lessons in a driving school, when you are suffering with parallel parking, trying to change gear at the right time and not confuse the mirrors, feverishly recalling the sequence of actions while your foot nervously twitches on the gas pedals.  This is a difficult but necessary exercise.  The same is true of machine learning: before using modern facial recognition systems or forecasting algorithms in the stock market, you will have to deal with the appropriate tools and set of instructions in order to create your own systems without any problems. <br><br>  Newcomers to machine learning will appreciate the applied focus of this book, because its goal is to introduce the basics so that they can quickly get down to solving real problems.  From a review of machine learning concepts and principles of working with TensorFlow, you will go to basic algorithms, study neural networks, and be able to independently solve the problems of classification, clustering, regression, and forecasting. <br><a name="habracut"></a><br><h3>  Excerpt  Convolutional neural networks </h3><br>  Shopping in stores after a grueling day is a very burdensome task.  My eyes are attacked by too much information.  Sales, coupons, a variety of colors, small children, flickering lights and human-filled aisles are just a few examples of all the signals that are sent to the visual cortex of the brain, regardless of whether I want or don‚Äôt want to pay attention to it.  The visual system absorbs a wealth of information. <br><br>  Surely you know the phrase "better to see once than hear a hundred times."  This may be true for you and for me (that is, for people), but can the machine find meaning in the images?  Our visual photoreceptors select the wavelengths of light, but this information does not seem to extend to our consciousness.  In the end, I can not say exactly what the length of the light waves are watching.  Similarly, the camera receives image pixels.  But we want to instead receive something of a higher level, such as the names or positions of objects.  How do we get information from the pixels perceived at the human level? <br><br>  To get some sense from the source data, you need to design a model of a neural network.  In the previous chapters, several types of neural network models were introduced, such as fully connected models (Chapter 8) and auto-encoders (Chapter 7).  In this chapter, we will introduce another type of model called the convolutional neural network (CNN).  This model works great with images and other sensory data, such as sound.  For example, a CNN model can reliably classify which object is displayed in a picture. <br><br>  The CNN model, which will be discussed in this chapter, will be trained to classify images into one of 10 possible categories.  In this case, ‚Äúthe picture is better than just one word,‚Äù since we have only 10 possible options.  This is a tiny step towards perception at the human level, but we have to start with something, right? <br><br><h3>  9.1.  Disadvantages of neural networks </h3><br>  Machine learning is an eternal struggle for the development of a model that would be sufficiently expressive for presenting data, but at the same time it was not so universal as to reach retraining and memorizing patterns.  Neural networks are offered as a way to increase expressiveness;  although, as you might guess, they suffer greatly from retraining traps. <br><br><blockquote>  NOTE Overtraining occurs when a trained model is exceptionally accurate on a training data set and bad on a test data set.  This model is probably overly universal for the small amount of data available, and in the end it simply remembers the training data. </blockquote><br>  To compare the versatility of the two machine learning models, you can use a fast and coarse heuristic algorithm to calculate the number of parameters that need to be determined as a result of the training.  As shown in fig.  9.1, a fully connected neural network that takes a 256 √ó 256 image and maps it onto a layer of 10 neurons, will have 256 √ó 256 √ó 10 = 655 360 parameters!  Compare it with a model containing only five parameters.  It can be assumed that a fully connected neural network can represent more complex data than a model with five parameters. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_c/ey/qe/_ceyqe8jc1s-xzasfzbv4vezhum.png" alt="image"></div><br>  The following section discusses convolutional neural networks, which are a reasonable way to reduce the number of parameters.  Instead of engaging in fully connected networks, CNN reuses the same parameters repeatedly. <br><br><h3>  9.2.  Convolutional neural networks </h3><br>  The main idea underlying convolutional neural networks is that local understanding of the image is sufficient.  The practical advantage of convolutional neural networks is such that, having several parameters, it is possible to significantly reduce the time for training, as well as the amount of data necessary for training the model. <br><br>  Instead of fully connected networks with weights from each pixel, CNN has a sufficient number of weights needed to view a small portion of the image.  It is like reading a book with a magnifying glass: in the end, you read the entire page, but at any given time only look at a small piece of it. <br><br>  Imagine a 256 √ó 256 image. Instead of using the TensorFlow code that processes the entire image at once, you can scan an image fragment by fragment, say a 5 √ó 5 window. A 5 √ó 5 window slides through the image (usually from left to right and from top to bottom) as shown in fig.  9.2.  How ‚Äúfast‚Äù it slides is called stride length.  For example, step 2 length means that a 5 √ó 5 sliding window moves 2 pixels at a time until the entire image passes.  In TensorFlow, as will be shown shortly, you can adjust the step length and window size using the built-in function library. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tl/lj/br/tlljbr-3r61p02umh30yi-wtmhi.png" alt="image"></div><br>  This 5 √ó 5 window has an associated 5 √ó 5 weight matrix. <br><br><blockquote>  DEFINITION A convolution is a weighted summation of the image pixel intensity values ‚Äã‚Äãas the window passes through the entire image.  It turns out that this process of convolving an image with a weights matrix creates another image (of the same size, which depends on convolution).  Coagulation is the process of applying convolution. </blockquote><br>  All manipulations of the sliding window occur in the convolutional layer of the neural network.  A typical convolutional neural network has several convolutional layers.  Each convolutional layer usually creates many additional convolutions, therefore the matrix of weighting factors is a 5 √ó 5 √ó n tensor, where n is the number of convolutions. <br><br>  As an example, let the image pass through a convolutional layer with a matrix of weight coefficients 5 √ó 5 √ó 64 in size. This creates 64 convolutions by a sliding window of 5 √ó 5. Therefore, the corresponding model has 5 √ó 5 √ó 64 = 1600 parameters, which is significantly less than the number of parameters of a fully meshed network : 256 √ó 256 = 65,536. <br><br>  The attractiveness of convolutional neural networks (CNN) is that the number of parameters used by the model does not depend on the size of the original image.  You can perform the same convolutional neural network on images of 300 √ó 300, and the number of parameters in the convolutional layer will not change! <br><br><h3>  9.3.  Image preparation </h3><br>  Before using the CNN model with TensorFlow, let's prepare several images.  The listings in this section will help you set up a training dataset for the remainder of the chapter. <br><br>  First of all, download the CIFAR-10 <a href="http://www.cs.toronto.edu/~kriz/cifar-10-">dataset</a> from <a href="http://www.cs.toronto.edu/~kriz/cifar-10-">www.cs.toronto.edu/~kriz/cifar-10-</a> python.tar.gz.  This set contains 60,000 images evenly distributed in 10 categories, which represents a sufficiently large resource for classification tasks.  Then the image file should be placed in the working directory.  In fig.  9.3 shows examples of images from this dataset. <br><br>  We already used the CIFAR-10 dataset in the previous chapter on auto-encoders, and now we‚Äôll look at this code again.  The following listing is taken directly from the CIFAR-10 documentation located at <a href="http://www.cs.toronto.edu/~kriz/cifar.html">www.cs.toronto.edu/~kriz/cifar.html</a> .  Place the code in the cifar_tools.py file. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/rd/6n/xsrd6nmuunpim-aeqepzxwj1acy.png" alt="image"></div><br>  Listing 9.1.  Uploading images from a CIFAR-10 file to Python <br><br><pre><code class="plaintext hljs">import pickle def unpickle(file): fo = open(file, 'rb') dict = pickle.load(fo, encoding='latin1') fo.close() return dict</code> </pre> <br>  Neural networks are prone to retraining, so it is important to do everything possible to minimize this error.  To do this, do not forget to perform data cleansing before processing. <br><br>  Data cleansing is the main process of machine learning pipelines.  The code in Listing 9.2 uses the following three steps to clean the image set: <br><br>  1. If you have an image in color, try converting it to grayscale to reduce the dimension of the input data and, consequently, reduce the number of parameters. <br><br>  2. Consider cropping the image in the center, because the edges of the image do not provide any useful information. <br><br>  3. Normalize the input data by subtracting the average and dividing by the standard deviation of each data sample so that the gradients during back propagation do not change too dramatically. <br><br>  The following listing shows how to clear a dataset using these methods. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ba/nf/al/banfal_9g-9gy2lns0q_qv2qce8.png" alt="image"></div><br>  Save all the images from the CIFAR-10 dataset and run the clear function.  The following listing specifies a convenient method for reading, cleaning, and structuring data for use in TensorFlow.  There should also include the code from the file cifar_tools.py. <br><br>  Listing 9.3.  Pre-processing of all CIFAR-10 files <br><br><pre> <code class="plaintext hljs">def read_data(directory): names = unpickle('{}/batches.meta'.format(directory))['label_names'] print('names', names) data, labels = [], [] for i in range(1, 6): filename = '{}/data_batch_{}'.format(directory, i) batch_data = unpickle(filename) if len(data) &gt; 0: data = np.vstack((data, batch_data['data'])) labels = np.hstack((labels, batch_data['labels'])) else: data = batch_data['data'] labels = batch_data['labels'] print(np.shape(data), np.shape(labels)) data = clean(data) data = data.astype(np.float32) return names, data, labels</code> </pre> <br>  In the using_cifar.py file, you can use the method by importing cifar_tools for this.  Listings 9.4 and 9.5 show how to select multiple images from a dataset and visualize them. <br><br>  Listing 9.4.  Using the helper function cifar_tools <br><br><pre> <code class="plaintext hljs">import cifar_tools names, data, labels = \ cifar_tools.read_data('your/location/to/cifar-10-batches-py')</code> </pre> <br>  You can randomly select multiple images and draw them according to the label.  The following listing does just that, so you can better understand the type of data you are dealing with. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/3c/ls/9s3clsibt-fy4tqxlnmggitpydm.png" alt="image"></div><br>  By running this code, you will create a file cifar_examples.png, which will look like pic.  9.3. <br><br>  ¬ªMore information about the book can be found on <a href="https://www.piter.com/collection/all/product/mashinnoe-obuchenie-i-tensorflow%3F_gs_cttl%3D120%26gs_direct_link%3D1%26gsaid%3D82744%26gsmid%3D29789%26gstid%3Dc">the publisher's website.</a> <br>  ¬ª <a href="https://storage.piter.com/upload/contents/978544610826/978544610826_X.pdf">Table of Contents</a> <br>  ¬ª <a href="https://storage.piter.com/upload/contents/978544610826/978544610826_p.pdf">Excerpt</a> <br><br>  For Habrozhiteley a 20% discount on coupon - <b>Machine Learning</b> </div><p>Source: <a href="https://habr.com/ru/post/437964/">https://habr.com/ru/post/437964/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>