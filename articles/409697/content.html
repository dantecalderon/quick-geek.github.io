<div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/qu/r7/29/qur729mzadpog93xrhjwkj9c51i.jpeg"></p><br><p>  Over the past decade, deep neural networks (Deep Neural Networks, DNN) have become an excellent tool for a number of AI tasks such as image classification, speech recognition, and even participation in games.  As developers tried to show how DNN’s success in the field of image classification was created and they created visualization tools (for example, Deep Dream, Filters) to help them understand what the DNN model was “studying”, a new interesting application emerged. : extracting “style” from one image and applying to another, different content.  This is called “visual style transfer” (image style transfer). </p><a name="habracut"></a><br><p><img src="https://habrastorage.org/webt/ma/w_/uy/maw_uyr88ft3g7oiwpx8yp6vfwi.jpeg"><br>  <em>Left: image with useful content, in the center: image with style, right: content + style (source: <a href="https://research.googleblog.com/2016/02/exploring-intersection-of-art-and.html">Google Research Blog</a> )</em> </p><br><p>  This not only stirred the interest of many other researchers (for example, <a href="https://link.springer.com/chapter/10.1007/978-3-319-46475-6_43">1</a> and <a href="https://arxiv.org/pdf/1609.03057.pdf">2</a> ), but also led to the emergence of several successful mobile applications.  Over the past couple of years, these methods of transferring the visual style have greatly improved. </p><br><p><img src="https://habrastorage.org/webt/ij/oz/7-/ijoz7-xp5fibweiwdt3skobj_cy.jpeg"><br>  <em>Transferring style from Adobe (source: <a href="https://www.engadget.com/2017/03/30/adobes-experimental-app-copies-one-photos-style-to-another/">Engadget</a> )</em> </p><br><p><img src="https://habrastorage.org/webt/rx/sx/gc/rxsxgcb9dxcf7gk5iaq6e2dkjbo.jpeg"><br>  <em><a href="https://prisma-ai.com/p/fac6d4e6-2064-4264-a539-cc2d3e6c30b8">Prisma</a> example</em> </p><br><p>  A short introduction to the work of such algorithms: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/WHmp26bh0tI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  However, despite the advances in working with images, the application of these techniques in other areas, for example, for processing music, was very limited (see <a href="https://audiostyletransfer.wordpress.com/">3</a> and <a href="https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/">4</a> ), and the results are not at all as impressive as in the case of images.  This suggests that in music to transfer the style is much more difficult.  In this article we will look at the problem in more detail and discuss some possible approaches. </p><br><h2 id="pochemu-tak-trudno-perenosit-stil-v-muzyke">  Why is it so difficult to endure style in music? </h2><br><p>  Let's first answer the question: <strong>what is “style transfer” in music</strong> ?  The answer is not so obvious.  In the images, the concepts of “content” and “style” are intuitive.  “Image content” describes the objects represented, for example, dogs, houses, faces, and so on, and “image style” refers to colors, lighting, brush strokes, and texture. </p><br><p>  However, music is by its nature <strong>semantically abstract and multidimensional</strong> .  “Music Content” can mean different things in different contexts.  Often the music content is associated with the melody, and the style - with the arrangement or harmonization.  However, the content may be the lyrics of the song, and the different melodies used for singing can be interpreted as different styles.  In classical music, the content can be considered a score (including harmonization), while the style will be the interpretation of the notes by the performer who brings his own expression (varying and adding some sounds from himself).  To better understand the essence of the transfer of style in music, watch a couple of these videos: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/S75gYhODS0M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><iframe width="560" height="315" src="https://www.youtube.com/embed/buXqNqBFd6E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  In the second video, various machine learning <a href="https://youtu.be/0qnTaAz-xtQ">techniques are</a> used. </p><br><p>  So, the transfer of style in music is by definition difficult to formalize.  There are other key factors complicating the task: </p><br><ol><li>  <strong>BAD machines understand music</strong> (for now): success in porting style in images stems from the success of DNN in tasks related to understanding images, for example, recognizing objects.  Since DNNs can learn properties that vary across different objects, back propagation techniques can be used to modify the target image to match the properties of the content.  Although we have made significant <a href="">progress</a> in creating models based on DNN, capable of understanding musical tasks (for example, transcribing a melody, defining a genre, etc.), we are still far from the heights achieved in image processing.  This is a serious obstacle to the transfer of style in music.  Existing models simply can not learn the "excellent" properties that allow to classify music, and therefore the direct use of style transfer algorithms used when working with images, does not give the same result. </li><li>  Music is <strong>transient</strong> : it is data that represents a dynamic series, that is, a piece of music changes with time.  This complicates learning.  And although recurrent neural networks and <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a> (Long Short-Term Memory, long short-term memory) make it possible to study more efficiently with transient data, we still have to create reliable models that can learn to reproduce the long-term structure of music (note: this is a current research direction, and scientists from Google Magenta achieved some success in <a href="https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn">this</a> ). </li><li>  Music is <strong>discrete</strong> (at least at the symbolic level): character, or music recorded on paper is inherently discrete.  In a <a href="https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn">uniformly tempered system</a> , the most popular system for tuning musical instruments today, sound tones occupy discrete positions on a continuous scale of frequencies.  At the same time, the duration of the tones also lies in the discrete space (they usually single out quarter tones, full tones, and so on).  Therefore, it is very difficult to adapt pixel back-propagation methods (used for working with images) in the field of symbolic music. </li></ol><br><p><img src="https://habrastorage.org/webt/tb/cj/a0/tbcja0feucuwkgawqqlxqvfqtna.png"><br>  <em>The discrete nature of musical notes in a uniformly tempered structure.</em> </p><br><p>  Consequently, the techniques used to transfer the style in images are not directly applicable in music.  To do this, they need to recycle with an emphasis on musical concepts and ideas. </p><br><h2 id="dlya-chego-nuzhen-perenos-stilya-v-muzyke">  What is the transfer of style in music? </h2><br><p>  Why do we need to solve this problem?  As in the case of images, the potential applications of style transfer in music are quite interesting.  For example, the <strong>development of a tool to help composers</strong> .  For example, an automatic instrument that can transform a melody using arrangements from different genres will be extremely useful for composers who need to quickly try out different ideas.  Interested in such instruments and DJs. </p><br><p>  The indirect result of such research will be a significant improvement in musical informatics systems.  As explained above, in order for the transfer of style to work in music, the models we create must learn to better <strong>“understand”</strong> different aspects. </p><br><h2 id="uproschenie-zadachi-perenosa-stilya-v-muzyke">  Simplify the task of transferring style in music </h2><br><p>  Let's start with a very simple task of analyzing monophonic melodies in different genres.  Monophonic melodies are sequences of notes, each of which is determined by tone and duration.  The tone progression mostly depends on the melody scale, and the duration progression depends on the rhythm.  So, first, we clearly <strong>distinguish between “</strong> pitch content” and <strong>“rhythmic style”</strong> (rhythmic style) as two entities with which you can rephrase the style transfer task.  Also, when working with monophonic melodies, we will now avoid tasks related to arrangement and text. </p><br><p>  In the absence of pre-trained models that can successfully distinguish between tonal progression and rhythms of monophonic melodies, we first resort to a very simple approach to transferring style.  Instead of trying to change the tone content learned on the target melody, the rhythm style learned on the target rhythm, we will try individually to teach patterns of tones and durations from different genres, and then try to combine them.  Approximate approach scheme: </p><br><p><img src="https://habrastorage.org/webt/ek/i8/3o/eki83obepvf1nd44pgzupr-czmu.png"><br>  <em>Diagram of inter-genre transfer method of style.</em> </p><br><h2 id="obuchaem-otdelno-tonovym-i-ritmovym-progressiyam">  We teach separately tonal and rhythmic progression </h2><br><p>  <strong><em>Data presentation</em></strong> </p><br><p>  We will present monophonic melodies as a sequence of musical notes, each of which has an index of tone and sequence.  In order for our presentation key to be independent, we use the interval-based representation: the tone of the next note will be represented as a deviation (± semitones) from the tone of the previous note.  Create two dictionaries for tones and durations in which each discrete state (for a tone: +1, -1, +2, -2, and so on; for durations: a quarter note, full note, quarter with a dot, and so on) assigned an index dictionary. </p><br><p><img src="https://habrastorage.org/webt/v2/h6/z6/v2h6z6pbrq6ezu9br3jv_actjia.png"><br>  <em>Presentation of data.</em> </p><br><p>  <strong><em>Model architecture</em></strong> </p><br><p> We will use the same architecture that <a href="http://arxiv.org/abs/1606.07251">Colombo and colleagues used</a> — they simultaneously taught two LSTM neural networks to the same music genre: a) the tone network learned to predict the next tone based on the previous note and the previous duration, b) the duration network learned to predict the next duration based on the next note and previous duration.  Also, before the LSTM networks, we will add embedding layers (embedding layer) to match the input tone and duration indices in memorized embedding spaces.  The neural network architecture is shown in the picture: </p><br><p><img src="https://habrastorage.org/webt/6q/fg/nk/6qfgnkiake_rls5yymzbvxdvzg0.png"></p><br><p>  <strong><em>Learning procedure</em></strong> </p><br><p>  For each genre, the networks responsible for tones and durations are trained simultaneously.  We will use two datasets: a) <a href="http://www.norbeck.nu/abc/">Norbeck Folk Dataset</a> , covering about 2000 Irish and Swedish folk tunes, b) jazz dataset (not publicly available), covering about 500 jazz tunes. </p><br><p>  <strong><em>Merging trained models</em></strong> </p><br><p>  When testing, the melody is first generated using a tone network and a network of durations that are trained in the first genre (say, folk).  Then the sequence of tones from the generated melody is used at the input for a network of sequences trained in a different genre (say, jazz), and the result is a new sequence of durations.  Therefore, a melody created using a combination of two neural networks has a sequence of tones corresponding to the first genre (folk), and a sequence of durations corresponding to the second genre (jazz). </p><br><h2 id="predvaritelnye-rezultaty">  Preliminary results </h2><br><p>  Short excerpts from some of the resulting tunes: <br>  <a href="">Folk-tones and folk-duration</a> </p><br><p><img src="https://habrastorage.org/webt/c9/ic/zv/c9iczvv4qkjjv6lve-sj5wnb3dk.png"><br>  <em>Excerpt from musical notation.</em> </p><br><p>  <a href="">Folk tones and jazz duration</a> </p><br><p><img src="https://habrastorage.org/webt/-p/oz/57/-poz57oqe5kpunzo3jqpikcjjju.png"><br>  <em>Excerpt from musical notation.</em> </p><br><p>  <a href="">Jazz tones and jazz sequences</a> </p><br><p><img src="https://habrastorage.org/webt/8s/ia/4x/8sia4xupwzfbnz2tyzof4xw14li.png"><br>  <em>Excerpt from musical notation</em> . </p><br><p>  <a href="">Jazz tones and folk sequences</a> </p><br><p><img src="https://habrastorage.org/webt/hf/mh/8p/hfmh8p9kyorpupmmvydj_psxb9u.png"><br>  <em>Excerpt from musical notation.</em> </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  Although the current algorithm is not bad for a start, it has some critical flaws: </p><br><ol><li>  <strong>It is impossible to "transfer style" based on a specific target melody</strong> .  Models learn patterns of tones and durations on the genre, which means all transformations are determined by the genre.  It would be ideal to change a piece of music in the style of a specific target song or piece. </li><li>  <strong>Unable to control the degree of</strong> style change.  It would be very interesting to get a “handle” that controls this aspect. </li><li>  When merging genres, <strong>it is impossible to preserve the musical structure</strong> in the transformed melody.  The long-term structure is important for musical evaluation as a whole, and for the generated melodies to be musically aesthetic, the structure must be preserved. </li></ol><br><p>  In future articles, we will look at ways to circumvent these shortcomings. </p></div>