<div class="post__text post__text-html js-mediator-article"><p>  Article written in collaboration with <a href="https://habr.com/ru/users/ananaskelly/" class="user_link">ananaskelly</a> . </p><br><h3 id="vvedenie">  Introduction </h3><br><p>  Hello everyone, Habr!  Working at the Speech Technology Center in St. Petersburg, we have gained a bit of experience in solving problems of classifying and detecting acoustic events and decided that we are ready to share it with you.  The purpose of this article is to introduce you to some tasks and talk about the <a href="http://dcase.community/challenge2018/index">“DCASE 2018”</a> automatic audio processing competition.  Telling you about the competition, we will do <u>without complex formulas and definitions</u> related to machine learning, so the general meaning of the article will be understood by a <u>wide audience</u> . </p><br><p>  For those who were attracted by the <b>classifier assembly</b> in the title, we prepared a small code in python, and from the link on the <a href="https://github.com/Ananaskelly/Event-detection">githaba</a> you can find a notebook, where we use the example of the <a href="https://habr.com/ru/company/speechpro/blog/437818/">second track</a> of the DCASE contest to create a simple convolutional network on keras for classifying audio files.  There we talk a little about the network and the features used for training, and how to get a result close to the baseline using simple architecture ( <a href="https://www.kaggle.com/wendykan/map-k-demo">MAP @ 3</a> = 0.6). </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cy/xk/ct/cyxkct2xgahwpajkvi4sxzvallu.png"></div><br><p>  Additionally, there will be described basic approaches for solving problems (baseline) proposed by the organizers.  Also in the future there will be several articles where we will talk in more detail and in detail both about our experience of participating in the competition, and about the solutions proposed by other participants of the competition.  Links to these articles will gradually appear here. </p><a name="habracut"></a><br><p>  Surely, many people have no idea of ​​any kind of <b>“DCASE” there</b> , so let's see what kind of fruit it is and what it is eaten with.  The “ <abbr title="Detection and Classification of Acoustic Scenes and Events (Detection and Classification of Acoustic Scenes and Events)">DCASE</abbr> ” competition is held annually, and every year it is laid out several tasks devoted to solving problems in the field of classification of audio recordings and the detection of acoustic events.  Anyone can take part in the competition, it is free, it’s enough just to register on the site as a participant.  Following the results of the competition, a conference devoted to the same subject is held, but, unlike the competition itself, participation in it is already paid, and we will not talk about it anymore.  Cash rewards for the best solutions usually do not rely, but there are exceptions (for example, the 3rd task in 2018).  This year the organizers proposed the following 5 tasks: </p><br><ol><li>  <a href="https://habr.com/ru/company/speechpro/blog/437818/">Classification of acoustic scenes (divided into 3 subtasks)</a> <br>  A. The training and verification data sets are recorded on the same device. <br>  B. Training and test data sets are recorded on different devices. <br>  C. allowed training using data that was not offered by the organizers </li><li>  <a href="https://habr.com/ru/company/speechpro/blog/437818/">Classification of acoustic events</a> </li><li>  <a href="https://habr.com/ru/company/speechpro/blog/437818/">Detection of birds singing</a> </li><li>  <a href="https://habr.com/ru/company/speechpro/blog/437818/">Detection of acoustic events in living conditions using a weakly-marked data set</a> </li><li>  <a href="https://habr.com/ru/company/speechpro/blog/437818/">Classification of household activity in the room for multi-channel recording</a> </li></ol><br><h4 id="o-detektirovanii-i-klassifikacii">  About detection and classification </h4><br><p>  As we see, in the names of all tasks there is one of two words: “detection” or “classification”.  Let us clarify the difference between these concepts so that there is no confusion. </p><br><p>  Imagine that we have an audio recording in which a dog barks at one time, and a cat meows at another, and there are simply no other events.  Then if we want to understand exactly when these events occur, we need to solve the problem of detecting an acoustic event.  That is, we need to know the start and end times for each event.  Having solved the detection problem, we will find out when exactly the events take place, but we don’t know who exactly the sounds found are produced - then we need to solve the classification problem, that is, to determine what exactly happened in a given time interval. </p><br><p>  To understand the description of the tasks of the competition, these examples will be quite enough, which means that the introductory part is over, and we can proceed to a detailed description of the tasks themselves. </p><br><hr><br><h3 id="anchortrack1anchortrack-1-klassifikaciya-akusticheskih-scen"><a name="Track1"></a>  Track 1. Classification of acoustic scenes </h3><br><p>  The first task is to determine the environment (acoustic stage) in which the audio was recorded, for example, “Metro Station”, “Airport” or “Pedestrian Street”.  Solving such a problem can be useful in assessing the environment by an artificial intelligence system, for example, in cars with autopilot. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/db/4g/ff/db4gffctu9tgvd4upbse_eaaiwg.jpeg"></div><br><p>  In this task, TUT Urban Acoustic Scenes 2018 and TUT Urban Acoustic Scenes 2018 Mobile datasets were presented for training, which were prepared by the Tampere University of Technology (Finland).  A detailed description of the dataset preparation process, as well as the basic solution, is described in the <a href="https://arxiv.org/pdf/1807.09840.pdf">article</a> . </p><br><p>  In total, 10 acoustic scenes were presented for the competition, which were to be predicted by the participants. </p><br><h4 id="podzadacha-a">  Subtask A </h4><br><p>  As we have said, the task is divided into 3 subtasks, each of which is distinguished by the quality of audio recordings.  For example, in subtask A, special microphones were used for recording, which were located in human ears.  Thus, the stereo recording was closer to the human perception of sound.  Participants had the opportunity to use this approach to recording to improve the quality of recognition of the acoustic scene. </p><br><h4 id="podzadacha-v">  Subtask B </h4><br><p>  In subtask B, other devices were also used for recording (for example, mobile phones).  Data from subtask A has been converted to mono format, the sampling rate has been reduced, there is no imitation of the “audibility” of sound by a person in the data set for this task, but there is more data to learn. </p><br><h4 id="podzadacha-s">  Subtask C </h4><br><p>  The data set for subtask C is the same as in subtask A, but in solving this problem it is allowed to use any external data that the participant can find.  The goal of this task is to find out whether it is possible to improve the result obtained in subtask A, by attracting third-party data. </p><br><p>  The quality of decisions on this track was evaluated by the <a href="https://habr.com/company/ods/blog/328372/">Accuracy</a> metric. </p><br><p>  The baseline for this task is a two-layer <a href="http://cs231n.github.io/convolutional-networks/">convolutional neural network</a> that <a href="https://habr.com/post/140828/">studies the</a> log <a href="https://habr.com/post/140828/">spectra of the</a> original audio data on logarithms.  The proposed architecture uses the standard BatchNormalization and Dropout techniques.  The code on GitHub can be viewed <a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task1">here</a> . </p><br><hr><br><h3 id="anchortrack2anchortrack-2-klassifikaciya-akusticheskih-sobytiy"><a name="Track2"></a>  Track 2. Classification of acoustic events </h3><br><p>  In this task, it is proposed to create a system that performs the classification of acoustic events.  Such a system can be an addition to “smart” homes, increase safety in crowded places or make life easier for people with hearing impairments. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ut/r5/9_/utr59_-ugehiq9jyhe_hgpgzbrm.jpeg"></div><br><p>  The data set for this task consists of files taken from the <a href="https://freesound.org/">Freesound</a> data <a href="https://freesound.org/">set</a> and tagged using tags from Google's <a href="https://research.google.com/audioset/index.html">AudioSet</a> .  In more detail the process of preparing dataset is described in <a href="https://arxiv.org/abs/1807.09902">an article</a> prepared by the organizers of the competition. </p><br><p>  Let's return to the task itself, which has several features. </p><br><p>  First, the participants had to create a model capable of identifying differences between acoustic events of a very different nature.  The data set is divided into 41 classes, it presents various musical instruments, sounds made by humans, animals, household sounds, and so on. </p><br><p>  Secondly, besides the usual data markup, there is also additional information about manually checking the tag.  That is, participants know which files from the data set were checked by a person for compliance with the label, and which are not.  As practice has shown, the participants who in one way or another used this additional information occupied prizes in solving this problem. </p><br><p>  Additionally, it must be said that the length of records in a data set varies greatly: from 0.3 seconds to 30 seconds.  In this task, the amount of data per class also varies greatly, for which the model needs to be trained.  It is best to depict it in the form of a histogram, the code for which is taken <a href="https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data">from here</a> . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/r4/gq/twr4gqwzkdivbpzkipwh6jesugi.jpeg"></div><br><p>  As can be seen from the histogram, manual marking for the classes presented is also unbalanced, which adds difficulty if you wish to use this information when training models. <br>  The results in this track were estimated by the average accuracy metric (Mean Mean Precision, MAP @ 3), a fairly simple demonstration of the calculation of this metric with examples and code can be found <a href="https://www.kaggle.com/wendykan/map-k-demo">here</a> . </p><br><hr><br><h3 id="anchortrack3anchortrack-3-detektirovanie-ptichego-peniya"><a name="Track3"></a>  Track 3. Detection of bird singing </h3><br><p>  The next track is the detection of birds singing.  A similar problem arises, for example, in various systems of automatic monitoring of wilderness - this is the first step of data processing before, for example, classification.  Such systems often need to be tuned, they are unstable to new acoustic conditions, so the goal of this track is to invoke the power of machine learning to solve such problems. </p><br><p>  This track is an enhanced version of the <a href="http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/">“Bird Audio Detection challenge”</a> competition, organized by the St. Mary’s University of London in 2017/2018.  For those interested <a href="https://arxiv.org/abs/1807.05812">,</a> you can familiarize yourself with the article from the authors of the competition, which provides details about the formation of data, the organization of the competition itself and the analysis of the solutions obtained. </p><br><p>  However, back to the DCASE problem.  The organizers provided six data sets - three for training, three for testing - all of them are very different - recorded in different acoustic conditions, with the help of different recording devices, there are different noises in the background.  Thus, the main message is that the model should not depend on the environment or be able to adapt to it.  Despite the fact that the name means “detection”, the task is not to define the boundaries of the event, but to simple classification - the final solution is a kind of binary classifier that receives a short audio recording at the input and makes a decision whether there is a bird singing on it or not .  AUC metric was used to assess accuracy. </p><br><p>  In general, participants attempted to generalize and adapt through various data augmentation.  One of the commands <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Liaqat_96.pdf">describes the</a> use of various techniques - changing the frequency resolution in the extracted features, pre-noise cleaning, <a href="https://arxiv.org/abs/1511.05547">adaptation method</a> , based on the alignment of second-order statistics for different data sets.  However, such methods, as well as different types of augmentation, give a very small increase over the basic solution, as many participants note. </p><br><p>  As a basic solution, the authors prepared a modification of the most successful solution from the original competition “Bird Audio Detection challenge”.  The code, as usual, <a href="https://github.com/DCASE-REPO/bulbul_bird_detection_dcase2018">is available on github</a> . </p><br><hr><br><h3 id="anchortrack4anchortrack-4-detektirovanie-akusticheskih-sobytiy-v-bytovyh-usloviyah-s-ispolzovaniem-slabo-razmechennogo-nabora-dannyh"><a name="Track4"></a>  Track 4. Detection of acoustic events in living conditions using a weakly labeled data set. </h3><br><p> In the fourth track, the detection problem itself is solved directly.  The participants were given a relatively small dataset of marked data - only 1578 audio recordings of 10 seconds each, having only markup by classes: it is known that the file contains one or several events of the indicated classes, but the time marking is completely absent.  In addition, two large datasets of unallocated data were provided - 14412 files containing target events of the same classes as in the training and test sample, as well as 39999 files containing arbitrary events that are not included in the targetgroups.  All data is a subset of a <a href="https://research.google.com/audioset/">huge dataset of audioset collected by Google</a> . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lx/td/h7/lxtdh7uaqxktdqu2bxzd4drrq0q.jpeg"></div><br><p>  Thus, it was necessary for participants to create a model capable of learning from weakly marked data to find the time marks of the beginning and end of events (events can be intersecting) and try to improve it with a large amount of unmarked additional data.  In addition, it is worth noting that a fairly rigid metric was used in this track - it was necessary to predict the timestamps of events with an accuracy of 200 ms.  In general, the participants had to solve a rather complicated task of creating an adequate model, while practically having no good data for training. <br>  Most of the solutions were based on convolutional recurrent networks — a rather popular architecture in the field of detecting acoustic events lately (for an example, see the <a href="https://arxiv.org/pdf/1702.06286.pdf">link</a> ). </p><br><p>  The basic solution from the authors, also on convolutional recurrent networks, is based on two models.  The models have almost the same architecture: three convolutional and one recurrent layer.  The only difference is in the output of networks.  The first model is trained to mark up unallocated data to extend the source dataset - so, at the output, we have classes of events present in the file.  The second is to directly solve the problem of detection, that is, at the output we obtain the time markup for the file.  Code by <a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task4/">reference</a> . </p><br><hr><br><h3 id="anchortrack5anchortrack-5-klassifikaciya-bytovoy-aktivnosti-v-pomeschenii-po-mnogokanalnoy-zapisi"><a name="Track5"></a>  Track 5. Classification of household activity in the room for multi-channel recording. </h3><br><p>  The last track was different from the rest in the first place because the participants were offered multichannel recordings.  The task itself was classification: it is necessary to predict the class of the event that occurred on the record.  Unlike the previous track, the task is somewhat simpler - it is known that there is only one event in the recording. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hr/l1/ud/hrl1udue-onkcjfot9kptycgcw8.jpeg"></div><br><p>  Dataset is represented by approximately 200 hours of recordings on a linear microphone array of 4 microphones.  Events are all sorts of household activities - cooking, washing dishes, social activity (talking on the phone, visit and personal conversation), etc., also highlighted the absence of any class of events. </p><br><p>  The authors of the track emphasize that the conditions of the problem are relatively simple so that the participants focus directly on the use of spatial information from multi-channel records.  Participants were also given the opportunity to use additional data and pre-trained models.  Quality was evaluated by F1-measure. </p><br><p>  As a basic solution, the authors of the track proposed a simple convolutional network with two convolutional layers.  Spatial information was not used in their solution — data from four microphones was used for learning independently, and when testing, predictions were averaged.  Description and code are available <a href="https://github.com/DCASE-REPO/dcase2018_baseline/tree/master/task5">here</a> . </p><br><hr><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  In the article we tried to briefly talk about the detection of acoustic events and about such a competition as DCASE.  Perhaps we were able to interest someone to participate in 2019 - the competition starts in March. </p></div>