<div class="post__text post__text-html js-mediator-article"><p>  When I think about how naive RPC clients work, I am reminded of a joke: </p><br><blockquote>  Court. <br>  - Defendant, why did you kill a woman? <br>  - I'm on the bus, the conductor approaches the woman, demanding to buy a ticket.  A woman opened her purse, took out her purse, closed her purse, opened her purse, took out her purse, closed her purse, opened her purse, put her purse in there, closed her purse, opened her purse, took out money, opened her purse, opened her purse, closed her purse, opened her purse, put there purse , closed the purse, opened her purse, put the purse in there. <br>  - So what? <br>  - The controller gave her a ticket.  A woman opened her purse, took out her purse, closed her purse, opened her purse, took out her purse, closed her purse, opened her purse, put her purse in there, closed her purse, opened her purse, put there ticket, closed her purse, opened her purse, opened her purse , put the purse in there, closed the purse, opened the purse, put the purse in there, closed the purse. <br>  “Take the change,” came the voice of the controller.  A woman ... opened her purse ... <br>  - Yes, it is not enough to kill her, - the prosecutor does not stand up. <br>  - So I did it. <br>  © S.Altov </blockquote><p><img src="https://habrastorage.org/webt/qx/ne/-b/qxne-bocftxvmk99ouizmn3iqyo.jpeg"></p><a name="habracut"></a><br><p>  Approximately the same thing happens in the "request-response" process, if this is not taken seriously: </p><br><ul><li>  the user process writes a serialized request "to the socket", actually copying it into the socket buffer at the OS level; <br>  This is a rather difficult operation.  it is necessary to make a context switch (even if it can be “easy”); </li><li>  when it seems to OSes that something can be written to the network, a packet is formed (the request is copied again) and sent to the network card; </li><li>  the network card writes the packet to the network (possibly, pre-buffering); </li><li>  (along the way, the packet may be buffered several times in routers); </li><li>  finally, the packet arrives at the destination host and is buffered on the network card; </li><li>  the network card sends a notification to the operating system, and when the operating system finds the time, it copies the packet to its buffer and sets the ready flag on the file descriptor; </li><li>  (we must still remember to send an ACK in response); </li><li>  after some time, the server application realizes that readiness is on the descriptor (using epoll), and someday copies the request to the application buffer; </li><li>  and finally, the server application handles the request. </li></ul><br><p>  As you understand, the transmission of the answer occurs in exactly the same way only in the opposite direction.  Thus, each request spends a considerable amount of time on the operating system for its service, and each answer spends the same time again. </p><br><p>  This became especially noticeable after Meltdown / Specter, since the released patches led to a strong increase in the cost of system calls.  In early January 2018, our Redis cluster suddenly began to consume a half to two times more CPU.  Amazon has applied the appropriate kernel patches to close these vulnerabilities.  (True, Amazon later applied a new version of the patch, and the CPU consumption decreased almost to previous levels. But the connector has already begun to be born.) </p><br><p>  Unfortunately, all the widely known Go connectors to Redis and Memcached work this way: the connector creates a pool of connections, and when you need to send a request, it pulls the connection from the pool, writes one request to it, and then waits for a response.  (It’s especially sad that the memcached connector was written by Brad Fitzpatrick himself.) And some connectors have such an unsuccessful implementation of this pool that the process of withdrawing a connection from the pool becomes a botnet itself. </p><br><p>  There are two ways to alleviate this hard work of sending a request / response in several ways: </p><br><ol><li>  Use direct access to the network map: DPDK, netmap, PF_RING, etc. </li><li>  Do not send each request / response in a separate package, but combine them into larger packages if possible, that is, spread out the overhead of working with the network on several requests.  Together more fun! </li></ol><br><p>  The first option, of course, is possible.  But, first, it is for the brave in spirit, because you have to write the TCP / IP implementation yourself (for example, as in ScyllaDB).  And secondly, in this way we facilitate the situation only on one side - on the one we write ourselves.  I still do not want to rewrite Redis (for now), so the servers will consume as much, even if the client uses the cool DPDK. </p><br><p>  The second option is much simpler, and most importantly, it eases the situation immediately, both on the client and on the server.  For example, <a href="https://tarantool.io/ru/developers/">one in-memory database</a> boasts that it can serve millions of RPS, while Redis cannot serve a <a href="https://redis.io/topics/benchmarks">couple of hundreds of thousands</a> .  However, this success is not so much the realization of that in-memory database as the decision taken once that the protocol will be completely asynchronous, and clients should use this asynchrony whenever possible.  What many clients (especially used in benchmarks) successfully implement by sending requests through one TCP connection and, if possible, sending them to the network together. </p><br><p>  <a href="https://redis.io/topics/pipelining">A well-known article</a> shows that Redis can also give a million responses per second if pipelining is used.  Personal experience in the development of in-memory storadzhey also suggests that pipelining significantly reduces the consumption of SYS CPU and allows much more efficient use of the processor and the network. </p><br><p>  The only question is how to use pipelining, if in the application requests to Redis are often received one at a time?  And if one server is missing and Redis Cluster is used with a large number of shards, then even when a packet of requests is encountered, it splits into single requests for each shard. </p><br><p>  The answer, of course, is “obvious”: do implicit pipelaying by collecting requests from all parallel working gorutin to one Redis server and sending them through one connection. </p><br><p>  By the way, implicit pipelining is not so rare in connectors in other languages: nodejs <a href="https://github.com/NodeRedis/node_redis">node_redis</a> , C # <a href="https://github.com/andrew-bn/RedisBoost">RedisBoost</a> , python's <a href="https://github.com/aio-libs/aioredis">aioredis,</a> and many others.  Many of these connectors are written on top of event loops, and therefore the collection of requests from parallel “calculation threads” looks natural there.  Go also promotes the use of synchronous interfaces, and apparently because very few people decide to organize their own loops. </p><br><p>  We wanted to use Redis as efficiently as possible and therefore we decided to write a new “better” (tm) connector: <a href="https://github.com/joomcode/redispipe">RedisPipe</a> . </p><br><h2 id="kak-sdelat-neyavnyy-payplayning">  How to make implicit paylayning? </h2><br><p>  Basic scheme: </p><br><ul><li>  application logic of gorutin do not write requests directly to the network, but transfer them to the gorutine-collector; </li><li>  the collector collects a packet of requests whenever possible, writes them to the network, and transfers them to the gorutine reader; </li><li>  Goretin-reader reads the answers from the network, compares them with the corresponding requests, and notifies the logic of the arrived answer. </li></ul><br><p> You must somehow notify the reply.  A smart programmer on Go will of course say: “Through the channel!” <br>  But this is not the only possible synchronization primitive and not the most efficient even in the Go environment.  And since the needs of different people are different, we will make the mechanism extensible, allowing the user to implement the interface (let's call it <code>Future</code> ): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Future <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span> { Resolve(val <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span>{}) }</code> </pre> <br><p>  And then the basic scheme will look like this: </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> future <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { req Request fut Future } <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Conn <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c net.Conn futmtx sync.Mutex wfutures []future futtimer *time.Timer rfutures <span class="hljs-keyword"><span class="hljs-keyword">chan</span></span> []future } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Send</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r Request, f Future)</span></span></span></span> { c.futmtx.Lock() <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> c.futmtx.Unlock() c.wfutures = <span class="hljs-built_in"><span class="hljs-built_in">append</span></span>(c.wfutures, future{req: r, fut: f}) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(c.wfutures) == <span class="hljs-number"><span class="hljs-number">1</span></span> { futtimer.Reset(<span class="hljs-number"><span class="hljs-number">100</span></span>*time.Microsecond) } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">writer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> c.futtimer.C { c.futmtx.Lock() futures, c.wfutures = c.wfutures, <span class="hljs-literal"><span class="hljs-literal">nil</span></span> c.futmtx.Unlock() <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> b []<span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, ft := <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> futures { b = AppendRequest(b, ft.req) } _, _err := ccWrite(b) c.rfutures &lt;- futures } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">reader</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { rd := bufio.NewReader(cc) <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> futures []future <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> { response, _err := ParseResponse(rd) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(futures) == <span class="hljs-number"><span class="hljs-number">0</span></span> { futures = &lt;- c.rfutures } futures[<span class="hljs-number"><span class="hljs-number">0</span></span>].fut.Resolve(response) futures = futures[<span class="hljs-number"><span class="hljs-number">1</span></span>:] } }</code> </pre> <br><p>  Of course, this is a very simplified code.  Omitted: </p><br><ul><li>  connection setup; </li><li>  I / O timeouts; </li><li>  error handling on read / write; </li><li>  re-establishing the connection; </li><li>  the ability to cancel the request before sending it to the network; </li><li>  optimization of memory allocation (reuse of the buffer and futures arrays). </li></ul><br><p>  Any input-output error (including timeout) in the real code leads to a rezolv error of all Future, the corresponding send and waiting for sending requests. <br>  The connection layer does not re-request requests, and if you need (and can) re-repeat the request, it can be done at the higher level of abstraction (for example, in the implementation of Redis Cluster support described below). </p><br><p>  Remark.  Initially, the scheme looked a little more complicated.  But in the process of experiments simplified to this option. </p><br><p>  Remark 2. There are very strict requirements for the Future.Resolve method: it should be as fast as possible, almost non-blocking and in no case panic.  This is due to the fact that it is called synchronously in the reader cycle, and any “brakes” will inevitably lead to degradation.  The implementation of Future.Resolve should do the necessary minimum of linear actions: to awaken the waiting;  it is possible to handle the error and send an asynchronous repeat (used in the implementation of cluster support). </p><br><h2 id="effekt">  Effect </h2><br><p>  A good benchmark is half the article! </p><br><p>  A good benchmark is one that is as close as possible to combat use by the observed effects.  And this is not easy to do. </p><br><p>  <a href="https://gist.github.com/funny-falcon-at-joomcode/263b31d1331af6ab9febb5463638374c">Option benchmark</a> , which, I think, looks quite similar to this: </p><br><ul><li>  the main “script” emulates 5 parallel clients, </li><li>  in each “client” for every 300-1000 “desired” rps is launched on a gorutina (3 gorutins are launched for 1000 rps, 124 gorutins for 128,000 rps), </li><li>  Gorutina uses a separate instance of the limiter and sends requests with random batches - from 5 to 15 requests. </li></ul><br><p>  The randomness of a series of queries allows you to achieve a random distribution of series in the time scale, which more correctly reflects the actual load. </p><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><p>  Wrong options were: <br>  a) use one rate-limiter for all “client” gorutinas and contact him for every request - this leads to excessive CPU consumption by the rate-limiter himself, as well as increased time-sequence alternation of gorutin, which degrades the characteristics of RedisPipe at medium rps (but inexplicably improves on high); <br>  b) use one rate-limiter on all the “client” gorutines and send requests in series - the rate-limiter no longer eats the CPU so much, but the alternation of the gorutins in time only increases; <br>  c) use a rate limiter for each gorutina, but send the same series of 10 requests, - in this scenario, the gorutines wake up too simultaneously, which unfairly improves the results of RedisPipe. </p></div></div><br><p>  Testing took place on the quad-core AWS c5-2xlarge instance.  Redis Version 5.0. </p><br><p>  The ratio of the desired intensity of requests, the resulting total intensity and consumed by radish cpu: </p><br><table><thead><tr><th>  intended rps </th><th>  redigo <br>  rps /% cpu </th><th>  redispipe no wait <br>  rps /% cpu </th><th>  redispipe 50µs <br>  rps /% cpu </th><th>  redispipe 150µs <br>  rps /% cpu </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  5015/7% </td><td>  5015/6% </td><td>  5015/6% </td><td>  5015/6% </td></tr><tr><td>  2000 * 5 </td><td>  10022/11% </td><td>  10022/10% </td><td>  10022/10% </td><td>  10022/10% </td></tr><tr><td>  4000 * 5 </td><td>  20036/21% </td><td>  20036/18% </td><td>  20035/17% </td><td>  20035/15% </td></tr><tr><td>  8000 * 5 </td><td>  40020/45% </td><td>  40062/37% </td><td>  40060/26% </td><td>  40056/19% </td></tr><tr><td>  16000 * 5 </td><td>  79994/71% </td><td>  80102/58% </td><td>  80096/33% </td><td>  80090/23% </td></tr><tr><td>  32,000 * 5 </td><td>  159590/96% </td><td>  160180/80% </td><td>  160167/39% </td><td>  160150/29% </td></tr><tr><td>  64000 * 5 </td><td>  187774/99% </td><td>  320313/98% </td><td>  320283/47% </td><td>  320258/37% </td></tr><tr><td>  92000 * 5 </td><td>  183206/99% </td><td>  480443/97% </td><td>  480407/52% </td><td>  480366/42% </td></tr><tr><td>  128,000 * 5 </td><td>  179744/99% </td><td>  640484/97% </td><td>  640488/55% </td><td>  640428/46% </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/g-/d7/7k/g-d77kznsmwmgejiyplpipwv7t4.png" alt="Request Rate"><img src="https://habrastorage.org/webt/cs/xf/lz/csxflztfjbnmzc0kuhpnqddeokq.png" alt="Redis CPU"></p><br><p>  You can see that with the connector working in the classical scheme (request / answer + connection pool), the Redis pretty quickly survives the processor core, after which getting more than 190 krps becomes an impossible task. </p><br><p>  RedisPipe also allows you to squeeze out all the required power from Redis.  And the more we pause to collect parallel requests, the less Redis consumes CPU.  The tangible benefit is obtained already at 4krps from the client (20krps in total), if a pause of 150 microseconds is used. </p><br><p>  Even if the pause is clearly not used when Redis runs into the CPU, the delay appears by itself.  In addition, requests begin to be buffered by the operating system.  This allows RedisPipe to increase the number of successfully executed requests when the classic connector is already lowering the legs. </p><br><p>  This is the main result for which it was required to create a new connector. </p><br><p>  What happens with the consumption of CPU on the client and with the delay of requests? </p><br><table><thead><tr><th>  intended rps </th><th>  redigo <br>  % cpu / ms </th><th>  redispipe nowait <br>  % cpu / ms </th><th>  redispipe 50ms <br>  % cpu / ms </th><th>  redispipe 150ms <br>  % cpu / ms </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  13 / 0.03 </td><td>  20 / 0.04 </td><td>  46 / 0.16 </td><td>  44 / 0.26 </td></tr><tr><td>  2000 * 5 </td><td>  25 / 0.03 </td><td>  33 / 0.04 </td><td>  77 / 0.16 </td><td>  71 / 0.26 </td></tr><tr><td>  4000 * 5 </td><td>  48 / 0.03 </td><td>  60 / 0.04 </td><td>  124 / 0.16 </td><td>  107 / 0.26 </td></tr><tr><td>  8000 * 5 </td><td>  94 / 0.03 </td><td>  119 / 0.04 </td><td>  178 / 0.15 </td><td>  141 / 0.26 </td></tr><tr><td>  16000 * 5 </td><td>  184 / 0.04 </td><td>  206 / 0.04 </td><td>  228 / 0.15 </td><td>  177 / 0.25 </td></tr><tr><td>  32,000 * 5 </td><td>  341 / 0.08 </td><td>  322 / 0.05 </td><td>  280 / 0.15 </td><td>  226 / 0.26 </td></tr><tr><td>  64000 * 5 </td><td>  316 / 1.88 </td><td>  469 / 0.08 </td><td>  345 / 0.16 </td><td>  307 / 0.26 </td></tr><tr><td>  92000 * 5 </td><td>  313 / 2.88 </td><td>  511 / 0.16 </td><td>  398 / 0.17 </td><td>  366 / 0.27 </td></tr><tr><td>  128,000 * 5 </td><td>  312 / 3.54 </td><td>  509 / 0.37 </td><td>  441 / 0.19 </td><td>  418 / 0.29 </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/fp/ws/ra/fpwsra-pgtexl6ua7crpc4vt5fa.png" alt="Client CPU"><img src="https://habrastorage.org/webt/kw/kl/lk/kwkllkjyf-vfpv77miu4jnnlrqm.png" alt="latency"></p><br><p>  You may notice that on small rps RedisPipe itself consumes more CPU than the "competitor", especially if a short pause is used.  This is mainly due to the implementation of timers in Go and the implementation of the system calls used in the operating system (on Linux, this is futexsleep), since the difference is noticeably smaller in the “no pause” mode. </p><br><p>  As rps rises, the overhead of timers is compensated for by less network overhead, and after 16 krps per customer, using RedisPipe with a pause of 150 microseconds starts to bring tangible benefits. </p><br><p>  Of course, after Redis rested in the CPU, the delay in requests using the classic connector begins to grow rampant.  Not sure, though, that in practice you often reach 180 krps from a Redis instance.  But if so, keep in mind that you may have problems. </p><br><p>  While Redis does not run into the CPU, the delay in requests, of course, suffers from the use of a pause.  This compromise is intentionally embedded in the connector.  However, this trade-off is only noticeable if Redis and the client are on the same physical host.  Depending on the network topology, a roundtrip to a neighboring host can be from one hundred microseconds to milliseconds.  Accordingly, the difference in the delay, instead of nine times (0.26 / 0.03), becomes threefold (0.36 / 0.13) or measured only by a couple of tens of percent (1.26 / 1.03). </p><br><p>  In our workload, when Redis is used as a cache, the total waiting for responses from the database with a cache miss is greater than the total waiting for responses from Redis, therefore it is considered that the increase in latency is not significant. </p><br><p>  The main positive result is the tolerance to the growth of load: if suddenly the load on the service increases N times, Redis will not consume the CPU N times the same.  To withstand load quadrupling from 160 krps to 640 krps, Redis spent only 1.6 times more CPU, increasing consumption from 29 to 46%.  This allows us to not be afraid that Redis suddenly bends.  The scalability of the application will also not be due to the work of the connector and the cost of networking (read: the cost of the SYS CPU). </p><br><p>  Remark.  The benchmark code operates on small values.  To clear my conscience, I repeated the test with values ​​of 768 bytes.  The CPU consumption of “radish” has increased significantly (up to 66% at a pause of 150 µs), and the ceiling for the classic connector drops to 170 krps.  But all the proportions seen remained the same, and therefore the conclusions. </p><br><h2 id="klaster">  Cluster </h2><br><p>  For scaling we use <a href="https://redis.io/topics/cluster-tutorial">Redis Cluster</a> .  This allows us to use Redis not only as a cache, but also as a volatile storage and at the same time not losing data when expanding / compressing a cluster. </p><br><p>  Redis Cluster uses the principle of a smart client, i.e.  The client must monitor the status of the cluster on its own, as well as respond to auxiliary errors returned by the “radish” when the “bouquet” moves from the instance to the instance. </p><br><p>  Accordingly, the client must keep connections to all Redis instances in the cluster and issue a connection to the necessary one for each request.  And it is in this place that the client used before (we will not point with a finger) has screwed up badly.  The author, who overestimated Go Marketing (CSP, channels, goroutines), implemented synchronization of work with the state of a cluster through sending callbacks to the central mountain.  This has become a serious bottleneck for us.  As a temporary patch, we had to run four clients to one cluster, each, in turn, lifted up to a hundred connections in the pool to each Redis instance. </p><br><p>  Accordingly, in the new connector there was a task to prevent this error.  All interaction with the state of the cluster in the path of the query is done as much as possible lock-free: </p><br><ul><li>  the state of the cluster is made almost immutable, and not numerous mutations flavored with atoms </li><li>  access to the state occurs using atomic.StorePointer / atomic.LoadPointer, and therefore can be obtained without blocking. </li></ul><br><p>  Thus, even during a cluster state update, queries can use the previous state without fear of waiting for a lock. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// storeConfig atomically stores config func (c *Cluster) storeConfig(cfg *clusterConfig) { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) atomic.StorePointer(p, unsafe.Pointer(cfg)) } // getConfig loads config atomically func (c *Cluster) getConfig() *clusterConfig { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) return (*clusterConfig)(atomic.LoadPointer(p)) } func (cfg *clusterConfig) slot2shardno(slot uint16) uint16 { return uint16(atomic.LoadUint32(&amp;cfg.slots[slot])) } func (cfg *clusterConfig) slotSetShard(slot, shard uint16) { atomic.StoreUint32(&amp;cfg.slots[slot], shard) }</span></span></code> </pre> <br><p>  The cluster status is updated every 5 seconds.  But if there is a suspicion of cluster instability, the update is forced: </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">control</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { t := time.NewTicker(c.opts.CheckInterval) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> t.Stop() <span class="hljs-comment"><span class="hljs-comment">// main control loop for { select { case &lt;-c.ctx.Done(): // cluster closed, exit control loop c.report(LogContextClosed{Error: c.ctx.Err()}) return case cmd := &lt;-c.commands: // execute some asynchronous "cluster-wide" actions c.execCommand(cmd) continue case &lt;-forceReload: // forced mapping reload c.reloadMapping() case &lt;-tC: // regular mapping reload c.reloadMapping() } } } func (c *Cluster) ForceReloading() { select { case c.forceReload &lt;- struct{}{}: default: } }</span></span></code> </pre> <br><p>  If the answer MOVED or ASK, obtained from radish, contains an unknown address, it is initiated asynchronously added to the configuration.  (I apologize, I didn’t think how to simplify the code, because <a href="">here’s the link</a> .) It’s not without the use of locks, but they are taken for a short period of time.  The main expectation is realized through saving the callback in the array - the same future, side view. </p><br><p>  Connections are established to all Redis instances, and to the masters, and to the slaves.  Depending on the preferred policy and the type of request (read or write), the request can be sent both to the master and to the slave.  This takes into account the "liveliness" of the instance, which consists of both the information obtained during the update of the cluster status and the current connection status. </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">connForSlot</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(slot </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint16</span></span></span></span><span class="hljs-function"><span class="hljs-params">, policy ReplicaPolicyEnum)</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*redisconn.Connection, *errorx.Error)</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conn *redisconn.Connection cfg := c.getConfig() shard := cfg.slot2shard(slot) nodes := cfg.nodes <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> addr <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> policy { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> MasterOnly: addr = shard.addr[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-comment"><span class="hljs-comment">// master is always first node := nodes[addr] if conn = node.getConn(c.opts.ConnHostPolicy, needConnected); conn == nil { conn = node.getConn(c.opts.ConnHostPolicy, mayBeConnected) } case MasterAndSlaves: n := uint32(len(shard.addr)) off := c.opts.RoundRobinSeed.Current() for _, needState := range []int{needConnected, mayBeConnected} { mask := atomic.LoadUint32(&amp;shard.good) // load health information for ; mask != 0; off++ { bit := 1 &lt;&lt; (off % n) if mask&amp;bit == 0 { // replica isn't healthy, or already viewed continue } mask &amp;^= bit addr = shard.addr[k] if conn = nodes[addr].getConn(c.opts.ConnHostPolicy, needState); conn != nil { return conn, nil } } } } if conn == nil { c.ForceReloading() return nil, c.err(ErrNoAliveConnection) } return conn, nil } func (n *node) getConn(policy ConnHostPolicyEnum, liveness int) *redisconn.Connection { for _, conn := range n.conns { switch liveness { case needConnected: if c.ConnectedNow() { return conn } case mayBeConnected: if c.MayBeConnected() { return conn } } } return nil }</span></span></code> </pre> <br><p>  There is a mysterious <code>RoundRobinSeed.Current()</code> .  This, on the one hand, is the source of chance, on the other - randomness that does not change often.  If you choose a new connection for each request, it degrades the effectiveness of pipe-planning.  That is why the default implementation changes the value of Current every several tens of milliseconds.  In order for the time lags to be smaller, each host selects its own interval. </p><br><p>  As you remember, the connection uses the concept of Future for asynchronous requests.  The cluster uses the same concept: the user Future turns into a cluster one, and that one is fed to the connection. </p><br><p>  Why wrap a custom future?  Firstly, in Cluster mode, “radish” returns wonderful “errors” of MOVED and ASK with information where you need to go for the key you need, and, having received such an error, you need to repeat the request to another host.  Secondly, since we still need to implement the redirection logic, then why not embed and re-repeat the request in case of an I / O error (of course, only if the read request): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> request <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c *Cluster req Request cb Future slot <span class="hljs-keyword"><span class="hljs-keyword">uint16</span></span> policy ReplicaPolicyEnum mayRetry <span class="hljs-keyword"><span class="hljs-keyword">bool</span></span> } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SendWithPolicy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy ReplicaPolicyEnum, req Request, cb Future)</span></span></span></span> { slot := redisclusterutil.ReqSlot(req) policy = c.fixPolicy(slot, req, policy) conn, err := c.connForSlot(slot, policy, <span class="hljs-literal"><span class="hljs-literal">nil</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { cb.Resolve(err) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } r := &amp;request{ c: c, req: req, cb: cb, slot: slot, policy: policy, mayRetry: policy != MasterOnly || redis.ReplicaSafe(req.Cmd), } conn.Send(req, r, <span class="hljs-number"><span class="hljs-number">0</span></span>) } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r *request)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Resolve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(res </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">interface</span></span></span></span><span class="hljs-function"><span class="hljs-params">{}, _ </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint64</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span> { err := redis.AsErrorx(res) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err == <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { r.resolve(res) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> err.IsOfType(redis.ErrIO): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> !r.mayRetry { <span class="hljs-comment"><span class="hljs-comment">// It is not safe to retry read-write operation r.resolve(err) return } fallthrough case err.HasTrait(redis.ErrTraitNotSent): // It is request were not sent at all, it is safe to retry both readonly and write requests. conn, err := rcconnForSlot(r.slot, r.policy, r.seen) if err != nil { r.resolve(err) return } conn.Send(r.req, r) return case err.HasTrait(redis.ErrTraitClusterMove): addr := movedTo(err) ask := err.IsOfType(redis.ErrAsk) rcensureConnForAddress(addr, func(conn *redisconn.Connection, cerr error) { if cerr != nil { r.resolve(cerr) } else { r.lastconn = conn conn.SendAsk(r.req, r, ask) } }) return default: // All other errors: just resolve. r.resolve(err) } }</span></span></code> </pre> <br><p>  This is also a simplified code.  Omitted a restriction in the number of replays, memorization of problem connections, etc. </p><br><h2 id="komfort">  Comfort </h2><br><p>  Asynchronous requests, Future is a superkul!  But terribly uncomfortable. </p><br><p>  The interface is the most important thing.  You can sell anything if he has a nyashny interface.  That is why Redis and MongoDB gained their popularity. </p><br><p>  So, it is required to turn our asynchronous requests into synchronous ones. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Sync provides convenient synchronous interface over asynchronous Sender. type Sync struct { S Sender } // Do is convenient method to construct and send request. // Returns value that could be either result or error. func (s Sync) Do(cmd string, args ...interface{}) interface{} { return s.Send(Request{cmd, args}) } // Send sends request to redis. // Returns value that could be either result or error. func (s Sync) Send(r Request) interface{} { var res syncRes res.Add(1) sSSend(r, &amp;res) res.Wait() return res.r } type syncRes struct { r interface{} sync.WaitGroup } // Resolve implements Future.Resolve func (s *syncRes) Resolve(res interface{}) { sr = res s.Done() } // Usage func get(s redis.Sender, key string) (interface{}, error) { res := redis.Sync{s}.Do("GET", key) if err := redis.AsError(res); err != nil { return nil, err } return res, nil }</span></span></code> </pre> <br><p>  <code>AsError</code> does not look like a native go-way to get an error.  But I like, because  in my view, the result is <code>Result&lt;T,Error&gt;</code> , and <code>AsError</code> is an ersatz match pattern. </p><br><h2 id="nedostatki">  disadvantages </h2><br><p>  But, unfortunately, there is a spoon of tar in this well-being. </p><br><p>  Redis protocol does not involve reordering requests.  And at the same time, it has blocking queries of the type BLPOP, BRPOP. </p><br><p>  This is a failure. </p><br><p>  As you understand, if such a request is blocked, it will block all requests following it.  And nothing can be done about it. </p><br><p>  After a long discussion, it was decided to prohibit the use of these requests in RedisPipe. </p><br><p>  Of course, if you really need to, you can: set the <code>ScriptMode: true</code> parameter <code>ScriptMode: true</code> , and everything is on your conscience. </p><br><h2 id="alternativy">  Alternatives </h2><br><p>  In fact, there is another alternative, which I did not mention, but which the well-informed readers thought about - the king of cluster twemproxy caches. </p><br><p>  It does what Reds does for Redis: it transforms a rude and soulless “request / response” into a gentle “pipeline”. </p><br><p>  But twemproxy itself will suffer from the fact that he will have to work on the system "request / response."  This time.  And secondly, we use “radishes” including as “unreliable storage” and sometimes change the size of the cluster.  And twemproxy does not facilitate the task of rebalancing in any way and, moreover, requires a reboot when changing the cluster configuration. </p><br><h2 id="vliyanie">  Influence </h2><br><p>  I did not have time to write an article, and the waves from RedisPipe have already gone.  In Radix.v3, a patch was adopted, adding pipelaying to their Pool: </p><br><p>  <a href="https://github.com/mediocregopher/radix/issues/58">Impact redisPipe and figure out</a> <br>  <a href="https://github.com/mediocregopher/radix/pull/61">Automatic pipelining for commands in Pool</a> </p><br><p>  In speed, they are slightly inferior (judging by their benchmarks; but I will not say for sure).  But their advantage is that they can send blocking commands to other connections from the pool. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  Soon a year, as RedisPipe contributes to the effectiveness of our service. <br>  And on the threshold of any “hot days”, one of the resources whose capacity does not cause concern is the CPU on the Redis servers. </p><br><p>  <a href="https://github.com/joomcode/redispipe">Repository</a> <br>  <a href="https://gist.github.com/funny-falcon-at-joomcode/263b31d1331af6ab9febb5463638374c">Benchmark</a> </p></div>