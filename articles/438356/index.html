<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>SQL Server Data Warehouse Fast Track (DWFT) certified architecture: what it means and how it works</title>
  <meta name="description" content="Large manufacturers of popular software care about their customers in different ways. One way is to create a certification program. So that when custo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>SQL Server Data Warehouse Fast Track (DWFT) certified architecture: what it means and how it works</h1><div class="post__text post__text-html js-mediator-article">  Large manufacturers of popular software care about their customers in different ways.  One way is to create a certification program.  So that when customers wander in meditations between hardware configs for a specific software, the manufacturer of this software could come up and point out with confidence: ‚ÄúTake this and everything will be fine.‚Äù <br><br>  Such a program for its SQL Server was developed by Microsoft - SQL Server Fast Track (DWFT).  It is used to certify storage configurations ‚Äî those that meet the requirements of the workload and can be implemented with less risk, cost, and complexity.  It sounds great, but it's still interesting to evaluate these criteria in practice.  To do this, we will analyze in detail one of the configurations certified by SQL Server Data Warehouse Fast Track. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12e/15b/2a9/12e15b2a946ab669b59875e70afb9210.png"><br><a name="habracut"></a><br>  For a start - a little more about DWFT.  This program includes not only the certification of reference architectures from system suppliers, but also a number of best practice guides ‚Äî if you want to put together something of your own.  Microsoft updates the program for the development of new features, and on the basis of customer feedback. <br><br>  With the DWFT storage, you can know for sure that all SQL Server features will be available.  Column storage technologies increase productivity in transactional and analytical workloads.  Both traditional structured relational and unstructured big data will be supported ‚Äî in Hadoop, Spark, or Azure Data Lake.  With the SQL Server ‚ÄúPolyBase‚Äù feature, you can merge big data into SQL Server Universe, query both relational and unstructured data, merging it together without having to move it. <br><br>  A general list of certified architectures is at the bottom of <a href="https://www.microsoft.com/en-us/sql-server/data-warehousing">this page</a> .  Next, we present one of them. <br><br><h2>  "Reference" architecture </h2><br>  Now - to the analysis of one of the architectures certified by DWFT.  It is based on our BullSequana S400 and S800 servers and the Dell EMC VMAX storage.  Storage capacity can be increased to 4 PB, if you scale the BullSequana S server to 32 processors. <br><br>  In order to ensure high availability of the database, failover clustering of Windows is additionally recommended.  That is, the use of at least two servers.  Through the Microsoft clustering services, one database server is configured as the primary (active) server, and the second as the secondary (passive) server.  The passive server must have the same configuration as the main server. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f35/1eb/de5/f351ebde5469e6765a5a12651a1c4b6d.png"><br>  <i>List and characteristics of infrastructure components</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/088/6ed/f56/0886edf56802b4666692121973837d1b.png"><br>  <i>General scheme of two configurations</i> <br><br><h2>  Hardware components </h2><br><h3>  BullSequana S800 server </h3><br>  The BullSequana S800 server is an eight-processor server with up to 12 TB of memory, advanced I / O capabilities, and the addition of a module with SSD / HDD / NVMe storage or with NVidia Tesla GPUs. <br><br>  The BullSequana S line is available in versions from S200 with two processors to S3200 with 32 processors.  Line scale step - two processors.  For the SQL Server 2017 DWFT architecture, the S200, S400 and S800 are equally certified. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/446/511/28e/44651128e0526bf86c0665139ced4b2b.png"><br><br><h3>  Host Bus Adapter Emulex LPe31002-M6 16 Gbps </h3><br>  Emulex Gen 6 FC adapters with a dynamic multicore architecture, compared with previous generations of devices, offer higher performance, lower latency, improved diagnostics and manageability.  Provides speeds of up to 12,800 MB / s (two 32GFC ports or four 16GFC ports), a throughput of 1.6 million IOPS per adapter.  The four-port version of the LPe32004 provides up to 3.2 million IOPS per adapter. <br><br><h3>  Brocade 6510 Switch </h3><br>  The Brocade 6510 switch from Broadcom is a 1U, 48-port Fiber Channel switch that can be installed in a server rack. <br><ul><li>  Fiber Channel Performance: 16 Gbps <br></li><li>  Up to 48 ports providing aggregate bandwidth of 768 Gbps <br></li><li>  Frame trunking at speeds up to 128 Gbps <br></li><li>  Power Consumption: 14 W / Gbps <br></li></ul><br><h3>  Dell EMC VMAX 250F </h3><br>  VMAX All Flash is focused on petabyte data volumes and large transaction processing.  VMAX All Flash is scalable and allows you to combine hundreds of multi-core Intel processors for dynamic mixed workloads.  The main element of VMAX All Flash is V-Brick ‚Äî it is possible to combine up to eight V-Brick as a single system with fully separated connectivity, processing and bandwidth capabilities.  Each V-Brick supports up to 72 processor cores for performance scaling ‚Äî up to 576 cores per array.  Flash Capacity Packs modules are used for scaling. <br><br>  The storage system must clearly comply with the requirements for high-precision real-time transaction processing (OLTP), virtualized applications, and Oracle and SQL databases.  Millions of IOPS, petabyte throughput and predicted performance (response time of 350 ¬µs) are maintained. <br><br>  Finally, the architecture should have enhanced error isolation, reliable data integrity checks, and proven, reliable hardware and software updates.  Another requirement: accessibility for 24x7 operations using SRDF software. <br><br><h2>  How is the storage connected </h2><br>  Configuration of VMAX 250F (All Flash): <br><br><ul><li>  2 vBrick with 2 TB of cache memory per block. <br></li><li>  32 * 16 Gbit / s host FC ports <br></li><li>  64 * 3.84 TB flash (Raid5-7 + 1) + reserve <br></li></ul><br>  The appearance of the VMAX vBricks in the rack: <br><br><img src="https://habrastorage.org/webt/yb/aw/tg/ybawtgjefv2ri-qv9yh-ka_0rlo.jpeg"><br><br>  Port locations: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/902/5c3/d8c/9025c3d8cc9f68e072c8a29fe4e400ff.png"><br><br>  The logical array configuration in a certified architecture is aligned with the Dell EMC configuration and best performance practices. <br><br><ul><li>  HyperMaxOS Version: 5977.1131.1131 <br></li><li>  Encryption: enabled <br></li><li>  Compression: disabled <br></li><li>  System Performance Profile: Basic <br></li></ul><br>  Main distribution: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9a4/e37/d6f/9a4e37d6fe21659c149817c2039458af.png" width="550" height="284"><br><img src="https://habrastorage.org/getpro/habr/post_images/f03/548/2e3/f035482e3c58abe81d2c92d5cc28e44d.png" width="550" height="353"><br><br><h3>  LUN configuration </h3><br>  All LUNs are configured as THIN LUNs in a virtual pool created on 64 drives.  Data is distributed across all flash drives on both vBricks.  In total, 35 LUNs are defined on the Dell EMC VMAX: <br><br><ul><li>  16 LUN size 8 TB for user data (128 TB total) <br></li><li>  16 LUN or 2 TB for tempdb (32 TB total) <br></li><li>  2 LUN size 2 TB for the magazine (4 TB in total) <br></li></ul><br>  Front Fiber Channel (FC) ports are configured to use four domains with failures in virtual port mode.  Each port is one to one connected to a port on the VMAX, which serves only one data LUN and one Tempdb LUN.  For each of them, there is a LUN to separate read and write operations on the LUN (when performing GROUP BY or ORDER BY, data is first written to Tempdb before being used in another part of the query or presented to the user).  Each LUN is mapped to four FA ports and is available in four ways. <br><br>  Mapping ports: <br><br><img src="https://habrastorage.org/webt/27/qx/w7/27qxw7rwce63tllr2-sx0xfef10.png"><br><br><h3>  VMAX Storage Groups (SG) </h3><br>  The following storage groups have been created: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/19e/02a/f23/19e02af23e89d960a182d53196d0c2b8.png"><br><br><h3>  Read and write cache </h3><br>  The VMAX cache is global, dynamically used for reading and writing.  The default settings are used to limit system write waiting.  Since the system uses only one application, there is no need to create cache sections. <br><br><h3>  Connect to server and MPIO </h3><br>  Windows Server 2016 used its own MPIO.  Its alternative is PowerPath software for Dell EMC applications. <br><br><h3>  Cable laying </h3><br>  The top ports of the Brocade switches are used to connect the BullSequana S800 server.  Each even-numbered Emulex HBA port is connected to the upper ports to the left of the Brocade switch.  Each of the Outbus Emulex HBA ports is connected to the upper ports to the left of the Brocade_2 switch.  Bottom ports are used only to connect FC Dell EMC VMAX ports. <br><br><h3>  Single Server Configuration </h3><br>  The following diagram shows the cabling between the BullSequana S800 and the Dell EMC VMAX 250F.  The placement of the HBA may differ depending on the placement of the HBA modules. <br><br><img src="https://habrastorage.org/webt/tp/v_/n2/tpv_n21oqa-0yj-yjgjgddzt6sa.png"><br><br><h3>  Connecting cables in high available server configuration </h3><br>  The diagram below shows how to connect two BullSequana S800 servers in a high-availability configuration and the Dell EMC VMAX 250F.  Again, the placement of the HBA may differ depending on the placement of the HBA modules. <br><br><img src="https://habrastorage.org/webt/p0/la/qp/p0laqpph-_vtnh5brejensuzfb8.png"><br><br>  Above, on the left side of the Brocade switch, the left half of the ports for the first S800 server is highlighted, and on the right side of the picture, the right ports are highlighted, and the second S800 server is connected to them. <br><br><h2>  Server Configuration BullSequana S800 </h2><br><h3>  System BIOS </h3><br>  All parameters, except the BMC network configuration, were left at the factory default settings.  The ‚ÄúLogic Processor‚Äù parameter in the ‚ÄúProcessor Parameters‚Äù section remains on by default.  This activates Intel Hyper-Threading Technology, which maximizes the number of logical processors available for SQL Server. <br><br><h3>  Emulex LPe31002-M6 16 Gbps </h3><br>  All Emulex HBAs are available through the Emulex OneCommand Manager.  The firmware and driver version used during Microsoft DWH Fast Track certification is 12.0.193.13. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/447/81a/349/44781a34977a699e1b070560f0feb19f.png"><br><br>  It is important that all HBAs have the same firmware.  The latest firmware can be downloaded from the Broadcom website.  Firmware updates can be performed using the ‚ÄúDownload Firmware‚Äù button in the Emulex OneCommand Manager.  Changing the driver and / or firmware may require rebooting the server. <br><br>  With the exception of the QueryDept parameter, which was changed from 32 to 64, all other host and HBA parameters remained by default. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/576/2b9/26f/5762b926fabf810b5bfbcfc136c2b25a.png"><br><br><h2>  Windows Server 2016 Configuration </h2><br><h3>  Installation </h3><br>  Windows installation was performed with default settings.  After installation, the Windows MPIO feature was activated, as shown in the figure below. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d0f/432/266/d0f432266c604ae29130377f0fb24ca3.png"><br><br>  After installing the MPIO function, Windows must be restarted. <br><br><h3>  Installing drivers and packages </h3><br>  It is important to have drivers and packages on USB storage, since they are not included in the Windows driver catalog included on the installation DVD (network adapters are newer than Windows Server 2016).  After installing Windows and MPIO, the following drivers and packages were installed (in the specified order): <br><br><ol><li>  Intel INF Chipset version 10.1.17711.8088_PV or later (requires reboot) </li><li>  MegaRAID driver for Windows 2016 version 6.14-6.714.05.00-WHQL (reboot required) </li><li>  PROWinx64 for Intel XL7xx Family Version 23_2 or Newer </li><li>  OneInstall-Setep-12.0.193.18.exe or later </li></ol><br><h3>  Power setting </h3><br>  To maximize performance, the server was configured to use a high performance plan. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c3a/ecb/ff8/c3aecbff80d68ec3fc5ae422d9ac7cc9.png"><br><br><h3>  Lock pages in memory </h3><br>  So that the Windows operating system did not send data pages to virtual memory on disk and SQL Server could use the process for storing data in physical (RAM) memory, the Lock pages in memory option for the SQL Server service account was enabled.  You must restart SQL Server for this option to take effect. <br><br><h3>  Windows drives </h3><br>  After zoning is completed, VMAX and LUN switches are displayed in Windows Disk Management. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/46d/83a/b05/46d83ab059f166a677bd464779d7f0f1.png" width="500" height="366"><br><br>  If all disks are formatted correctly, Windows Disk Management displays the following list: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f2f/f25/ccb/f2ff25ccba6e28ddcdb71ada57164c1c.png"><br><br>  There are 34 LUNs in total: <br><br><ul><li>  16 LUN size 8 TB for user data (128 TB total) <br></li><li>  16 LUN or 2 TB for tempdb (32 TB total) <br></li><li>  2 LUN size 2 TB for the magazine (4 TB in total) <br></li></ul><br>  For DWFT reference architectures, we with Dell EMC recommend using mount points for volumes instead of drive letters.  We recommend that you assign appropriate volume and mount point names to simplify troubleshooting, and performance analysis.  Ideally, mount point names should be assigned in such a way as to simplify the identification of the VMAX volume for a given Windows volume. <br><br>  The following table shows the volume labels and access paths used for the reference configuration: <br><br><img src="https://habrastorage.org/webt/9a/g1/sp/9ag1sppoc0gc9op8bzaxc8cjalg.png"><br><br>  After all the LUNs are ready, the C: \ Storage directory looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2fb/109/afd/2fb109afdaf5d746922ae22c1bc042df.png"><br><br><h3>  MPIO </h3><br>  The MPIO policy for all volumes is set in the ‚ÄúLeast Queue Dept‚Äù section. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cdc/503/c25/cdc503c25e2efa569bd7fc1f73792a06.png" width="550" height="655"><br><h3>  Windows Defender Configuration </h3><br>  Windows Defender is a standard antivirus and antispam component.  To prevent SQL Server data and log files from being scanned (for performance), you need to add the following exceptions. <br><br>  Folders in one server: <br><br><ul><li>  C: \ Program Files (x86) \ Microsoft SQL Server <br></li><li>  C: \ Program Files \ Microsoft SQL Server <br></li><li>  C: \ Storage (single server solution) <br></li></ul><br>  Folders in a high availability (HA) solution: <br><ul><li>  C: \ Program Files (x86) \ Microsoft SQL Server <br></li><li>  C: \ Program Files \ Microsoft SQL Server <br></li><li>  C: \ ClusterStorage (high available solution) <br></li><li>  C: \ Windows \ Cluster <br></li><li>  Quorum drive <br></li><li>  MSDTC Drive <br></li></ul><br>  File types: <br><ul><li>  .ldf <br></li><li>  .mdf <br></li><li>  .ndf <br></li></ul><br><h2>  SQL Server 2017 Enterprise Edition Configuration </h2><br>  Installation is performed mainly using default settings.  Some exceptions are listed below.  During the installation of SQL Server 2017, the ‚ÄúPerform Volume Maintenance Task‚Äù option is selected. <br><br><h3>  Tempdb configuration </h3><br>  The Tempdb database is configured to use 16 files of the same size.  Tempdb data files are located on 16 volumes.  The log file of transactions tempdb is placed on a disk where logs are kept.  Auto zoom enabled. <br><br><h3>  Startup options for a SQL Server instance </h3><br>  SQL Server 2017 automatically sets the trace flags -T1117 and -T1118, so you no longer need to add them to the instance startup parameters.  If you do not use the trace flag -T834, the best performance is achieved.  Microsoft recommends that you do not use this flag when using clustered column storage indexes. <br><br>  The only startup parameter is the -E flag: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e4/b12/008/4e4b120089f219131f2105e7bcfd56f4.png"><br><br>  Additional information on the topic can be found in the Microsoft <a href="https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-traceon-trace-flags-transact-sql%3Fview%3Dsql-server-2017">DBCC TRACEON - Trace Flags</a> and <a href="https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/database-engine-service-startup-options%3Fview%3Dsql-server-2017">Database Engine Startup Options</a> articles. <br><br><h3>  Maximum SQL Server Memory </h3><br>  The maximum server memory for this reference architecture should be set to 11.534.336 MB or 11.264 GB, where 1.024 GB is allocated to the operating system.  If additional applications share a server, you need to adjust the amount of memory available for the operating system. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/11f/8bd/a17/11f8bda17baade55cca173555e3ee99c.png"><br><br><h3>  Maximum degree of parallelism (MAXDOP) </h3><br>  The maximum degree of parallelism is set at 448, which corresponds to the number of logical cores available on the server. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2f9/648/f9e/2f9648f9e3adfac0b1aa229c29c6c78e.png"><br><br><h3>  Resource management </h3><br>  Depending on the type of workload used ‚Äî the storage with the largest number of rows (RS) or cluster column storage (CS) ‚ÄîYou need to properly configure the Resource Governor settings. <br><br>  For row storage, the resource control parameter used to limit the maximum amount of memory is set to 12 percent. <br><br><pre><code class="plaintext hljs">ALTER WORKLOAD GROUP [default] WITH (request_max_memory_grant_percent=12); ALTER RESOURCE GOVERNOR RECONFIGURE;</code> </pre> <br>  For column storage, the resource governor parameter used to limit the maximum amount of memory is set to 25 percent. <br><br><pre> <code class="plaintext hljs">ALTER WORKLOAD GROUP [default] WITH (request_max_memory_grant_percent=25); ALTER RESOURCE GOVERNOR RECONFIGURE;</code> </pre> <br><h3>  Database Setup </h3><br>  The data warehouse database was configured to use several groups of files, each of which contains 16 files distributed evenly across sixteen data volumes.  All files allowed to automatically grow.  The file groups have been configured with the AUTOGROW_ALL_FILES parameter to ensure that all files in this file group remain the same. <br><br><pre> <code class="plaintext hljs">ALTER DATABASE &lt;database name&gt; MODIFY FILEGROUP &lt;file group name&gt; AUTOGROW_ALL_FILES;</code> </pre> <br><h3>  Add-ons for high availability reference architecture (HA) </h3><br>  HA Reference Architecture uses Windows Failover Clustering to provide high availability.  When configuring a Windows failover cluster, there are additional storage issues.  The recommended configuration allows all cluster votes to be allowed to vote for quorum and use the disk. <br><br>  An additional volume must be created and configured as a witness disk.  Dell EMC recommends using 2GB of disk space.  All volumes must be mapped to each cluster node, configured as a cluster resource, and added to the SQL Server cluster resource group. <br><br><h2>  Conclusion </h2><br>  In conclusion, we highlight probably the most important advantage of the configuration - the ability to connect a large number of HBA adapters.  This allows for a balanced increase in storage modules with them. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86e/f69/784/86ef697842f322cc25e54c25a3ebe296.png"><br>  <i>And this is the certificate for the ATOS BullSequana S800 and Dell EMC VMAX 250F configuration.</i>  <i>You can see test metrics in it.</i> <br><br>  In the near future, we plan to transfer solutions to the architecture of Cascade Lake.  This will lead to the beginning of a new certification, which will complement the current one.  We are pleased to answer your questions about certification and our architecture in the comments. </div><p>Source: <a href="https://habr.com/ru/post/438356/">https://habr.com/ru/post/438356/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>