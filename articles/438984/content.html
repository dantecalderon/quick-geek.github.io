<div class="post__text post__text-html js-mediator-article">  <a href="https://habr.com/ru/company/ruvds/blog/438982/">Last</a> time we considered two approaches to working with microservices.  In particular, one of them involves the use of Docker containers, in which you can run microservice code and auxiliary programs.  Today, using images of containers that we already have, we will work with Kubernetes. <br><br> <a href="https://habr.com/ru/company/ruvds/blog/438984/"><img src="https://habrastorage.org/webt/13/lv/dr/13lvdrwhhap-ouchegvweul0fg0.jpeg"></a> <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Meet Kubernetes</font> </h2><br>  I promise, and at the same time I do not exaggerate at all that when you finish reading this article, ask yourself: “Why don't you call Kubernetes Supernetes?”. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d76/9f9/e76/d769f9e7670a725759dd7415949177a0.png"></div><br>  <i><font color="#999999">Supernetes</font></i> <br><br>  If you read the previous part of this material, then you know that there we have analyzed a lot of things related to preparing applications for containerization and working with Docker containers.  It may seem to you that the most difficult thing is waiting for you now, but, in fact, what we are going to talk about here is much simpler than what we have already figured out.  The only reason for which the study of Kubernetes may seem to someone a very difficult task is the amount of additional information that you need to have in order to understand Kubernetes and to use this system effectively.  We have already discussed all the “additional information” necessary for the successful development of Kubernetes. <br><br><h3>  <font color="#3AC1EF">▍What is Kubernetes?</font> </h3><br>  In the first part of this material, after running microservices in containers, you were asked to think about the issue of scaling containerized applications. <br>  I propose to reflect on it together, in the format of questions and answers: <br><br>  <b>Q:</b> How are containerized applications scaled? <br>  <b>Answer:</b> Launch additional containers. <br><br>  <b>Question:</b> How do they distribute the load between them?  What if some server is already used to the maximum, and the container needs to be deployed on another server?  How to find the most efficient way to use hardware? <br>  <b>Answer:</b> So ... I will look on the Internet ... <br><br>  <b>Question:</b> How to update the program without disturbing the system?  And, if the update contains an error, how to return to the working version of the application? <br><br>  In fact, it is Kubernetes technology that provides decent answers to these and many other questions.  I will try to shrink the definition of Kubernetes to one sentence: “Kubernetes is a container management system that abstracts the basic infrastructure (the environment in which containers run).” <br><br>  I believe that now you are not particularly clear about the concept of "container management", although we have already mentioned this.  Below we consider this technology in practice.  However, the concept of “abstraction of basic infrastructure” is encountered for the first time.  Therefore, now we consider it. <br><br><h3>  <font color="#3AC1EF">Б Abstraction of basic infrastructure</font> </h3><br>  Kubernetes allows applications to abstract from the infrastructure, giving us a simple API to which you can send requests.  Kubernetes tries to fulfill these requests using all its capabilities.  For example, in ordinary language, a similar query can be described as: “Kubernetes, expand 4 containers of image X”.  After receiving the command, Kubernetes will find not too loaded nodes (they are also called “nodes” - from the English “node”), on which you can deploy new containers. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/735/b88/a2a/735b88a2a717e9c01bfc197f3c1b20fd.png"></div><br>  <i><font color="#999999">API Server Request</font></i> <br><br>  What does this mean for the developer?  This means that it does not need to worry about the number of nodes, about exactly where the containers are started, how they interact.  It does not have to deal with hardware optimization or worry about nodes whose operation can be broken (and something like that, according to Murphy's law, is bound to happen), since, if necessary, new nodes can be added to the Kubernetes cluster.  If there is something wrong with some existing nodes, Kubernetes will deploy containers on those nodes that are still in a healthy state. <br><br>  Much of what is shown in the previous figure, you are already familiar.  But there is something new there: <br><br><ul><li>  API Server.  Making calls to this server is the only way to interact with the cluster, which we have, whether we are talking about starting or stopping containers, checking the state of the system, working with logs, or performing other actions. </li><li>  Kubelet.  This is the agent that monitors the containers inside the node and interacts with the main node. </li></ul><br>  Please note that in a couple of previous sentences we use the term "container", but here it would be better to use the term "pod".  These entities in Russian-language publications are often called "podami", and sometimes - "pods", in the <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">documentation</a> , clarifying the concept of "pod", refer to "a flock of whales" (pod of whales) or "pea pod" , but no one calls them "flocks" or "pods".  We, speaking of them, will use the word "under."  Now you can quite consider them as containers, we will talk about feeds in more detail below. <br><br>  We will dwell on this, as we can talk about all this further, and, moreover, there are a lot of good materials concerning the theory of Kubernetes.  For example, this is official documentation, although reading it is not easy, or books like <a href="https://www.amazon.com/Kubernetes-Action-Marko-Luksa/dp/1617293725">this</a> . <br><br><h3>  <font color="#3AC1EF">▍Standardization of work with cloud service providers</font> </h3><br>  Another strength of Kubernetes is that this technology contributes to the standardization of work with cloud service providers (Cloud Service Provider, CSP).  This is a bold statement.  Consider the following example.  A specialist who knows Azure well or the Google Cloud Platform has to work on a project designed for a completely new cloud environment with which he is unfamiliar.  In such a situation, much can go wrong.  For example, the deadlines for project delivery may be disrupted, the project company may need to rent more cloud resources than planned, and so on. <br><br>  When using Kubernetes, this problem simply cannot arise, since, regardless of what kind of cloud service provider we are talking about, working with Kubernetes always looks the same.  The developer, in a declarative style, tells the API server what he needs, and Kubernetes works with the system resources, allowing the developer to abstract from the details of the implementation of this system. <br><br>  Stay a little bit on this idea, as this is a very powerful Kubernetes opportunity.  For companies, this means that their solutions are not tied to a particular CSP.  If a company finds a more profitable offer on the cloud service market, it can freely use this offer by moving to a new provider.  At the same time, the experience gained by the specialists of the company is not lost anywhere. <br><br>  Now let's talk about the practical use of Kubernetes <br><br><h2>  <font color="#3AC1EF">Practice working with Kubernetes: pods</font> </h2><br>  We set up the launch of microservices in containers, the setup process was rather tedious, but we managed to get to a working system.  In addition, as already mentioned, our solution does not scale well and is not resistant to failures.  We will solve these problems with the help of Kubernetes.  Next we bring our system to the form corresponding to the following scheme.  Namely, the containers will be managed by Kubernetes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/53d/19c/3ba/53d19c3bac2f8cdd66213c9b34e7b05b.png"></div><br>  <i><font color="#999999">Microservices work in a cluster managed by Kubernetes</font></i> <br><br>  Here, we will use Minikube to locally deploy the cluster and test the capabilities of Kubernetes, although everything that we are going to do here can also be done using cloud platform tools such as Azure or the Google Cloud Platform. <br><br><h3>  <font color="#3AC1EF">▍Minikube installation and launch</font> </h3><br>  Follow the directions in the <a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">documentation</a> to install Minikube.  During the installation of Minikube, you also install Kubectl.  This is a client that allows you to make requests to the Kubernetes API server. <br><br>  To start Minikube, execute the <code>minikube start</code> command, and after it runs, execute the <code>kubectl get nodes</code> command.  As a result, you should see something like the following: <br><br><pre> <code class="plaintext hljs">kubectl get nodes NAME       STATUS ROLES     AGE VERSION minikube   Ready &lt;none&gt;    11m v1.9.0</code> </pre> <br>  Minikube puts at our disposal a cluster that consists of only one node.  True, it suits us perfectly.  Those who work with Kubernetes do not need to worry about how many nodes are present in the cluster, since Kubernetes allows you to abstract from such details. <br><br>  Now let's talk about the sub. <br><br><h3>  <font color="#3AC1EF">▍Fets</font> </h3><br>  I really like containers, and you probably also like them now.  Why, then, Kubernetes suggests that we use sweats, entities that are minimally deployable computing units in this system?  What functions does it perform under?  The point is that the composition of the hearth can include one or more containers that share the same execution environment. <br><br>  But is it necessary to carry out, for example, two containers in one bag?  How to say ... Usually, there is only one container per one, and that’s what we are going to do.  But for those cases when, for example, two containers need common access to the same data storage, or if they are connected with the use of interprocess communication techniques, or if they are closely related for some other reason, all this can be done by running them in one hearth.  Another possibility that pitches differ in is that they do not have to use Docker containers.  If necessary, other application containerization technologies can be applied here, for example, <a href="https://coreos.com/rkt/">Rkt</a> . <br><br>  The following diagram shows the numbered properties of the hearth. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f6/2a4/bb1/4f62a4bb18bddccc49a9d224a4aa919d.png"></div><br>  <i><font color="#999999">Properties podov</font></i> <br><br>  Consider these properties. <br><br><ol><li>  Each hearth in the Kubernetes cluster has a unique IP address. </li><li>  In the hearth can contain many containers.  They share the available port numbers, that is, for example, they can exchange information with each other through <code>localhost</code> (of course, they cannot use the same ports).  Interaction with containers located in other sub-fields is organized using the IP addresses of these subfields. </li><li>  The containers in the sub-sites share the data storage volumes, the IP address, the port numbers, the IPC namespace. </li></ol><br>  It should be noted that containers have their own isolated file systems, but they can share data using the Kubernetes resource called <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volume</a> . <br><br>  We have what is already said about the subframes, enough to continue to master the Kubernetes.  You can read more about them <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">here</a> . <br><br><h3>  <font color="#3AC1EF">▍Description</font> </h3><br>  Below is a manifest file for the <code>sa-frontend</code> application. <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Pod                                            # 1 metadata: name: sa-frontend                                  # 2 spec:                                                # 3 containers:   - image: rinormaloku/sentiment-analysis-frontend # 4     name: sa-frontend                              # 5     ports:       - containerPort: 80</code> </pre> <br>  Let us explain some of the parameters specified in it. <br><br><ol><li>  <code>Kind</code> : sets the type of Kubernetes resource we want to create.  In our case, this is a <code>Pod</code> . </li><li>  <code>Name</code> : the name of the resource.  We called it <code>sa-frontend</code> . </li><li>  <code>Spec</code> : An object that describes the desired state of the resource.  The most important property here is an array of containers. </li><li>  <code>Image</code> : image of the container that we want to run in this pane. </li><li>  <code>Name</code> : a unique name for the container located in the pod. </li><li>  <code>ContainerPort</code> : the port that listens on the container.  This parameter can be considered an indication for who reads this file (if you omit this parameter, it will not restrict access to the port). </li></ol><br><h3>  <font color="#3AC1EF">▍Creating a SA-Frontend Bottom</font> </h3><br>  File description of the pod, which we talked about, can be found at <code>resource-manifests/sa-frontend-pod.yaml</code> .  In this folder, you must either go through the terminal, or, when calling the appropriate command, specify the full path to the file.  Here is this command and an example of the system's reaction to it: <br><br><pre> <code class="plaintext hljs">kubectl create -f sa-frontend-pod.yaml pod "sa-frontend" created</code> </pre> <br>  To find out if the sub works, run the following command: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                          READY STATUS RESTARTS AGE sa-frontend                   1/1 Running 0 7s</code> </pre> <br>  If the status of the poda when executing this command is <code>ContainerCreating</code> , then you can run the same command with the <code>--watch</code> key.  Because of this, when you go to the <code>Running</code> state, this information will be displayed automatically. <br><br><h3>  <font color="#3AC1EF">▍Access to the application from outside</font> </h3><br>  In order to organize access to the application from the outside, it will be correct to create a Kubernetes resource of the Service type, which we will discuss below, but here, for the sake of brevity, we will use a simple port forwarding: <br><br><pre> <code class="plaintext hljs">kubectl port-forward sa-frontend 88:80 Forwarding from 127.0.0.1:88 -&gt; 80</code> </pre> <br>  If you now go through the browser at <code>127.0.0.1:88</code> , you will see the React-application page. <br><br><h3>  <font color="#3AC1EF">▍ Wrong approach to scaling</font> </h3><br>  We have already said that one of the opportunities Kubernetes is scaling applications.  In order to experience this opportunity, run another one under.  Create another resource <code>Pod</code> description by placing the following code in the <code>sa-frontend-pod2.yaml</code> file: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Pod                                           metadata: name: sa-frontend2      # Единственное изменение spec:                                                containers:   - image: rinormaloku/sentiment-analysis-frontend     name: sa-frontend                                  ports:       - containerPort: 80</code> </pre> <br>  As you can see, if we compare this description with what we considered above, the only change in it is the value of the <code>Name</code> property. <br><br>  Create a new under: <br><br><pre> <code class="plaintext hljs">kubectl create -f sa-frontend-pod2.yaml pod "sa-frontend2" created</code> </pre> <br>  Make sure that it is running: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                          READY STATUS RESTARTS AGE sa-frontend                   1/1 Running 0 7s sa-frontend2                  1/1 Running 0 7s</code> </pre> <br>  Now we have two pod!  True, there is nothing special to be happy about.  Please note that the solution to the problem of scaling an application shown here has many drawbacks.  How to do it right, we will talk in the section dedicated to another resource Kubernetes, which is called Deployment (deployment). <br><br>  Now let's consider what we did after launching two identical pods.  Namely, the Nginx web server is now running in two different hearths.  In this regard, we can ask two questions: <br><br><ol><li>  How to give access to these servers from the outside, by URL? </li><li>  How to organize load balancing between them? </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ff/3a9/6f4/1ff3a96f4b930fe55727d1063b3c117b.png"></div><br>  <i><font color="#999999">Wrong approach to scaling</font></i> <br><br>  Among the means of Kubernetes are resources of the form Service.  Let's talk about them. <br><br><h2>  <font color="#3AC1EF">Practice working with Kubernetes: services</font> </h2><br>  Kubernetes services play the role of access points to sets of pods, which provide the same functionality as these pods.  Services perform the solution of difficult tasks to work with the hearths and load balancing between them. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bbd/95f/bd8/bbd95fbd8562bed4a09ab4930a20f98d.png"></div><br>  <i><font color="#999999">Kubernetes service serves IP addresses</font></i> <br><br>  In our cluster, Kubernetes will be set up, implementing various functions.  This is a front-end application, a Spring web application and a Flask application written in Python.  This raises the question of how the service should understand which subframes it needs to work with, that is, how to find out on the basis of what information the system should generate a list of endpoints for the pods. <br><br>  This is done using another Kubernetes abstraction called Label.  Work with tags consists of two stages: <br><br><ol><li>  The purpose of the label will be the one with which the service should work. </li><li>  Application to the service of "selector", which determines the fact with which particular trays, which are assigned tags, the service will work. </li></ol><br>  Perhaps it is easier to present in the form of an illustration than to describe. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb9/fcf/f0c/bb9fcff0cded591f1a5ab8a0b825245a.png"></div><br>  <i><font color="#999999">Pods with labels and their manifest files</font></i> <br><br>  We see here two hearths, which, using the <code>app: sa-frontend</code> construction, are assigned the same labels.  Service interested pods with such tags. <br><br><h3>  <font color="#3AC1EF">▍Tags</font> </h3><br>  Labels give the developer a simple way to organize Kubernetes resources.  They are key-value pairs, you can assign them to any resources.  Modify the description files of the frontend applets and bring them to the form shown in the previous figure.  After that, save these files and run the following commands: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-pod.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply pod "sa-frontend" configured kubectl apply -f sa-frontend-pod2.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply pod "sa-frontend2" configured</code> </pre> <br>  When executing these commands, the system will issue warnings (it does not suit us that we use <code>apply</code> instead of <code>create</code> , we understand this), but, after the warning, it reports that the corresponding heartbeats are configured.  To check whether tags have been assigned, we can, by filtering the tags, the information about which we want to display: <br><br><pre> <code class="plaintext hljs">kubectl get pod -l app=sa-frontend NAME           READY STATUS    RESTARTS AGE sa-frontend    1/1 Running   0 2h sa-frontend2   1/1 Running   0 2h</code> </pre> <br>  Another way to check that the tags were actually assigned is to attach the <code>--show-labels</code> key to the previous command.  Thanks to this information about their tags will be included in the list of information about the subfields. <br><br>  Now I’ll assign the tags assigned and we are ready to set up the service to work with them  Therefore, let us take a description of the service type <code>LoadBalancer</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e0f/081/7e9/e0f0817e9fc090628aa1d0d89577ce80.gif"></div><br>  <i><font color="#999999">Load balancing using a LoadBalancer type service</font></i> <br><br><h3>  <font color="#3AC1EF">▍Description of service</font> </h3><br>  Here is the YAML description of the <code>LoadBalancer</code> type <code>LoadBalancer</code> : <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service              # 1 metadata: name: sa-frontend-lb spec: type: LoadBalancer       # 2 ports: - port: 80               # 3   protocol: TCP          # 4   targetPort: 80         # 5 selector:                # 6   app: sa-frontend       # 7</code> </pre> <br>  We explain this text: <br><br><ol><li>  <code>Kind</code> : we create a service, resource <code>Service</code> . </li><li>  <code>Type</code> : the type of resource indicated in its specification.  We chose the type <code>LoadBalancer</code> , because with the help of this service we want to solve the problem of load balancing between the sweeps. </li><li>  <code>Port</code> : the port on which the service receives requests. </li><li>  <code>Protocol</code> : the protocol used by the service. </li><li>  <code>TargetPort</code> : the port to which incoming requests are redirected. </li><li>  <code>Selector</code> : an object containing information about which subsection the service should work with. </li><li>  <code>app: sa-frontend</code> : this property indicates which subscription the service will work with.  Namely, these are the pods that have the <code>app: sa-frontend</code> label assigned <code>app: sa-frontend</code> . </li></ol><br>  In order to create a service, run the following command: <br><br><pre> <code class="plaintext hljs">kubectl create -f service-sa-frontend-lb.yaml service "sa-frontend-lb" created</code> </pre> <br>  You can check the status of the service as follows: <br><br><pre> <code class="plaintext hljs">kubectl get svc NAME             TYPE CLUSTER-IP      EXTERNAL-IP PORT(S) AGE sa-frontend-lb   LoadBalancer 10.101.244.40   &lt;pending&gt; 80:30708/TCP 7m</code> </pre> <br>  Here you can see that the <code>EXTERNAL-IP</code> property is in the <code>&lt;pending&gt;</code> state, and you can not wait for its change.  This happens due to the fact that we use Minikube.  If we created such a service, working with a certain cloud service provider, like Azure or the Google Cloud Platform, then the service would have a public IP address that would enable it to access it from the Internet. <br><br>  Despite this, Minikube will not allow us to idle, giving us a useful command to debug the system locally: <br><br><pre> <code class="plaintext hljs">minikube service sa-frontend-lb Opening kubernetes service default/sa-frontend-lb in default browser...</code> </pre> <br>  Thanks to this command, a browser will be launched that will access the service.  After the service receives the request, it will redirect it to one of the pods (it doesn't matter which one it will be under).  This abstraction allows us to perceive the group of pods as a single entity and work with them, using the service as a single access point to them. <br><br>  In this section, we talked about how to assign tags to resources, how to use them when setting up services as selectors.  Here we also described and created a <code>LoadBalancer</code> type <code>LoadBalancer</code> .  Thanks to this, we solved the task of scaling the application (scaling is to add new pods with corresponding labels to the cluster) and to organize load balancing between the pods using the service as an entry point. <br><br><h2>  <font color="#3AC1EF">Practice working with Kubernetes: deployment</font> </h2><br>  Deployment is an abstraction of Kubernetes that allows us to manage what is always present in the application life cycle.  This is a change management application.  Applications that do not change are, so to speak, "dead" applications.  If the application "lives", then you may be faced with the fact that the requirements for it periodically change, its code expands, this code is packaged and deployed.  In this case, at each step of this process errors may occur. <br><br>  Resource type Deployment allows you to automate the transition from one version of the application to another.  This is done without interrupting the system, and if an error occurs during this process, we will be able to quickly return to the previous, working version of the application. <br><br><h3>  <font color="#3AC1EF">▍Use of deployments</font> </h3><br>  Now the cluster has two hearths and a service that gives access to them from the outside and balances the load on them. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/426/651/0c4/4266510c40a1faa6086178e5db23d20c.png"></div><br>  <i><font color="#999999">Current cluster status</font></i> <br><br>  We said that launching two different pods with the same functionality is not the best idea.  When using such a scheme, we have to work with each hearth on an individual basis, creating, updating, deleting each individual under, observing its condition.  With this approach, one does not have to talk about a quick system update or a quick rollback of an unsuccessful update.  We are not satisfied with this state of affairs, therefore we are going to resort to the possibility of the Deployment resource, which is aimed at solving the above mentioned problems. <br><br>  Before we continue our work, let's formulate its goals, which will give us guidelines that will be useful when parsing the deployment manifest file.  So, this is what we need: <br><br><ol><li>  We want to be able to create two pods based on one <code>rinormaloku/sentiment-analysis-frontend</code> container. </li><li>  We need an application deployment system that allows it, when it is updated, to work without interruption. </li><li>  We want the <code>app: sa-frontend</code> tag to be assigned, which will allow the <code>sa-frontend-lb</code> service to detect these feeds. </li></ol><br>  Now we will express these requirements as a description of the Deployment resource. <br><br><h3>  <font color="#3AC1EF">▍Description of deployment</font> </h3><br>  Here is the YAML description of the Deployment type resource, which was created taking into account the above described system requirements: <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Deployment                                          # 1 metadata: name: sa-frontend spec: replicas: 2                                             # 2 minReadySeconds: 15 strategy:   type: RollingUpdate                                   # 3   rollingUpdate:     maxUnavailable: 1                                   # 4     maxSurge: 1                                         # 5 template:                                               # 6   metadata:     labels:       app: sa-frontend                                  # 7   spec:     containers:       - image: rinormaloku/sentiment-analysis-frontend         imagePullPolicy: Always                         # 8         name: sa-frontend         ports:           - containerPort: 80</code> </pre> <br>  Let's sort this description: <br><br><ol><li>  <code>Kind</code> : it says here that we are describing the resource of the <code>Deployment</code> view. </li><li>  <code>Replicas</code> : a property of the deployment specification object, which specifies how many instances (replicas) of the sweeps need to be run. </li><li>  <code>Type</code> : describes the strategy used in this deployment when upgrading from the current version to the new one.  <code>RollingUpdate</code> strategy provides zero system downtime when upgrading. </li><li>  <code>MaxUnavailable</code> : This is a property of the <code>RollingUpdate</code> object that specifies the maximum number of inaccessible sweeps (compared to the desired number of sweeps) when performing a sequential system update.  In our deployment, which implies the presence of 2 replicas, the value of this property indicates that after the completion of one hearth, another one will be executed, which makes the application available during the update. </li><li>  <code>MaxSurge</code> : This is a property of the <code>RollingUpdate</code> object that describes the maximum number of pods that can be added to the deployment (as compared to the specified number of pods).  In our case, its value, 1, means that, when upgrading to a new version of the program, we can add one more to the cluster, which will lead to the fact that we can simultaneously run up to three subs. </li><li>  <code>Template</code> : This object specifies the pod template that the <code>Deployment</code> resource will use to create new pods.  This setting will probably seem familiar to you. </li><li>  <code>app: sa-frontend</code> : label for pods created on a given pattern. </li><li>  <code>ImagePullPolicy</code> : determines the order of working with images.  In our case, this property is set to <code>Always</code> , that is, during each deployment, the corresponding image will be loaded from the repository. </li></ol><br>  Having examined all this, let's move on to practice.  Run the deployment: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment.yaml deployment "sa-frontend" created</code> </pre> <br>  Check the system status: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                           READY STATUS RESTARTS AGE sa-frontend                    1/1 Running 0 2d sa-frontend-5d5987746c-ml6m4   1/1 Running 0 1m sa-frontend-5d5987746c-mzsgg   1/1 Running 0 1m sa-frontend2                   1/1 Running 0 2d</code> </pre> <br>  As you can see, now we have 4 days.  Two of them were created using the Deployment resource, two more are those that we created ourselves.  Now you can remove those trails that we created ourselves, using the commands of the following form: <br><br><pre> <code class="plaintext hljs">kubectl delete pod &lt;pod-name&gt;</code> </pre> <br>  By the way, here is your task for independent work.  Remove one of the pods created using the Deployment resource and watch the system.  Think about the reasons for what is happening before reading on. <br><br>  When you remove one sub-resource, the Deployment resource learns that the current state of the system (1 sub) is different from the desired one (2 sub-fields), so another sub flow is launched. <br><br>  What is the benefit of Deployment resources, besides the fact that when using them the system is maintained in the right state?  Consider the strengths of these resources. <br><br><h3>  <font color="#3AC1EF">▍ Execute deployment with zero system downtime</font> </h3><br>  Suppose a product manager comes to us and reports that the customer for whom we have created this product wants a green button in the client application.  The developers implement this requirement and give us the only thing we need from them - the image container called <code>rinormaloku/sentiment-analysis-frontend:green</code> .  Now comes our time.  We, the DevOps team, need to perform the deployment of the updated system and ensure zero downtime.  Now let's see if the efforts to master and configure the Deployment resource are justified. <br><br>  Edit the <code>sa-frontend-deployment.yaml</code> file, replacing the image container name with a new one, with <code>rinormaloku/sentiment-analysis-frontend:green</code> , and then save this file as <code>sa-frontend-deployment-green.yaml</code> and execute the following command: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment-green.yaml --record deployment "sa-frontend" configured</code> </pre> <br>  Check the system status with the following command: <br><br><pre> <code class="plaintext hljs">kubectl rollout status deployment sa-frontend Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 of 2 updated replicas are available... deployment "sa-frontend" successfully rolled out</code> </pre> <br>  In accordance with the data output in response to this command, we can conclude that the deployment of the update was successful. В ходе обновления старые реплики, по одной, заменялись на новые. Это означает, что наше приложение, в ходе процесса обновления, всегда было доступно. Прежде чем мы продолжим работу, давайте убедимся в том, что приложений действительно обновилось. <br><br><h4> Проверка развёртывания </h4><br> Для того чтобы взглянуть на то, как приложение выглядит в браузере, воспользуемся уже известной вам командой: <br><br><pre> <code class="plaintext hljs">minikube service sa-frontend-lb</code> </pre> <br> В ответ на неё будет запущен браузер, а в нём откроется страница приложения. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/777/489/902/777489902694f46438ceae41ce59db9b.png"></div><br> <i><font color="#999999">Зелёная кнопка</font></i> <br><br> Как видно, кнопка и правда стала зелёной, значит — обновление системы действительно удалось. <br><br><h4> Закулисье обновления системы по схеме RollingUpdate </h4><br> После того, как мы выполнили команду <code>kubectl apply -f sa-frontend-deployment-green.yaml --record</code> , Kubernetes сравнил состояние системы, к которому мы стремимся прийти, с её текущим состоянием. В нашем случае для перехода в новое состояние нужно, чтобы в кластере имелись бы два пода, основанные на образах <code>rinormaloku/sentiment-analysis-frontend:green</code> . Так как это отличается от того состояния, в котором пребывает система, запускается операция обновления. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/465/7ee/290/4657ee29097dc99e4fa2a0ebd9180e7a.png"></div><br> <i><font color="#999999">Замена подов в ходе обновления системы</font></i> <br><br> Механизм <code>RollingUpdate</code> действует в соответствии с заданными нами правилами, а именно, речь идёт о параметрах <code>maxUnavailable: 1</code> и <code>maxSurge: 1</code> . Это значит, что ресурс Deployment может, при наличии двух работающих подов, остановить один из них, или запустить ещё один под. Этот процесс, показанный на предыдущем рисунке, повторяется до тех пор, пока все старые поды не будут заменены на новые. <br><br> Теперь поговорим об ещё одной сильной стороне ресурсов Deployment. Для того чтобы было интересней, добавим повествованию драматизма. Вот рассказ про ошибку в продакшне. <br><br><h3> <font color="#3AC1EF">▍Откат к предыдущему состоянию системы</font> </h3><br> Менеджер по продукту, сгорая от волнения, влетает в офис. «Баг! В продакшне! Верните всё как было!», — кричит он. Но его беспокойство не заражает вас нездоровым энтузиазмом. Вы, не теряя хладнокровия, открываете терминал и вводите следующую команду: <br><br><pre> <code class="plaintext hljs">kubectl rollout history deployment sa-frontend deployments "sa-frontend" REVISION  CHANGE-CAUSE 1         &lt;none&gt;    2         kubectl.exe apply --filename=sa-frontend-deployment-green.yaml --record=true</code> </pre> <br> Вы смотрите на ранее выполненные развёртывания и спрашиваете у менеджера: «Так, свежая версия даёт сбои, но предыдущая работала отлично?». <br><br> «Да. Вы что, меня не слышали?», — продолжает надрываться менеджер. <br><br> Вы же, не обращая внимания на очередную его попытку вас растревожить, просто вводите в терминале следующее: <br><br><pre> <code class="plaintext hljs">kubectl rollout undo deployment sa-frontend --to-revision=1 deployment "sa-frontend" rolled back</code> </pre> <br> После этого вы открываете страницу приложения. Зелёная кнопка исчезла, а вместе с ней и ошибки. <br><br> Менеджер застывает с отвисшей от удивления челюстью. <br><br> Вы только что спасли компанию от катастрофы. <br><br> Занавес! <br><br> На самом деле, скучновато получилось. До существования Kubernetes в подобных рассказах было куда больше неожиданных поворотов сюжета, больше действия, и они так быстро не заканчивались. Эх, старые добрые времена! <br><br> Большинство вышеупомянутых команд и результатов их работы говорят сами за себя. Пожалуй, непонятной тут может быть лишь одна деталь. Почему <code>CHANGE-CAUSE</code> у первой ревизии имеет значение <code>&lt;none&gt;</code> , а у второй — <code>kubectl.exe apply –filename=sa-frontend-deployment-green.yaml –record=true</code> ? <br><br> Если вы предположите, что причиной появления таких сведений стало использование флага -- <code>record</code> при развёртывании новой версии приложения, то окажетесь совершенно правы. <br><br> В следующем разделе мы воспользуемся всем тем, что уже изучили, для того, чтобы выйти на полноценно работающее приложение. <br><br><h2> <font color="#3AC1EF">Практика работы с Kubernetes: совместное использование изученных механизмов</font> </h2><br> Мы уже разобрались с ресурсами Kubernetes, необходимыми нам для того, чтобы построить полноценное кластерное приложение. На следующем рисунке выделено всё то, что нам ещё нужно сделать. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7cb/af4/880/7cbaf4880d435df50761d22508f61e83.png"></div><br> <i><font color="#999999">Текущее состояние приложение</font></i> <br><br> Начнём работу с нижней части этой схемы. <br><br><h3> <font color="#3AC1EF">▍Развёртывание подов sa-logic</font> </h3><br> Перейдите с помощью терминала в папку проекта <code>resource-manifests</code> и выполните следующую команду: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-logic-deployment.yaml --record deployment "sa-logic" created</code> </pre> <br> Развёртывание <code>sa-logic</code> создаёт три пода. В них выполняются контейнеры Python-приложения. Им назначены метки <code>app: sa-logic</code> . Это позволяет нам работать с ними с помощью сервиса <code>sa-logic</code> , используя соответствующий селектор. Откройте файл <code>sa-logic-deployment.yaml</code> и ознакомьтесь с его содержимым. <br><br> В общем-то, ничего нового для себя вы там не найдёте, поэтому давайте займёмся следующим ресурсом — сервисом <code>sa-logic</code> . <br><br><h3> <font color="#3AC1EF">▍Сервис sa-logic</font> </h3><br> Подумаем о том, зачем нам нужен этот ресурс вида Service. Дело в том, что наше Java-приложение, которое будет выполняться в подах с меткой <code>sa-webapp</code> , зависит от возможностей по анализу текстов, реализуемых Python-приложением. Но сейчас, в отличие от ситуации, в которой всё работает на локальной машине, у нас нет единственного Python-приложения, прослушивающего некий порт. У нас есть несколько подов, количество которых, при необходимости, можно увеличить. <br><br> Именно поэтому нам и нужен сервис, который, как мы уже говорили, действует в качестве точки доступа к сущностям, реализующим одинаковые возможности. Это означает, что мы можем использовать сервис <code>sa-logic</code> в качестве абстракции, дающей возможность работать со всеми подами <code>sa-logic</code> . <br><br> Выполним следующую команду: <br><br><pre> <code class="plaintext hljs">kubectl apply -f service-sa-logic.yaml service "sa-logic" created</code> </pre> <br> Теперь посмотрим на то, как изменилось состояние приложения после выполнения этой команды. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/693/79b/3e3/69379b3e373ad1bc728242db341411ab.png"></div><br> <i><font color="#999999">Изменённое состояние приложения</font></i> <br><br> Теперь сервис <code>sa-logic</code> позволяет, из подов <code>sa-webapp</code> , работать с набором подов, реализующих функционал анализа текста. <br><br> Развернём поды <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍Развёртывание подов sa-webapp</font> </h3><br> Мы уже не раз выполняли развёртывания, но в данном случае в файле описания соответствующего ресурса Deployment вы можете встретить кое-что новое. Так, если заглянуть в файл <code>sa-web-app-deployment.yaml</code> , там можно обратить внимание на следующее: <br><br><pre> <code class="plaintext hljs">- image: rinormaloku/sentiment-analysis-web-app imagePullPolicy: Always name: sa-web-app env:   - name: SA_LOGIC_API_URL     value: "http://sa-logic" ports:   - containerPort: 8080</code> </pre> <br> Какую роль играет свойство <code>env</code> ? Можно предположить, что оно объявляет, внутри подов, переменную окружения <code>SA_LOGIC_API_URL</code> со значением <code>http://sa-logic</code> . Если это так, то хорошо бы понять, почему значение переменной содержит такой необычный адрес. На что он указывает? <br><br> Для того чтобы ответить на этот вопрос нам нужно познакомиться с концепцией kube-dns. <br><br><h3> <font color="#3AC1EF">▍DNS-сервер кластера Kubernetes</font> </h3><br> В Kubernetes есть специальный под, который называется <code>kube-dns</code> . По умолчанию все поды используют его как DNS-сервер. Одной из важных особенностей <code>kube-dns</code> является тот факт, что этот под создаёт DNS-запись для каждого сервиса кластера. <br><br> Это означает, что когда мы создаём сервис <code>sa-logic</code> , ему назначается IP-адрес. В <code>kube-dns</code> делается запись со сведениями об имени и IP-адресе сервиса. Это позволяет всем подам преобразовывать адрес вида <code>http://sa-logic</code> в IP-адрес. <br><br> Теперь продолжим работу с ресурсом Deployment <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍Развёртывание подов sa-webapp</font> </h3><br> Выполните следующую команду: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-web-app-deployment.yaml --record deployment "sa-web-app" created</code> </pre> <br> Теперь нам осталось лишь обеспечить доступ к подам <code>sa-webapp</code> с помощью сервиса, организующего балансировку нагрузки. Это позволит React-приложению выполнять запросы к сервису, который является точкой доступа к подам <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍Сервис sa-webapp</font> </h3><br> Если вы откроете файл <code>service-sa-web-app-lb.yaml</code> , то поймёте, что всё, что там можно увидеть, уже вам встречалось. Поэтому, без ненужных пояснений, выполним следующую команду: <br><br><pre> <code class="plaintext hljs">kubectl apply -f service-sa-web-app-lb.yaml service "sa-web-app-lb" created</code> </pre> <br> Теперь кластер полностью готов. Но, чтобы всё было совсем хорошо, нам надо решить ещё одну задачу. Так, когда мы разворачивали поды <code>sa-frontend</code> , контейнеризированное приложение было рассчитано на обращение к Java-приложению <code>sa-webapp</code> , находящемуся по адресу <code>http://localhost:8080/sentiment</code> . Теперь же нам нужно сделать так, чтобы оно обращалось бы к балансировщику нагрузки, к сервису <code>sa-webapp</code> , который обеспечит взаимодействие React-приложения с подами, в которых запущены экземпляры Java-приложения. <br><br> Исправление этого недостатка даст нам возможность быстро пробежаться по всему тому, изучением чего мы тут занимались. Кстати, если хотите извлечь из проработки этого материала максимум эффективности — вы можете, не читая пока дальше, попытаться исправить этот недостаток самостоятельно. <br><br> Собственно говоря, вот как выглядит пошаговое решение данной проблемы: <br><br><ol><li> Узнаем IP-адрес балансировщика нагрузки <code>sa-webapp</code> , выполнив следующую команду: <br><br> <code>minikube service list <br> |-------------|----------------------|-----------------------------| <br> |  NAMESPACE  | NAME         | URL       | <br> |-------------|----------------------|-----------------------------| <br> | default     | kubernetes         | No node port       | <br> | default     | sa-frontend-lb       | http://192.168.99.100:30708 | <br> | default     | sa-logic         | No node port       | <br> | default     | sa-web-app-lb        | http://192.168.99.100:31691 | <br> | kube-system | kube-dns             | No node port | <br> | kube-system | kubernetes-dashboard | http://192.168.99.100:30000 | <br> |-------------|----------------------|-----------------------------|</code> </li> <li> Используем найденный IP-адрес в файле <code>sa-frontend/src/App.js</code> . Вот фрагмент файла, в который мы вносим изменения: <br><br><pre> <code class="plaintext hljs">analyzeSentence() {       fetch('http://192.168.99.100:31691/sentiment', { /* убрано ради краткости */})           .then(response =&gt; response.json())           .then(data =&gt; this.setState(data));   }</code> </pre> </li><li> Соберём React-приложение, перейдя с помощью терминала в папку <code>sa-frontend</code> и выполнив команду <code>npm run build</code> . </li><li> Соберём образ контейнера: <br><br><pre> <code class="plaintext hljs">docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:minikube.</code> </pre> </li><li> Отправим образ в репозиторий Docker Hub: <br><br><pre> <code class="plaintext hljs">docker push $DOCKER_USER_ID/sentiment-analysis-frontend:minikube</code> </pre> </li><li> Отредактируем файл <code>sa-frontend-deployment.yaml</code> , внеся в него сведения о новом образе. </li><li> Выполним следующую команду: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment.yaml</code> </pre> </li></ol><br> Теперь можно обновить страницу приложения, открытую в браузере, или, если вы уже закрыли окно браузера, можно выполнить команду <code>minikube service sa-frontend-lb</code> . Испытайте систему, попытавшись проанализировать какое-нибудь предложение. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/53d/19c/3ba/53d19c3bac2f8cdd66213c9b34e7b05b.png"></div><br> <i><font color="#999999">Готовое кластерное приложение</font></i> <br><br><h2>  <font color="#3AC1EF">Results</font> </h2><br> Использование технологии Kubernetes способно принести пользу командам разработчиков, это благотворно сказывается на работе над различными проектами, упрощает развёртывание приложений, помогает их масштабировать, делает их устойчивыми к сбоям. Благодаря Kubernetes можно пользоваться ресурсами, предоставляемыми самыми разными облачными провайдерами, и при этом не зависеть от решений конкретных поставщиков облачных услуг. Поэтому предлагаю переименовать Kubernetes в Supernetes. <br><br> Вот что вы узнали, освоив этот материал: <br><br><ul><li> Сборка, упаковка и запуск приложений, основанных на React, Java и Python. </li><li> Работа с контейнерами Docker, а именно, их описание и сборка с использованием файла <code>Dockerfile</code> . </li><li> Работа с репозиториями контейнеров, в частности, с Docker Hub. </li></ul><br> Кроме того, вы освоили важнейшие понятия Kubernetes: <br><br><ul><li> Поды </li><li>  Services </li><li> Развёртывания </li><li> Важные концепции наподобие выполнения обновления приложений без остановки работы системы </li><li> Масштабирование приложений </li></ul><br> В процессе работы мы превратили приложение, состоящее из микросервисов, в кластер Kubernetes. <br><br>  <b>Dear readers!</b> Пользуетесь ли вы Kubernetes? <br><br> <a href="https://ruvds.com/ru-rub/"><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div>