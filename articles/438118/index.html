<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Dex-Net 4.0 allows ambidexter robots to choose the best capture</title>
  <meta name="description" content="The ability to choose a grip helps two-handed robots lift objects faster than ever 


 For several years we have been following the progress of the De...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>Dex-Net 4.0 allows ambidexter robots to choose the best capture</h1><div class="post__text post__text-html js-mediator-article"><h3>  The ability to choose a grip helps two-handed robots lift objects faster than ever </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/b94/a52/853/b94a528531c30220fa11e83bdfb2178c.jpg"><br><br>  For several years we have been following the progress of the <a href="https://berkeleyautomation.github.io/dex-net/">Dex-Net</a> project, which is trying to develop a universal capture for robots, and in the middle of January a new work appeared in the journal Science Robotics, in which scientists from the University of California at Berkeley present Dex-Net 4.0.  The most important and interesting news related to this work is that the latest version of Dex-Net has successfully coped with the capture of 95% of previously unknown objects at a speed of 300 pieces per hour, thanks to the added ambidexter robot, which allows it to choose one of the following on the fly two kinds of grips. <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/r-0PKne9e_w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  So that you can compare, let us say that a person is able to lift such objects twice as fast, from 400 to 600 pieces per hour.  And I would say that in the case of a person, you can expect 100% success in capturing - or at least a good approximation to this figure, if you allow the subject to try several times with each of the objects.  So we set a very high bar for cars.  Part of our success in capturing objects (and the ability to grasp in general) is in the great life experience of working with objects of many shapes, sizes, weights, with friction of various materials and with the possible deformation of objects during capture.  Even without realizing this, we are able to build detailed models of objects in our heads, and they help us to easily grab and lift objects that have never been seen before. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/391/1f2/0e2/3911f20e25fb01e231cb4d6b4758b501.jpg"><br><br>  But robots do not have access to this experimental model of the world.  They rely on learning based on a specific task - and here comes into play Dex-Net.  She learns to capture things, training in simulations, using millions of models of three-dimensional objects and a little random physics, in order to better transfer success in the simulation to the real world.  Artificial ambiguity allows the system to work with things like sensor noise and small gradual calibration shifts ‚Äî of course, more realistic results could be obtained by training real robots, but then such restrictions as the need to have many real robots come into effect time to work - and who wants to wait for them? <br><br>  The uniqueness of Dex-Net 4.0 lies in the fact that the rules for capturing ‚Äúambidext‚Äù objects produced by it, that is, the robot has two captures, and decides which of them should be used at the moment.  However, unlike ambidextral people, this robot has different grips on its hands: a two-finger clamp and a vacuum sucker.  On the basis of a preliminary assessment of the quality of capture, Dex-Net chooses which of the captures can more reliably grab an object.  This technology makes it possible to snap objects quickly and reliably: ABB YuMi in the above video can capture about 300 objects that it has never seen before, per hour, with an efficiency of 95%.  And Dex-Net allows you to connect other types of captures.  After additional training (and by adding grips to a robot, you can teach him to work with electrostatic grippers, five-finger fingers, grips like ‚Äúpaws of a gecko‚Äù or anything else. <br><br>  Of course, it is always interesting to study those 5% of cases when the robot failed to take the object, and here are some examples: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ef1/97d/171/ef197d1711be206d05d5e482f1a09b70.png"><br><br>  The first photo shows ‚Äúproblematic‚Äù objects that are especially difficult to lift due to ‚Äúproblematic geometry, transparency, mirror surface and deformability‚Äù.  Dex-Net copes with such objects only in 63% of cases, although if you allow the system to memorize previous failures and move the object a little, if it is not clear how to grab it, then reliability increases to 80%. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/047/e52/a11/047e52a110fe958257093e508e8b430d.png"><br><br>  The second photo shows objects with which Dex-Net 4.0 cannot cope at all, ‚Äúdue to reflective properties, such as transparency, which affects the perception of depth, and material properties, such as porosity and pliability (for example, loose packaging), which affects the ability to stick to the surface with a vacuum sucker. "  It is worth noting that the two-finger grip does not have any force sensors and tactile sensors, so there is something else to improve in the system. <br><br>  You can also consider cases in which efficiency that does not reach 100% will be acceptable.  There are many realistic ways of handling failures with grips: you can instruct the robot to collect all things from the basket, and send the remainder to a person who will cope with complex objects.  Or, perhaps, at some point it will make sense to change the packaging of items so that it becomes easier to take the items that are especially difficult for a robotic capture.  In any case, this is more a question of ‚Äúwhen‚Äù, rather than ‚Äúif‚Äù, and due to how attractive it is to increase supply chain automation, this is the ‚Äúwhen‚Äù probably will come very, very soon. </div><p>Source: <a href="https://habr.com/ru/post/438118/">https://habr.com/ru/post/438118/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>