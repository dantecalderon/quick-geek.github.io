<div class="post__text post__text-html js-mediator-article"><p>  Digital transformation is a global trend for big business and is vital for adapting an enterprise to the modern needs of a client.  In addition to the usual for large companies, the centralization of systems and the integration of billing systems and subscriber databases are added to the requirements for high availability and real-time operation to which customers are already accustomed to industry leaders (Google, Amazon, Netflix). </p><br><p>  New challenges require new technologies and approaches that are needed to reduce the time to implement customer-friendly functions, personalized business proposals, quickly respond to competitors' proposals, as well as control costs for systems, IT infrastructure, data centers and qualified personnel.  These trends carry a big minus: the complexity of the architecture and bloated transactional databases that do not cope with the flow and processing of information.  Previous generation technologies have a vertical scaling ceiling.  For example, an instance of the Oracle DBMS runs at the limit of the most powerful server on x86 processors with a load of a billion transactions per day. </p><br><img src="https://habrastorage.org/webt/ai/bq/m-/aibqm-rewdzdcsg-6vtcbcv36lw.gif"><br><p>  In order to withstand a similar load with which the Internet industry has long been faced, a new stack of technologies, such as In-Memory caches and NoSQL databases, is used.  So, Apple uses Cassandra, Sberbank - Ignite (GridGain), in MegaFon we use Couchbase and Tarantool. </p><br><p>  MegaFon uses different architectural templates for In-Memory DBMS: </p><br><ol><li>  Simple cache, updated by schedule or by event from the database and applications </li><li>  All changes to the database are made through the cache (write-through script), for example, connecting an Oracle client to the DCP Couchbase </li></ol><br><p><a name="habracut"></a>  For one of our decision-making systems for the subscriber’s life cycle, we use the first template, since only one application takes a decision from the aggregate of data and sends it to all systems, including the Oracle database.  One of the bright cases of using the subscriber’s life cycle is locking and unlocking on a negative balance.  After all, all subscribers of mobile operators after recharging the balance want to immediately be in touch and make calls.  Thanks to a separate application and Couchbase, we were able to reduce the time to break the lock from 90 seconds to 30, and this is not the limit.  The main database will only get a record of the change in the status of the subscriber (Fig. 1) </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lf/to/o5/lftoo5voqj_ycrfngqgwo74_i6w.png"></div><br>  <i>Figure 1 (Interaction Example)</i> <br><br><p>  With the use of new technologies, we were able to reduce the time to exit the financial block by 3 times.  But in order to get current results, we have traveled a long way to the architectural transformation of the billing circuit and the choice of NoSQL database. </p><br><p>  Why did we choose Couchbase?  There are several reasons for this. </p><br><p>  <strong>Performance requirement</strong> </p><br><ol><li>  Handling up to 200,000 requests per second. </li><li>  Average response time (50%) - up to 5 ms (within one data center). </li><li>  The maximum response time (99%) - up to 15 ms (within one data center). </li><li>  Maximum insert capacity 500 MB / sec </li><li>  The maximum number of insert operations 100000 / s </li><li>  Maximum number of change operations (document updates) 100,000 / s </li><li>  Maximum performance changes (document updates) 500 MB / sec </li><li>  The maximum number of read operations 100000 / s </li><li>  Maximum read speed 500 MB / sec </li></ol><br><p>  <strong>High-performance key search and data access</strong> </p><br><p>  Couchbase is based on distributed keystore (KV).  KV storage is an extremely simple data management approach that stores a unique identifier (key) along with a piece of arbitrary information.  The KV storage itself can accept any data, be it a binary blob or a JSON document.  Due to the simplicity of KV implementation, data access is provided with minimal latency.  As our experience shows, more often network latency is 2-3 times higher than providing data by key on the Couchbase side. </p><br><p>  <strong>Dynamic storage scheme (</strong> <strong>JSON)</strong> </p><br><p>  Documents are stored on the Couchbase server in JSON format.  The format supports both basic data types, such as numbers, strings, and complex types, as well as built-in dictionaries and arrays. </p><br><p>  The data scheme in Couchbase is a logical construction, defined by the application and the developer.  Due to its flexibility and ability to use several options, we can use a tag in the document, for example, with version information.  This allows the application to determine in which mode to process the document, as well as to ensure the smooth migration of the database to the new data scheme. </p><br><p>  <strong>High availability</strong> </p><br><p>  One of the components of the information system is its availability.  Couchbase provides high data availability with many different features.  One of them is data replication (distribution of several copies of data on different servers of the cluster), which allows us to provide service during routine maintenance or the failure of some servers. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/jd/3h/5ejd3hwshvqdvwjeuqlovji9ocy.png"></div><br>  <i>Figure 2 (Couchbase server replicas)</i> <br><br><p>  The second important feature for high availability is the Internal Database Change Protocol (DCP).  It provides high-speed transfer of changes to all copies of data, secondary indices (GSI), intercluster replication (XDCR) and external customers. </p><br><p>  <strong>Bidirectional replication</strong> </p><br><p>  The correct practice in companies is to use the redundancy of all business processes and equipment.  Ideally, this is reservation in Active-Active mode, when switching between problem nodes occurs automatically.  Couchbase bidirectional replication enables AA mode.  But replication testing has shown that it is effective only in close data centers.  With the separation of more than 100 km conflicts appear.  Couchbase has conflict resolution mechanisms: based on Timestamp and Sequence Number.  However, due to the time delay on the network, outdated data fall into the database.  We refused to use bi-directional replication (cross-cluster consistency).  All changes are carried out on only one cluster.  Data availability in the “read” mode is provided in all data centers (AA). </p><br><p>  <strong>Horizontal scaling</strong> </p><br><p>  One of the important characteristics of most NoSQL DBs is horizontal scaling (Fig. 3).  The main difference between Couchbase is support for multidimensional scaling, when we in a cluster can increase only the service we need in performance.  For example, the game Pokemon GO uses a split architecture.  At the start of the project 5 servers with combined services were used.  After increasing the load, they applied a spaced architecture: 5 servers with data and 55 servers for processing queries and indexes.  One of the disadvantages of scaling Couchbase is the occurrence of problems with the orchestrator, if there are more than 50 data nodes in the cluster. </p><br><p></p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/79/9c/ne/799cnegj-o20dtxdfcasl3ihm1u.png"></div><br><p>  <i>Figure 3 MDS</i> </p><br><p>  <strong>IS requirements</strong> </p><br><p> Information security requirements influenced our choice to a lesser extent, but their presence in the system made an additional argument in favor of one or another database.  Since the cache may contain personal data, we must comply with the requirements of the regulator.  It is worth deciding: will we use additional equipment or will we be able to provide this with the database itself ?! </p><br><p>  In the enterprise version, Couchbase supports traffic encryption, data encryption and personalized access.  This allows you to save on hardware, such as Cisco ASA. </p><br><p>  <strong>Easy to upgrade</strong> </p><br><p>  One of the significant advantages of Couchbase is a transparent update mechanism and API support for older versions.  At the time of the cluster upgrade, it works in compatibility mode.  New mechanisms will work only after a full upgrade of the cluster.  The impact on running applications is minimal due to the support of the old API. </p><br><p>  <strong>PS:</strong> Update / Downgrade is allowed on neighboring major versions only. </p><br><p>  <strong>Additional functionality</strong> </p><br><p>  <em>Logical distribution</em> </p><br><p>  Another interesting feature is the consolidation of servers in a cluster into logical groups, linked to replicas.  This allows you to distribute complete copies of the replicas of a single cluster to different avtozalam.  That allows to have a complete copy of the data in the second when one of the car halls fails </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/yf/nf/po/yfnfpots5tbrzzew39w4xsehey8.png"></div><br>  <i>Figure 4 Server Gropus</i> <br><br><p>  <b>Backup and Restore</b> </p><br><p>  Couchbase contains ready-made backup and recovery tools.  The backup process can operate in three modes: full, differential and cumulative.  This allows in some cases to save disk space and processor resources. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xe/in/ff/xeinff0eawasua0xiu4ec3yon1y.png"></div><br><p>  <strong>Couchbase vs mongo</strong> </p><br><p>  It’s difficult to answer the question of choosing alternative NoSQL databases, and often the best Unix is ​​the one that your admin knows.  Let us try to formulate why we preferred Couchbase, rather than another very popular platform - MongoDB. </p><br><p>  It is quite difficult to compare two different projects with different architecture and functionality.  One of the parameters to which we paid attention is the ease of maintenance and the ability to quickly reconfigure the system to fit the needs of the business. </p><br><p>  Table 1 Comparison </p><br><table><tbody><tr><td><p> <sup><strong>&nbsp;</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup><strong>Couchbase</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup><strong>MongoDB</strong></sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Scaling</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Automatic for the entire data set</sup> </p> <sup><br></sup> </td><td><p>  <sup>Manual key selection</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Data distribution</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Data is always evenly distributed across all date nodes.</sup> </p> <sup><br></sup> </td><td><p>  <sup>Incorrect markup may result in misallocation of data.</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Adding / deleting a node or replica</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Added in one step via GUI, with rebalancing</sup> </p> <sup><br></sup> </td><td><p>  <sup>Quite a challenge with weight calculations for each collection.</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Distribution of replicas on racks / data center</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Implemented through logical groups</sup> </p> <sup><br></sup> </td><td><p>  <sup>Not implemented</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Automatic load sharing</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Each node has the same number of active records available for reading and writing.</sup> </p> <sup><br></sup> </td><td><p>  <sup>Not balanced.</sup>  <sup>Secondary nodes do not support writing</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Index scaling</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Flexible, you can add a separate node index due to the distributed architecture</sup> </p> <sup><br></sup> </td><td><p>  <sup>Hard scaling index is associated with scaling data.</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Cluster metadata</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Distributed across all cluster nodes</sup> </p> <sup><br></sup> </td><td><p>  <sup>Configuration servers required</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Integrated search</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>N1LQ (SQL ++)</sup> </p> <sup><br></sup> </td><td><p>  <sup>JSON request</sup> </p> <sup><br></sup> </td></tr></tbody></table><br><p>  Table 2 Replication Comparison </p><br><table><tbody><tr><td><p> <sup><strong>&nbsp;</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup><strong>Couchbase</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup><strong>MongoDB</strong></sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Architecture</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Intercluster replication has no dependencies, the cluster is independent of each other</sup> </p> <sup><br></sup> </td><td><p>  <sup>Intracluster Expansion Only</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Customization flexibility</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Flexible (setting up individual buckets, filters, tuning)</sup> </p> <sup><br></sup> </td><td><p>  <sup>Speed ​​tuning</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Topology</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Bidirectional replication, star, chain, etc.</sup> </p> <sup><br></sup> </td><td><p>  <sup>Star</sup> </p> <sup><br></sup> </td></tr><tr><td><p>  <sup><strong>Active-Active mode</strong></sup> </p> <sup><br></sup> </td><td><p>  <sup>Supported by</sup> </p> <sup><br></sup> </td><td><p>  <sup>Not supported</sup> </p> <sup><br></sup> </td></tr></tbody></table><br><p>  In general, Couchbase is more flexible and simpler in the settings required for our tasks and the rapidly changing hybrid architecture. </p><br><p>  <strong>Operating experience</strong> </p><br><p>  For a start, we would like to give the numbers with which the system and the cluster on Couchbase are now operating. </p><br><ol><li>  More than 80 million subscribers <a href="https://habr.com/ru/post/436762/">[i]</a> </li><li>  380 million JSON client information documents </li><li>  3.5 TB HDD (we use memcached, information is stored on the disk for a quick start) </li><li>  3 TB RAM </li><li>  50 thousand operations per second (Figure 5) </li><li>  50 microservices processing the entire message flow </li></ol><br><p></p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lv/nq/wc/lvnqwc_jmlr5jukgm10-ibu7qd8.png"></div><br>  <i>Figure 5 Load</i> <br><p>  We started the first milestones of transformation with the third version of Couchbase.  At the first stage, at the launch of the project, all applications worked stably.  But when transferring additional logic to the new mechanism, we were faced with the fact that the View mechanism began to work unpredictably.  Those.  at some point, the process <strong>hung</strong> and these views from such nodes stopped returning.  In this case, access to data and their processing is not interrupted.  The problem was corrected quite easily - by restarting the node, which generally reduced the availability of the service.  In the course of communication with Couchbase technical support, we were offered an undocumented command that restarts only the view process. </p><br><p>  <em>curl -s --data 'cb_couch_sup: restart_couch ().'</em>  <em>-u Administrator: pass <a href="http://127.0.0.1:8091/diag/eval">http://127.0.0.1:8091/diag/eval</a> <a href="https://habr.com/ru/post/436762/"><strong>[ii]</strong></a></em> </p><br><p>  The command is valid only in versions 3.x. </p><br><p>  <em>curl -s --data 'couch_server_sup: restart_core_server ().'</em>  <em>-u Administrator: Administrator</em> <em><a href="http://127.0.0.1:8091/diag/eval">http://127.0.0.1:8091/diag/eval</a></em> </p><br><p>  The command is valid only in versions 4.x. </p><br><p>  Another problem of the third version was the breaking data compression mechanism (compaction).  It had to be started manually according to the monitored monitoring metrics.  Both problems kept in suspense not only the duty shift, but also functional engineers. </p><br><p>  In this regard, we have decided to migrate to the fourth version.  Migration with minimal impact on the service took about two weeks.  The update process itself does not require complicated actions and control, but when adding or removing a node, a rebalance takes at least two hours to start.  In the course of work, we found a way to speed up the update process through a buffer server: in this case, it is not a pure rebalancing process that is launched, but data transfer from one node to another.  This reduced the update process to 30 minutes. </p><br><p>  When upgrading an industrial cluster, you need to consider the following nuance: working in compatibility mode, when the cluster is operating in the mode of the youngest software version.  The positive side is that the update process goes smoothly and painlessly, but nevertheless, new functions, such as the new compression mechanism, N1QL, cannot be used until the entire cluster is updated. </p><br><p>  After the upgrade, we managed to fix only one problem - compression.  It began to work properly.  With the View mechanism, the problem still remained, although it was repeated much less frequently.  It was possible to correct it only by the Couchbase developers in version 4.6.4. </p><br><p>  As part of solving technical support problems, it turned out that the view mechanism will not be updated anymore.  This was done on the basis that most Couchbase clients use views not for the purposes for which they were created, and Couchbase made the new N1QL mechanism.  It is implemented as a separate service and is now independent of data nodes (Fig. 7) </p><br><p></p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/df/xn/x2/dfxnx2uou3mvgpgdvtetuzsshj0.png"></div><br>  <i>Figure 7 Role Nodes</i> <br><br><p>  We have closed all critical problems with version 4.6.4.  But due to the increase in data volume, a decision was made to migrate to the fifth version, where a new database was added for the indices and, on our data, the volume in memory and disks decreased by one and a half times.  But, unfortunately, we did not see a decrease in the amount of data on the data nodes. </p><br><p>  <strong>findings</strong> </p><br><p>  In general, Couchbase proved to be a mature system, keeping a high load, even in non-specific cases (Viber - used as a database).  As part of the hybrid architecture of MegaFon, the cluster can be easily adapted for any purpose without equipment downtime and without serious reconfiguration of servers, which generally allows the company to reduce personnel costs and make the service for the subscriber as convenient as possible. </p><br><p>  PJSC MegaFon </p><br><p>  2018 Kovalchuk Egor </p><br><p>  <a href="https://habr.com/ru/post/436762/">[i] The</a> system processes not only subscribers, but also devices with built-in sim cards, modems, etc. </p><br><p>  <a href="https://habr.com/ru/post/436762/">[ii]</a> Consult a specialist before use. </p></div>