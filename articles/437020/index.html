<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What is wrong with reinforcement learning (Reinforcement Learning)?</title>
  <meta name="description" content="Back in the beginning of 2018, the Deep Reinforcement Learning Doesn't Work Yet article was published (" Reinforcement Training Doesn't Work Yet "). T...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>What is wrong with reinforcement learning (Reinforcement Learning)?</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/hv/1l/vs/hv1lvsyszoctmnrbxex7valfo8a.jpeg"></p><br><p>  Back in the beginning of 2018, the <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Deep Reinforcement Learning Doesn't Work Yet</a> article was published (" <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Reinforcement</a> Training <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Doesn't Work Yet</a> ").  The main complaint of which was reduced to the fact that modern learning algorithms with reinforcement require approximately the same time to solve a problem as a normal random search. </p><br><p>  Has anything changed since that time?  Not. </p><br><p>  Reinforcement training is considered one of the three main ways to build a strong AI.  But the difficulties faced by this area of ‚Äã‚Äãmachine learning, and the methods by which scientists are trying to deal with these difficulties, suggest that, perhaps with the very approach, there are fundamental problems. </p><a name="habracut"></a><br><h2 id="postoyte-chto-znachit-odin-iz-treh-a-ostalnye-dva-kakie">  Wait, what does one of the three mean?  And the other two are what? </h2><br><p>  Given the success of neural networks in recent years and an analysis of how they work with high-level cognitive abilities, previously considered characteristic only of humans and higher animals, today the scientific community has the opinion that there are three main approaches to creating a strong AI on based on neural networks that can be considered less realistic: </p><br><h2 id="1-obrabotka-tekstov">  1. Text processing </h2><br><p>  The world has accumulated a huge number of books and text on the Internet, including textbooks and reference books.  The text is convenient and fast for processing on the computer.  Theoretically, this array of texts should be enough for learning a strong conversational AI. </p><br><p>  In this case, it is implied that these text arrays reflect the complete structure of the world (at a minimum, it is described in textbooks and reference books).  But this is absolutely not a fact.  Texts as a type of representation of information are strongly separated from the real three-dimensional world and the flow of time in which we live. </p><br><p>  Good examples of AI trained in text arrays are chat bots and automatic translators.  Since the translation of the text you need to understand the meaning of the phrase and retell it with new words (in another language).  There is a common misconception that the rules of grammar and syntax, including the description of all possible exceptions, completely describe a specific language.  This is not true.  Language is only an auxiliary tool in life, it changes easily and adapts to new situations. </p><br><p>  The problem of word processing (even with expert systems, even with neural networks) is that <strong>there is no</strong> set of rules for which phrases in which situations to apply.  Pay attention - not the rules for constructing the phrases themselves (what the grammar and syntax do), but exactly which phrases in which life situations.  In the same situation, people pronounce phrases in different languages ‚Äã‚Äãthat are not related to each other at all in terms of the structure of the language.  Compare phrases with extreme astonishment: "oh my God!"  and "o, holy shit!".  Well, and how between them to hold the correspondence, knowing the language model?  Yes, nothing.  It so happened historically.  You need to know the situation and what is usually spoken in a particular language.  It is precisely because of this that automatic translators are so imperfect. </p><br><p>  Whether it is possible to isolate this knowledge purely from an array of texts is unknown.  But if automatic translators start to translate perfectly, without making silly and ridiculous mistakes, then this will be proof that creating a strong AI based on text alone is possible. </p><br><h2 id="2-raspoznavanie-izobrazheniy">  2. Image recognition </h2><br><p>  Look at this image </p><br><p><img src="https://habrastorage.org/webt/pa/od/nd/paodndrl6p5dkuhig3rwo68cu-q.jpeg"></p><br><p>  Looking at this photo, we understand that the shooting was done at night.  Judging by the flags, the wind blows from right to left.  And judging by the right-hand traffic, the case does not occur in England or Australia.  None of this information is explicitly indicated in the pixels of the picture, it is external knowledge.  In the photo there are only signs on which we can use the knowledge obtained from other sources. </p><br><div class="spoiler">  <b class="spoiler_title">Do you know something else, looking at this picture?</b> <div class="spoiler_text"><p>  About that and speech ... And find yourself a girl, finally </p></div></div><br><p>  Therefore, it is believed that if you train a neural network to recognize objects in the picture, then it will form an internal idea of ‚Äã‚Äãhow the real world works.  And this view, obtained from photographs, will surely correspond to our real and real world.  Unlike text arrays, where this is not guaranteed. </p><br><p>  The value of neural networks trained on the ImageNet photo array (and now <a href="https://storage.googleapis.com/openimages/web/index.html">OpenImages V4</a> , <a href="http://cocodataset.org/">COCO</a> , <a href="http://www.cvlibs.net/datasets/kitti/">KITTI</a> , <a href="https://bdd-data.berkeley.edu/">BDD100K,</a> and others) is not at all in the fact of recognition of the cat in the photo.  And that is stored in the penultimate layer.  It is there that is a set of high-level features that describe our world.  A vector of 1024 numbers is enough to get a description of 1000 different categories of objects with 80% accuracy (and in 95% of cases the correct answer will be in the 5 closest variants).  Just think about it. </p><br><p>  That is why these features from the penultimate layer are so successfully used in completely different tasks in computer vision.  Through Transfer Learning and Fine Tuning.  From this vector in 1024 numbers you can get, for example, a depth map for the picture </p><br><p><img src="https://habrastorage.org/webt/vs/k6/lm/vsk6lmod2grqjzous7knxl5ekaq.jpeg"></p><br><p>  (example from the <a href="https://arxiv.org/abs/1812.11941">work</a> where the practically unchanged pre-trained network Densenet-169 is used) </p><br><p>  Or determine the posture of the person.  There are many applications. </p><br><p><img src="https://habrastorage.org/webt/id/rs/sp/idrsspge5oaq0dae1-li5pghf3s.jpeg"></p><br><p>  As a consequence, image recognition can potentially be used to create a strong AI, as it truly reflects the model of our real world.  From photography to video is one step, and video is our life, since we receive about 99% of the information visually. </p><br><p>  But the photograph is completely incomprehensible how to motivate the neural network to think and draw conclusions.  She can be trained to answer questions like "how many pencils are on the table?"  (this class of tasks is called Visual Question Answering, an example of such a dataset: <a href="https://visualqa.org/">https://visualqa.org</a> ).  Or give a text description of what is happening in the photo.  This is the <a href="https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2">Image Captioning</a> class of tasks. </p><br><p><img src="https://habrastorage.org/webt/mp/lz/0y/mplz0y9uleukwz68u-lyc35wlqk.jpeg"></p><br><p> But is this an intelligence?  Having developed this approach, in the near future, neural networks will be able to answer video questions like "Two sparrows sat on wires, one of them flew away, how many sparrows left?".  This is already real mathematics, in slightly more complicated cases inaccessible to animals and located at the level of human school education.  Especially if, apart from sparrows, there will be titmouses sitting next to them, but they should not be taken into account, since the question was only about sparrows.  Yes, it will definitely be intelligence. </p><br><h2 id="3-obuchenie-s-podkrepleniem-reinforcement-learning">  3. Reinforcement Learning </h2><br><p>  The idea is very simple: to encourage actions that lead to reward, and to avoid leading to failure.  This is a universal way of learning and, obviously, it can definitely lead to the creation of a strong AI.  Therefore, to Reinforcement Learning such a great interest in recent years. </p><br><div class="spoiler">  <b class="spoiler_title">Mix, but do not shake</b> <div class="spoiler_text"><p>  Of course, it is best to create a strong AI by combining all three approaches.  The pictures and training with reinforcements can be obtained AI level of animals.  And by adding text names of objects to pictures (joking, of course, by forcing AI to watch videos where people interact and talk, as when teaching a baby), and after training on a text array to get knowledge (analog of our school and university), in theory you can get AI human level.  Able to talk. </p></div></div><br><p>  Reinforcement training has one big plus.  In the simulator, you can create a simplified model of the world.  So, for a human figure, only 17 degrees of freedom are sufficient, instead of 700 in a living person (approximate number of muscles).  Therefore, in the simulator, you can solve the problem in a very small dimension. </p><br><p>  Looking ahead, modern Reinforcement Learning algorithms are not capable of arbitrarily controlling a person‚Äôs model, even with 17 degrees of freedom.  That is, they cannot solve the optimization problem, where the input numbers are 44 and the output 17. It can only be done in very simple cases, with fine manual adjustment of the initial conditions and hyperparameters.  And even in this case, for example, to teach a humanoid model with 17 degrees of freedom to run, and starting from a standing position (which is much simpler), you need several days of calculations on a powerful GPU.  A little more complicated cases, such as learning how to get up from an arbitrary pose, can never learn at all.  This is a failure. </p><br><p>  In addition, all Reinforcement Learning algorithms work with disappointingly small neural networks, and with large learning can not cope.  Large convolutional networks are used only to reduce the dimension of the picture to a few features, which are fed to the input of learning algorithms with reinforcement.  The same running humanoid is controlled by a Feed Forward network with two or three layers of 128 neurons each.  Seriously?  And based on this, are we trying to build a strong AI? </p><br><p>  To try to understand why this is happening and what is wrong with reinforcement training, you must first familiarize yourself with the main architectures in modern Reinforcement Learning. </p><br><p>  The physical structure of the brain and nervous system is tuned by evolution to a specific animal species and its habitat conditions.  Thus, in the process of evolution, a fly developed such a nervous system and such work of neurotransmitters in the ganglia (analogous to the brain in insects) in order to quickly dodge the fly swatter.  Well, not from a fly swatter, but from birds that caught them 400 million years (joke, the birds themselves appeared 150 million years ago, rather from frogs 360 million years).  And the rhinoceros has enough of such a nervous system and brain to slowly turn towards the goal and start running.  And there, as they say, the rhino has poor eyesight, but these are not his problems. </p><br><p>  But besides evolution, it is the usual mechanism of reinforcement learning that works for each individual individual, starting at birth and throughout life.  In the case of mammals <a href="http://elementy.ru/novosti_nauki/431926/Nasekomye_oshchushchayut_udovolstvie_s_pomoshchyu_dofamina_kak_i_mlekopitayushchie">and insects, too</a> , this work is done by the dopamine system.  Her work is full of secrets and nuances, but it all comes down to the fact that in the case of receiving an award, the dopamine system, through memory mechanisms, somehow fixes the connections between neurons that were active just before.  This is how the associative memory is formed. </p><br><p>  Which, by virtue of its associativity, is then used in decision making.  Simply put, if the current situation (the current active neurons in this situation) activate the neurons of the memory of pleasure by associative memory, then the individual chooses the actions that she did in a similar situation and which she remembered.  "Choosing actions" is a bad definition.  No choice.  Simply activated neurons of the memory of pleasure, fixed by the dopamine system for this situation, automatically activate the motor neurons, leading to muscle contraction.  This is if immediate action is needed. </p><br><p>  Artificial learning with reinforcement, as a field of knowledge, needs to solve both of these tasks: </p><br><h3 id="1-podobrat-arhitekturu-neyroseti-chto-dlya-nas-uzhe-sdelala-evolyuciya">  1. Choose the neural network architecture (which evolution has already done for us) </h3><br><p>  The good news is that the higher cognitive functions performed in the neocortex in mammals (and <a href="https://habr.com/ru/company/dronk/blog/369395/">in the corpus striatum of corvids</a> ) are performed in an approximately homogeneous structure.  Apparently, this does not need some kind of rigidly prescribed "architecture". </p><br><p>  The diversity of brain areas is probably due to purely historical reasons.  When, as evolution progressed, new parts of the brain grew on top of the base ones left over from the very first animals.  By the principle works - do not touch.  On the other hand, in different people, the same parts of the brain react to the same situations.  This can be explained both by associativity (features and "grandmother's neurons" naturally formed in these places in the learning process) and by physiology.  That the signal paths encoded in the genes lead precisely to these areas.  There is no consensus here, but you can read, for example, this recent article: <a href="https://medium.com/%40culurciello/biological-and-artificial-intelligence-23a5c65160e6">"Biological and artificial intelligence"</a> . </p><br><h3 id="2-nauchitsya-obuchat-neyronnye-seti-po-principam-obucheniya-s-podkrepleniem">  2. Learn to teach neural networks on the principles of learning with reinforcement </h3><br><p>  This is precisely what modern Reinforcement Learning is doing.  And what is the success?  Not really. </p><br><h1 id="naivnyy-podhod">  Naive approach </h1><br><p>  It would seem that it is very simple to train a neural network with reinforcements: we do random actions, and if we receive an award, we consider the actions made as ‚Äúreference‚Äù.  We put them on the output of the neural network as standard labels and train the neural network using the method of back propagation of an error, so that it would give just such an output.  Well, the most common learning neural network.  And if actions lead to failure, then either we ignore this case, or we suppress these actions (we put some others as a reference at the output, for example, any other random action).  In general, this idea repeats the dopamine system. </p><br><p>  But if you try to train any neural network in this way, no matter how complex the architecture is, recurrent, convolutional, or regular direct propagation, then ... It won't work! </p><br><p>  Why?  Unknown. </p><br><p>  It is believed that the useful signal is so small that it is lost against the background of noise.  Therefore, the network does not learn the standard back propagation method.  Reward happens very rarely, maybe once out of hundreds or even thousands of steps.  And even LSTM remembers a maximum of 100-500 points of history, and then only in very simple tasks.  And on more complex ones, if there are 10-20 points of history, then this is already good. </p><br><p>  But the root of the problem is in very rare rewards (at least in tasks of practical value).  At the moment, we are not able to train neural networks that would memorize isolated cases.  What the brain copes with shine.  You can remember something just once, remember for a lifetime.  And, by the way, most of the training and work of the intellect is based on such cases. </p><br><p>  This is something like a terrible imbalance of classes from the field of image recognition.  There is simply no way to deal with this.  The best that we could come up with so far is simply to submit to the input of the network along with new situations, successful situations from the past saved in an artificial special buffer.  That is, constantly teach not only new cases, but also successful old ones.  Naturally, it is impossible to infinitely increase such a buffer, and it is not clear what exactly to store in it.  They are still trying to fix the paths within the neural network for some time, which were active during the successful event so that the subsequent training would not overwrite them.  A rather close analogy to what is happening in the brain, in my opinion, although we have not yet achieved much success in this direction either.  Since the new trained tasks in their calculation use the results of the output of neurons from the frozen paths, as a result, the signal only over these frozen interferes with the new ones, and the old tasks stop working.  There is another interesting approach: to teach the network new examples / tasks only in the orthogonal direction to the previous tasks ( <a href="https://arxiv.org/abs/1810.01256">https://arxiv.org/abs/1810.01256</a> ).  This does not overwrite previous experience, but drastically limits network capacity. </p><br><p>  A separate class of algorithms designed to deal with this disaster (and at the same time giving hope to achieve a strong AI), are being developed in Meta-Learning.  These are attempts to teach a neural network several tasks at once.  Not in the sense of recognizing different pictures in the same task, namely, different tasks in different domains (each with its own distribution and landscape of solutions).  Say, recognize pictures and simultaneously ride a bike.  Successes are not very good either, as it usually all comes down to preparing a neural network with common universal weights in advance, and then quickly, in just a few steps of a gradient descent, to adapt them to a specific task.  Examples of meta-learning algorithms are MAML and <a href="https://blog.openai.com/reptile/">Reptile</a> . </p><br><p>  In general, only this problem (the inability to learn from single successful examples) puts an end to modern training with reinforcement.  All the power of neural networks before this sad fact is powerless. </p><br><p>  This fact that the easiest and most obvious way does not work, forced the researchers to return to the classic tabular Reinforcement Learning.  Which as a science appeared in antiquity, when neural networks were not even in the project.  But now, instead of manually counting the values ‚Äã‚Äãin the tables and in the formulas, let's use such a powerful approximator as neural networks as target functions!  This is the essence of modern Reinforcement Learning.  And its main difference from the usual learning neural networks. </p><br><h1 id="q-learning-i-dqn">  Q-learning and DQN </h1><br><p>  Reinforcement Learning (even before neural networks) was born as a fairly simple and original idea: let's do, again, random actions, and then for each cell in the table and each direction of movement, we calculate using a special formula (called Bellman‚Äôs equation, you‚Äôll be to meet in virtually every training activity with reinforcement) how good this cell and the chosen direction are.  The higher this number is, the more likely this path leads to victory. </p><br><p><img src="https://habrastorage.org/webt/nx/zm/-7/nxzm-7q1_oc-igaim3j0mrr7vki.png"></p><br><p>  Whatever cell you are in, move upwards in green!  (towards the maximum number on the sides of the current cell). </p><br><p>  This number is called Q (from the word quality is the quality of choice, obviously), and the method is Q-learning.  Replacing the formula for calculating this number on a neural network, or rather, teaching the neural network using this formula (plus a couple of tricks related purely to the math of learning neural networks), Deepmind obtained the <a href="https://deepmind.com/research/dqn/">DQN</a> method.  This is which in 2015 won the Atari pile of games and marked the beginning of a revolution in Deep Reinforcement Learning. </p><br><p>  Unfortunately, this method in its architecture only works with discrete discrete actions.  In DQN, the current state (current situation) is fed to the input of the neural network, and the neural network predicts the Q number at the output. And since all the possible actions are listed at the output of the network (each with its predicted Q), it turns out that the neural network in DQN implements the classic Q function (s, a) from Q-learning.  Returns Q for state and action (therefore, Q (s, a) as a function of s and a).  We are simply looking for the usual argmax by array among the network outputs of the cell with the maximum number Q and do the action that corresponds to the index of this cell. </p><br><p>  And you can always choose an action with a maximum Q, then this policy will be called deterministic.  And you can choose an action as random from the available ones, but in proportion to their Q-values ‚Äã‚Äã(that is, actions with high Q will be chosen more often than with low).  This policy is called stochastic.  The stochastic choice plus is that the search and exploration of the world is automatically realized (Exploration), since each time different actions are selected, sometimes not seemingly the most optimal, but which can lead to a big reward in the future.  And then we will learn and increase the probability of these actions, so that now they are more often chosen according to their probability. </p><br><p>  But what if options are endless?  If this is not the 5 buttons on the joystick in Atari, but the continuous torque control of the robot engine?  Of course, the moment in the range -1..1 can be divided into subranges of 0.1, and at each moment of time you can choose one of these subranges, like pressing a joystick in Atari.  But not always the right number can be discretized at intervals.  Imagine that you are riding a bike through a mountain peak.  And you can only turn the steering wheel 10 degrees left or right.  At some point, the peak may become so narrow that turning 10 degrees in both directions will result in a fall.  This is the fundamental problem of discrete actions.  Moreover, DQN does not work with large dimensions, and even with 17 degrees of freedom it simply does not converge on a robot.  Everything is good, but there is a small nuance, as they say. </p><br><p>  Later, many original and sometimes ingenious algorithms based on DQN were developed, which allowed, among other things, to work with continuous actions (due to tricks and introduction of additional neural networks): DDQN, DuDQN, BDQN, CDQN, NAF, Rainbow.  Perhaps, here you can also include <a href="https://flyyufelix.github.io/2017/11/17/direct-future-prediction.html">Direct Future Prediction (DFP)</a> , which is related to the DQN network architecture and discrete actions.  Instead of predicting the Q number for all actions, DFP directly predicts how much health or ammo will be in the next step if you choose this action.  And one step forward and several steps forward.  We can only go through all the network outputs and find the maximum value of the parameter of interest to us and choose the appropriate action for this element of the array, depending on current priorities.  For example, if we are injured, we can look for an action among the exits of the network leading to the maximum increase in health. </p><br><p>  But more importantly, in the ensuing time, new architectures were developed specifically for Reinforcement Learning. </p><br><p><img src="https://habrastorage.org/webt/f3/lc/3t/f3lc3tno4mpvwren4rfocva9iv8.png"></p><br><h1 id="policy-gradient">  Policy gradient </h1><br><p>  Let's input the current state to the network input, and immediately predict actions at the output (either the actions themselves or the probability distribution for them in a stochastic policy).  We simply act using actions predicted by the neural network.  And then we look, what reward R has typed for an episode.  This award can be either higher than the initial (when won in the game) or lower (lost in the game).  You can also compare the reward with a certain average reward.  Above it is average or lower. </p><br><p>  Actually, the dynamics of the received award R as a result of actions that the neural network prompted can be used to calculate the gradient using a special formula.  And apply this gradient to the weights of the neural network!  And then use the usual reverse error propagation.  Simply, instead of "reference" actions at the exit of the network as labels (we do not know what they should be), we use the change of the reward to calculate the gradient.  According to this gradient, the network will learn to predict actions that lead to an increase in the R award. </p><br><p>  This is a classic policy gradient.  But he has a drawback - you have to wait until the end of the episode to calculate the cumulative reward R before changing the weights of the network according to its change.  And of the advantages - a flexible system of rewards and punishments, which not only works in both directions, but also depends on the size of the reward.  A big reward more strongly encourages the actions that led to it. </p><br><h1 id="actor-critic-ddpg">  Actor-critic, DDPG </h1><br><p>  Now imagine that we have two networks - one predicts what actions to take, and the second assesses how good these actions are.  That is, it gives a Q-number for these actions, as in the DQN algorithm.  The state is fed to the input of the first network, and it predicts action (s).  The second network also receives state at the input, but also the action actions predicted by the first network, and the output gives the number Q as a function of both of them: Q (s, a). </p><br><p>  Actually, this number Q (s, a), issued by the second network (it is called a critic, critic), can also be used to calculate the gradient, which updates the weights of the first network (which is called an actor), as we did above with the award R Well, the second network is updated in the usual way, according to the actual passage of the episode.  This method is called actor-critic.  Its plus in comparison with the classical Policy Gradient, that the weights of the network can be updated at each step, without waiting for the end of the episode.  What speeds up learning. </p><br><p>  As such, it is a DDPG network.  Since it directly predicts the actions of actions, it works great with continuous actions.  DDPG is a direct continuous competitor of DQN with its discrete actions. </p><br><p><img src="https://habrastorage.org/webt/9b/th/fk/9bthfkh7cfpymc6_f6xrt7sica0.png"></p><br><h1 id="advantage-actor-critic-a3ca2c">  Advantage Actor Critic (A3C / A2C) </h1><br><p>  The next step was to use the number of Q (s, a) for teaching the first network not just the critic's predictions ‚Äî how good the actions predicted by the actor actor, as it was in the DDPG.  And how much these predicted actions turned out to be better or worse than we expected. </p><br><p>  This is very close to what happens in the biological brain. –ò–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤—ã–±—Ä–æ—Å –¥–æ—Ñ–∞–º–∏–Ω–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–µ –≤–æ –≤—Ä–µ–º—è —Å–∞–º–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏—è, –∞ –≤–æ –≤—Ä–µ–º—è <strong>–æ–∂–∏–¥–∞–Ω–∏—è</strong> , —á—Ç–æ —Å–∫–æ—Ä–æ –ø–æ–ª—É—á–∏–º —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ. –í–ø—Ä–æ—á–µ–º, –µ—Å–ª–∏ –æ–∂–∏–¥–∞–Ω–∏—è –Ω–µ –æ–ø—Ä–∞–≤–¥–∞–ª–∏—Å—å, —Ç–æ –Ω–∞—Å—Ç—É–ø–∞—é—Ç —É–∂–∞—Å–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è, –±–æ–ª—å—à–∏–µ —á–µ–º –≤ –æ–±—ã—á–Ω–æ–º —Å–ª—É—á–∞–µ (–≤ –æ—Ä–≥–∞–Ω–∏–∑–º–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–∫–∞–∑–∞–Ω–∏—è, –æ–±—Ä–∞—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è). </p><br><p> –î–ª—è —ç—Ç–æ–≥–æ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å—Ç–∞–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ —á–∏—Å–ª–æ Q(s,a), –∞ —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–µ Advantage: A(s,a) = Q(s,a) ‚Äî V(s). –ß–∏—Å–ª–æ A(s,a) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ Q(s,a) –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, –∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π —Å—Ç–∞–Ω–µ—Ç –ª—É—á—à–µ, —á–µ–º —Ç–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è V(s). –ï—Å–ª–∏ A(s,a) &gt; 0, —Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –±—É–¥–µ—Ç –∏–∑–º–µ–Ω—è—Ç—å –≤–µ—Å–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –ø–æ–æ—â—Ä—è—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–µ—Ç—å—é –¥–µ–π—Å—Ç–≤–∏—è. –ï—Å–ª–∏ A(s,a) &lt; 0, —Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –±—É–¥–µ—Ç –∏–∑–º–µ–Ω—è—Ç—å –≤–µ—Å–∞ —Ç–∞–∫, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –±—É–¥—É—Ç –ø–æ–¥–∞–≤–ª—è—Ç—å—Å—è, —Ç.–∫. –æ–Ω–∏ –æ–∫–∞–∑–∞–ª–∏—Å—å –ø–ª–æ—Ö–∏–µ. </p><br><p> –í —ç—Ç–æ–π —Ñ–æ—Ä–º—É–ª–µ V(s) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à —Ç–µ–∫—É—â–∏–π state —Å–∞–º –ø–æ —Å–µ–±–µ, –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –¥–µ–π—Å—Ç–≤–∏—è–º (–ø–æ—ç—Ç–æ–º—É –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç s, –±–µ–∑ a). –ï—Å–ª–∏ –º—ã —Å—Ç–æ–∏–º –≤ —à–∞–≥–µ –æ—Ç –≤–µ—Ä—à–∏–Ω—ã –≠–≤–µ—Ä–µ—Å—Ç–∞ ‚Äî —ç—Ç–æ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∞—è —Å–∏—Ç—É–∞—Ü–∏—è state, —Å –±–æ–ª—å—à–∏–º V(s). –ê –µ—Å–ª–∏ –º—ã —É–∂–µ —Å–æ—Ä–≤–∞–ª–∏—Å—å –∏ –ø–∞–¥–∞–µ–º, —Ç–æ —ç—Ç–æ –æ—Ñ–∏–≥–µ—Ç—å –∫–∞–∫–æ–π –ø–ª–æ—Ö–æ–π state, —Å –Ω–∏–∑–∫–∏–º V(s). </p><br><p> –ö —Å—á–∞—Å—Ç—å—é, –ø—Ä–∏ —Ç–∞–∫–æ–º –ø–æ–¥—Ö–æ–¥–µ Q(s,a) –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–∞–≥—Ä–∞–¥—É r, –∫–æ—Ç–æ—Ä—É—é –ø–æ–ª—É—á–∏–º –ø–æ—Å–ª–µ —Å–æ–≤–µ—Ä—à–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è, –∏ —Ç–æ–≥–¥–∞ —Ñ–æ—Ä–º—É–ª–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ–ª—É—á–∞–µ—Ç—Å—è A = r ‚Äî V(s). </p><br><p> –í —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ V(s) (–∞ –Ω–∞–≥—Ä–∞–¥—É –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º —É–∂–µ –ø–æ —Ñ–∞–∫—Ç—É —á—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏), –∏ –¥–≤–µ —Å–µ—Ç–∏ ‚Äî actor –∏ critic, –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É! –ö–æ—Ç–æ—Ä–∞—è –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ state, –∞ –Ω–∞ –≤—ã—Ö–æ–¥–µ —Ä–∞–∑–¥–µ–ª—è–µ—Ç—Å—è –Ω–∞ –¥–≤–µ –≥–æ–ª–æ–≤—ã head: –æ–¥–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è actions, –∞ –¥—Ä—É–≥–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç V(s). –¢–∞–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç –ª—É—á—à–µ –ø–µ—Ä–µ–∏c–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞, —Ç.–∫. –æ–±–µ —Å–µ—Ç–∏ –¥–æ–ª–∂–Ω—ã –Ω–∞ –≤—Ö–æ–¥–µ –ø–æ–ª—É—á–∞—Ç—å state. –í–ø—Ä–æ—á–µ–º, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ –¥–≤–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–µ—Ç–∏. </p><br><p><img src="https://habrastorage.org/webt/eo/ph/5y/eoph5ypzawg11tachwn-nt_7nyg.png"></p><br><p> –£—á–µ—Ç –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–µ—Ç—å—é –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏ V(s) –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ –ø–æ–º–æ–≥–∞–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ. –¢–∞–∫ –∫–∞–∫ –ø—Ä–∏ –ø–ª–æ—Ö–æ–º V(s), –≥–¥–µ —É–∂–µ –Ω–∏—á–µ–≥–æ –Ω–µ–ª—å–∑—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–∏ –ø—Ä–∏ –∫–∞–∫–∏—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö action (–º—ã –ª–µ—Ç–∏–º –≤–Ω–∏–∑ –≥–æ–ª–æ–≤–æ–π —Å –≠–≤–µ—Ä–µ—Å—Ç–∞), –º–æ–∂–Ω–æ –Ω–µ –∏—Å–∫–∞—Ç—å –¥–∞–ª—å—à–µ –ø—É—Ç–∏ —Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Dueling Q-Network (DuDQN), –≥–¥–µ Q(s,a) –≤–Ω—É—Ç—Ä–∏ —Å–µ—Ç–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –Ω–∞ Q(s,a) = V(s) + A(a), –∞ –ø–æ—Ç–æ–º —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ. </p><br><p> Asynchronous Advantage Actor Critic (A3C) –æ–∑–Ω–∞—á–∞–µ—Ç –≤—Å–µ–≥–æ –ª–∏—à—å, —á—Ç–æ –µ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä, —Å–æ–±–∏—Ä–∞—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–∞ actor. –ò –æ–±–Ω–æ–≤–ª—è—é—â–∏–π –≤–µ—Å–∞ –∫–∞–∫ —Ç–æ–ª—å–∫–æ –Ω–∞–±–∏—Ä–∞–µ—Ç—Å—è –±–∞—Ç—á batch –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ü–æ—ç—Ç–æ–º—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, —á—Ç–æ –Ω–µ –∂–¥–µ—Ç –∫–∞–∂–¥–æ–≥–æ actor. –≠—Ç–æ –≤—Ä–æ–¥–µ –∫–∞–∫ —Ä–∞–∑–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä—ã, —É–±–∏—Ä–∞—è –∏–∑ –Ω–∏—Ö –Ω–µ–Ω—É–∂–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ. –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –ø–æ—Ç–æ–º –ø–æ—è–≤–∏–ª—Å—è A2C ‚Äî —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è A3C, –≤ –∫–æ—Ç–æ—Ä–æ–π —Å–µ—Ä–≤–µ—Ä –¥–æ–∂–∏–¥–∞–µ—Ç—Å—è –æ–∫–æ–Ω—á–∞–Ω–∏—è —ç–ø–∏–∑–æ–¥–æ–≤ —É –≤—Å–µ—Ö actor –∏ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ (–ø–æ—ç—Ç–æ–º—É —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π). A2C —Ç–æ–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø–æ—ç—Ç–æ–º—É –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –æ–±–µ –≤–µ—Ä—Å–∏–∏, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∫—É—Å–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞. </p><br><h1 id="trpo-ppo-sac"> TRPO, PPO, SAC </h1><br><p> –°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ, –Ω–∞ —ç—Ç–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–∫–æ–Ω—á–∏–ª—Å—è. </p><br><p> –ù–µ —Å–º–æ—Ç—Ä—è –Ω–∞ –∫—Ä–∞—Å–∏–≤–æ–µ –∏ –≤—ã–≥–ª—è–¥—è—â–µ–µ –ª–æ–≥–∏—á–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ, –≤—Å–µ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ –æ—á–µ–Ω—å. –î–∞–∂–µ –ª—É—á—à–∏–µ Reinforcement Learning –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ç—Ä–µ–±—É—é—Ç –¥–µ—Å—è—Ç–∫–∏ –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤, —Å—Ä–∞–≤–Ω–∏–º—ã –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º –ø–æ–∏—Å–∫–æ–º, –∞ —Å–∞–º–æ–µ –ø–µ—á–∞–ª—å–Ω–æ–µ, —á—Ç–æ –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å –∏—Ö –ø–æ–º–æ—â—å—é —Å–æ–∑–¥–∞—Ç—å —Å–∏–ª—å–Ω—ã–π –ò–ò ‚Äî —Ä–∞–±–æ—Ç–∞—é—Ç –ª–∏—à—å –Ω–∞ –∫—Ä–∞–π–Ω–µ –Ω–∏–∑–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö, –∏—Å—á–∏—Å–ª—è–µ–º—ã—Ö –µ–¥–∏–Ω–∏—Ü–∞–º–∏. –î–∞–∂–µ –Ω–µ –¥–µ—Å—è—Ç–∫–∞–º–∏. </p><br><p> –î–∞–ª—å–Ω–µ–π—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ ‚Äî TRPO –∏ PPO, —è–≤–ª—è—é—â–∏–µ—Å—è —Å–µ–π—á–∞—Å state-of-the-art, —è–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç—å—é Actor-Critic. –ù–∞ PPO –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–∞—é—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –º–∏—Ä–µ RL. –ö –ø—Ä–∏–º–µ—Ä—É, –∏–º –æ–±—É—á–∞–ª–∏ <a href="https://openai.com/five/">OpenAI Five</a> –¥–ª—è –∏–≥—Ä—ã –≤ Dota 2. </p><br><p> –í—ã –±—É–¥–µ—Ç–µ —Å–º–µ—è—Ç—å—Å—è, –Ω–æ –≤—Å–µ —á—Ç–æ –ø—Ä–∏–¥—É–º–∞–ª–∏ –≤ –º–µ—Ç–æ–¥–∞—Ö TRPO –∏ PPO ‚Äî —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏, —á—Ç–æ–±—ã –≤–µ—Å–∞ —Ä–µ–∑–∫–æ –Ω–µ –º–µ–Ω—è–ª–∏—Å—å. –î–µ–ª–æ –≤ —Ç–æ–º, —á—Ç–æ –≤ A3C/A2C –±—ã–≤–∞—é—Ç —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ä—Ç—è—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–π –æ–ø—ã—Ç. –ï—Å–ª–∏ —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ–±—ã –Ω–æ–≤–∞—è policy –Ω–µ —Å–ª–∏—à–∫–æ–º –æ—Ç–ª–∏—á–∞–ª–∞—Å—å –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π, —Ç–æ –º–æ–∂–Ω–æ –∏–∑–±–µ–∂–∞—Ç—å —Ç–∞–∫–∏—Ö –≤—ã–±—Ä–æ—Å–æ–≤. –ß—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ gradient clipping –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç—è—Ö –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —Ç–æ–ª—å–∫–æ –Ω–∞ –¥—Ä—É–≥–æ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –∞–ø–ø–∞—Ä–∞—Ç–µ. –°–∞–º —Ñ–∞–∫—Ç —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è —Ç–∞–∫ –≥—Ä—É–±–æ –æ–±—Ä–µ–∑–∞—Ç—å –∏ —É—Ö—É–¥—à–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ (–±–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–∞–º –≤–µ–¥—å –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–∞–∫ –ø–æ—è–≤–∏–ª–∏—Å—å, –æ–Ω–∏ –Ω—É–∂–Ω—ã –¥–ª—è –≤—ã–∑–≤–∞–≤—à–µ–≥–æ –∏—Ö –ø—Ä–∏–º–µ—Ä–∞), –∏ —á—Ç–æ —ç—Ç–æ –¥–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç, –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –º—ã —Å–≤–µ—Ä–Ω—É–ª–∏ –∫—É–¥–∞-—Ç–æ –Ω–µ —Ç—É–¥–∞. </p><br><p> –í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–π –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º Soft-Actor-Critic (SAC). –û–Ω –Ω–µ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç PPO, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞ —Ü–µ–ª—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ø–æ–≤—ã—à–∞—Ç—å —ç–Ω—Ç—Ä–æ–ø–∏—é –≤ policy. –î–µ–ª–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω—ã–º. –ù–µ—Ç, –Ω–µ —Ç–∞–∫. –ß—Ç–æ–±—ã –∞–≥–µ–Ω—Ç –±—ã–ª —Å–ø–æ—Å–æ–±–µ–Ω –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö. –≠—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–≤—ã—à–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏, —Ä–∞–∑ –∞–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –ª—é–±—ã–º —Å–ª—É—á–∞–π–Ω—ã–º —Å–∏—Ç—É–∞—Ü–∏—è–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, SAC —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —á–µ–º PPO, –∏ –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ —Ç–æ–∂–µ –ø–ª—é—Å. –û–¥–Ω–∞–∫–æ –¥–∞–∂–µ —Å SAC, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –±–µ–≥–∞—Ç—å –≥—É–º–∞–Ω–æ–∏–¥–∞ —Å 17 —Å—Ç–µ–ø–µ–Ω—è–º–∏ —Å–≤–æ–±–æ–¥—ã, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ–∑–∏—Ü–∏–∏ —Å—Ç–æ—è, –Ω—É–∂–Ω–æ –æ–∫–æ–ª–æ 20 –º–ª–Ω –∫–∞–¥—Ä–æ–≤ –∏ –ø—Ä–∏–º–µ—Ä–Ω–æ —Å—É—Ç–∫–∏ —Ä–∞—Å—á–µ—Ç–∞ –Ω–∞ –æ–¥–Ω–æ–º GPU. –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –Ω–∞—á–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è, —Å–∫–∞–∂–µ–º, –Ω–∞—É—á–∏—Ç—å –≤—Å—Ç–∞–≤–∞—Ç—å –≥—É–º–∞–Ω–æ–∏–¥–∞ –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –ø–æ–∑—ã, –º–æ–∂–µ—Ç –≤–æ–æ–±—â–µ –Ω–µ –æ–±—É—á–∏—Ç—å—Å—è. </p><br><p> –ò—Ç–æ–≥–æ, –æ–±—â–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º Reinforcement Learning: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SAC, PPO, DDPG, DQN (–≤ —Ç–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ, –ø–æ —É–±—ã–≤–∞–Ω–∏—é). </p><br><h1 id="model-based"> Model-Based </h1><br><p> –°—É—â–µ—Å—Ç–≤—É–µ—Ç –µ—â–µ –æ–¥–∏–Ω –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Å–≤–µ–Ω–Ω–æ –∫–∞—Å–∞—é—â–∏–π—Å—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã, –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç –µ—Å–ª–∏ –º—ã –ø—Ä–µ–¥–ø—Ä–∏–º–µ–º –∫–∞–∫–∏–µ-—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è. </p><br><p> –ï–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–º —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, —á—Ç–æ –æ–Ω –Ω–∏–∫–∞–∫ –Ω–µ –≥–æ–≤–æ—Ä–∏—Ç, –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—å. –õ–∏—à—å –æ–± –∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ. –ù–æ –∑–∞—Ç–æ —Ç–∞–∫—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ª–µ–≥–∫–æ –æ–±—É—á–∞—Ç—å ‚Äî –ø—Ä–æ—Å—Ç–æ –æ–±—É—á–∞–µ–º –Ω–∞ –ª—é–±–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ. –ü–æ–ª—É—á–∞–µ—Ç—Å—è —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞ –º–∏—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. </p><br><p> –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–≥—Ä–æ–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, –∏ –∫–∞–∂–¥–æ–µ –ø—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ —ç—Ç–æ—Ç —Å–∏–º—É–ª—è—Ç–æ—Ä (—á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å). –ò —Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–æ–µ –∏–∑ –Ω–∏—Ö –ø—Ä–∏–Ω–µ—Å–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É. –ï—Å—Ç—å –Ω–µ–±–æ–ª—å—à–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ –ø—Ä–æ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, –∞ –æ—Ç–∫–ª–æ–Ω—è—é—â–∏–µ—Å—è –ø–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º –∑–∞–∫–æ–Ω—É –æ—Ç —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –ò –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –º—ã –ø–æ–¥–Ω–∏–º–∞–µ–º —Ä—É–∫—É, —Ç–æ —Å –±–æ–ª—å—à–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –Ω—É–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –µ–µ –ø–æ–¥–Ω–∏–º–∞—Ç—å. –ü–æ—ç—Ç–æ–º—É –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. </p><br><p> –ó–¥–µ—Å—å —Ñ–æ–∫—É—Å —Å —Ç–æ–º, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–∏–º–∏—Ç–∏–≤–Ω—ã–π —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π —Å–∏–º—É–ª—è—Ç–æ—Ä –≤—Ä–æ–¥–µ MuJoCo –∏–ª–∏ pyBullet –≤—ã–¥–∞–µ—Ç –æ–∫–æ–ª–æ 200 FPS. –ê –µ—Å–ª–∏ –æ–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –≤–ø–µ—Ä–µ–¥ —Ö–æ—Ç—è –±—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤, —Ç–æ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π –ª–µ–≥–∫–æ –º–æ–∂–Ω–æ –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑ –ø–æ–ª—É—á–∞—Ç—å –±–∞—Ç—á–∏ –ø–æ 2000-5000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π. –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ—â–Ω–æ—Å—Ç–∏ GPU, –≤ —Å–µ–∫—É–Ω–¥—É –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –¥–µ—Å—è—Ç–∫–æ–≤ —Ç—ã—Å—è—á —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏ –≤ GPU –∏ —Å–∂–∞—Ç–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –ù–µ–π—Ä–æ—Å–µ—Ç—å –∑–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–æ–ª—å –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ–≥–æ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏. </p><br><p> –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Ä–∞–∑ —É–∂ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –º–æ–∂–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –º–∏—Ä (—ç—Ç–æ –∏ –µ—Å—Ç—å model-based –ø–æ–¥—Ö–æ–¥, –≤ –æ–±—â–µ–º —Å–º—ã—Å–ª–µ), —Ç–æ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º –≤ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–∏, —Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å. –≠—Ç–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –≤ Reinforcement Learning –ø–æ–ª—É—á–∏–ª–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ Dream Worlds, –∏–ª–∏ World Models. –≠—Ç–æ –Ω–µ–ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, —Ö–æ—Ä–æ—à–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –µ—Å—Ç—å —Ç—É—Ç: <a href="https://worldmodels.github.io/">https://worldmodels.github.io</a> . –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ç–æ –∏–º–µ–µ—Ç –ø—Ä–∏—Ä–æ–¥–Ω—ã–π –∞–Ω–∞–ª–æ–≥ ‚Äî –æ–±—ã—á–Ω—ã–µ —Å–Ω—ã. –ò –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –Ω–µ–¥–∞–≤–Ω–∏—Ö –∏–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ–º—ã—Ö —Å–æ–±—ã—Ç–∏–π –≤ –≥–æ–ª–æ–≤–µ. </p><br><h1 id="imitation-learning"> Imitation Learning </h1><br><p> –û—Ç –±–µ—Å—Å–∏–ª–∏—è, —á—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º—ã Reinforcement Learning –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö –∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –Ω–∞—Ä–æ–¥ –∑–∞–¥–∞–ª—Å—è —Ü–µ–ª—å—é —Ö–æ—Ç—è –±—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –∑–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –≤ –≤–∏–¥–µ –ª—é–¥–µ–π. –ó–¥–µ—Å—å —É–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–∏—á—å –Ω–µ–ø–ª–æ—Ö–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–Ω–µ–¥–æ—Å—Ç–∏–∂–∏–º—ã—Ö –æ–±—ã—á–Ω—ã–º Reinforcement Learning). –¢–∞–∫, OpenAI –ø–æ–ª—É—á–∏–ª–æ—Å—å <a href="https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration">–ø—Ä–æ–π—Ç–∏ –∏–≥—Ä—É Montezuma's Revenge</a> . –§–æ–∫—É—Å –æ–∫–∞–∑–∞–ª—Å—è –ø—Ä–æ—Å—Ç ‚Äî –ø–æ–º–µ—â–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ —Å—Ä–∞–∑—É –≤ –∫–æ–Ω–µ—Ü –∏–≥—Ä—ã (–≤ –∫–æ–Ω–µ—Ü –ø–æ–∫–∞–∑–∞–Ω–Ω–æ–π —á–µ–ª–æ–≤–µ–∫–æ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏). –¢–∞–º —Å –ø–æ–º–æ—â—å—é PPO, –±–ª–∞–≥–æ–¥–∞—Ä—è –±–ª–∏–∑–æ—Å—Ç–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã, –∞–≥–µ–Ω—Ç –±—ã—Å—Ç—Ä–æ —É—á–∏—Ç—Å—è –∏–¥—Ç–∏ –≤–¥–æ–ª—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –ø–æ–º–µ—â–∞–µ–º –µ–≥–æ –Ω–µ–º–Ω–æ–≥–æ –Ω–∞–∑–∞–¥, –≥–¥–µ –æ–Ω –±—ã—Å—Ç—Ä–æ —É—á–∏—Ç—Å—è –¥–æ—Ö–æ–¥–∏—Ç—å –¥–æ —Ç–æ–≥–æ –º–µ—Å—Ç–∞, –∫–æ—Ç–æ—Ä–æ–µ –æ–Ω —É–∂–µ –∏–∑—É—á–∏–ª. –ò —Ç–∞–∫ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–¥–≤–∏–≥–∞—è —Ç–æ—á–∫—É "—Ä–µ—Å–ø–∞–≤–Ω–∞" –≤–¥–æ–ª—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–æ —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞ –∏–≥—Ä—ã, –∞–≥–µ–Ω—Ç —É—á–∏—Ç—Å—è –ø—Ä–æ—Ö–æ–¥–∏—Ç—å/–∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é —ç–∫—Å–ø–µ—Ä—Ç–∞ –≤ —Ç–µ—á–µ–Ω–∏–∏ –≤—Å–µ–π –∏–≥—Ä—ã. </p><br><p> –î—Ä—É–≥–æ–π –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–π –∑–∞ –ª—é–¥—å–º–∏, —Å–Ω—è—Ç—ã–µ –Ω–∞ Motion Capture: <a href="https://bair.berkeley.edu/blog/2018/04/10/virtual-stuntman/">DeepMimic</a> . –†–µ—Ü–µ–ø—Ç –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω –º–µ—Ç–æ–¥—É OpenAI: –∫–∞–∂–¥—ã–π —ç–ø–∏–∑–æ–¥ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–µ —Å –Ω–∞—á–∞–ª–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –∞ —Å–æ —Å–ª—É—á–∞–π–Ω–æ–π —Ç–æ—á–∫–∏ –≤–¥–æ–ª—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –¢–æ–≥–¥–∞ PPO —É—Å–ø–µ—à–Ω–æ –∏–∑—É—á–∞–µ—Ç –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–æ–π —Ç–æ—á–∫–∏. </p><br><p> –ù–∞–¥–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ –Ω–∞—à—É–º–µ–≤—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º <a href="https://eng.uber.com/go-explore/">Go-Explore</a> –æ—Ç Uber, –ø—Ä–æ—à–µ–¥—à–∏–π —Å —Ä–µ–∫–æ—Ä–¥–Ω—ã–º–∏ –æ—á–∫–∞–º–∏ –∏–≥—Ä—É Montezuma's Revenge, –≤–æ–æ–±—â–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º Reinforcement Learning. –≠—Ç–æ –æ–±—ã—á–Ω—ã–π —Å–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫, –Ω–æ –Ω–∞—á–∏–Ω–∞—è —Å–æ —Å–ª—É—á–∞–π–Ω–æ–π –ø–æ—Å–µ—â–µ–Ω–Ω–æ–π —Ä–∞–Ω–µ–µ —è—á–µ–π–∫–∏ cell (–æ–≥—Ä—É–±–ª–µ–Ω–Ω–æ–π —è—á–µ–π–∫–∏, –≤ –∫–æ—Ç–æ—Ä—É—é –ø–æ–ø–∞–¥–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ state). –ò —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ —Ç–∞–∫–∏–º —Å–ª—É—á–∞–π–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –±—É–¥–µ—Ç –Ω–∞–π–¥–µ–Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –¥–æ –∫–æ–Ω—Ü–∞ –∏–≥—Ä—ã, —É–∂–µ –ø–æ –Ω–µ–π —Å –ø–æ–º–æ—â—å—é Imitation Learning –æ–±—É—á–∞–µ—Ç—Å—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å. –°–ø–æ—Å–æ–±–æ–º, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º –∫–∞–∫ –≤ OpenAI, —Ç.–µ. –Ω–∞—á–∏–Ω–∞—è —Å –∫–æ–Ω—Ü–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. </p><br><h1 id="curiosity-lyubopytstvo"> Curiosity (–ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ) </h1><br><p> –û—á–µ–Ω—å –≤–∞–∂–Ω—ã–º –ø–æ–Ω—è—Ç–∏–µ–º –≤ Reinforcement Learning —è–≤–ª—è–µ—Ç—Å—è –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ (Curiosity). –í –ø—Ä–∏—Ä–æ–¥–µ –æ–Ω–æ —è–≤–ª—è–µ—Ç—Å—è –¥–≤–∏–≥–∞—Ç–µ–ª–µ–º –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. </p><br><p> –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ü–µ–Ω–∫–∏ –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞ –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –æ—à–∏–±–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–µ—Ç–∏, —á—Ç–æ –±—É–¥–µ—Ç –¥–∞–ª—å—à–µ. –ò–Ω–∞—á–µ —Ç–∞–∫–∞—è —Å–µ—Ç—å –∑–∞–≤–∏—Å–Ω–µ—Ç –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –∂–µ –¥–µ—Ä–µ–≤–æ–º —Å –∫–∞—á–∞—é—â–µ–π—Å—è –ª–∏—Å—Ç–≤–æ–π. –ò–ª–∏ –ø–µ—Ä–µ–¥ —Ç–µ–ª–µ–≤–∏–∑–æ—Ä–æ–º —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –∫–∞–Ω–∞–ª–æ–≤. –¢–∞–∫ –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑-–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –±—É–¥–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∏ –æ—à–∏–±–∫–∞ –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç –±–æ–ª—å—à–æ–π. –í–ø—Ä–æ—á–µ–º, –∏–º–µ–Ω–Ω–æ —ç—Ç–æ –∏ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏—á–∏–Ω–æ–π, –ø–æ—á–µ–º—É –º—ã (–ª—é–¥–∏) —Ç–∞–∫ –ª—é–±–∏–º —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ª–∏—Å—Ç–≤—É, –≤–æ–¥—É –∏ –æ–≥–æ–Ω—å. –ò –Ω–∞ —Ç–æ, –∫–∞–∫ –¥—Ä—É–≥–∏–µ –ª—é–¥–∏ —Ä–∞–±–æ—Ç–∞—é—Ç =). –ù–æ —É –Ω–∞—Å –µ—Å—Ç—å –∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–Ω—É—Ç—å –Ω–∞–≤–µ—á–Ω–æ. </p><br><p> –û–¥–∏–Ω –∏–∑ —Ç–∞–∫–∏—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø—Ä–∏–¥—É–º–∞–ª–∏ –∫–∞–∫ Inverse Model –≤ —Ä–∞–±–æ—Ç–µ <a href="https://pathak22.github.io/noreward-rl/">Curiosity-driven Exploration by <br> Self-supervised Prediction</a> . –ï—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ, –∞–≥–µ–Ω—Ç (–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å) –∫—Ä–æ–º–µ —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–æ–≤–µ—Ä—à–∏—Ç—å –≤ –¥–∞–Ω–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å –º–∏—Ä–æ–º –ø–æ—Å–ª–µ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π. –ò –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç–æ —Å–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∏—Ä–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞, —á—Ç–æ–±—ã –ø–æ –Ω–µ–º—É –∏ –ø–æ —Ç–µ–∫—É—â–µ–º—É —à–∞–≥—É –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–≤–æ–∏ –∂–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—ã–µ —Ä–∞–Ω–µ–µ –¥–µ–π—Å—Ç–≤–∏—è (–¥–∞, —Å–ª–æ–∂–Ω–æ, –±–µ–∑ –ø–æ–ª–ª–∏—Ç—Ä–∞ –Ω–µ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è). </p><br><p> –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª—é–±–æ–ø—ã—Ç–Ω–æ–º—É —ç—Ñ—Ñ–µ–∫—Ç—É: –∞–≥–µ–Ω—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª—é–±–æ–ø—ã—Ç–Ω—ã–º —Ç–æ–ª—å–∫–æ –∫ —Ç–æ–º—É, –Ω–∞ —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å —Å–≤–æ–∏–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏. –ù–∞ –∫–∞—á–∞—é—â–∏–µ—Å—è –≤–µ—Ç–∫–∏ –¥–µ—Ä–µ–≤–∞ –æ–Ω –Ω–∏–∫–∞–∫ –Ω–µ –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –µ–º—É –Ω–µ–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã. –ê –≤–æ—Ç –ø–æ—Ö–æ–¥–∏—Ç—å –ø–æ –æ–∫—Ä—É–≥–µ –æ–Ω –º–æ–∂–µ—Ç, –ø–æ—ç—Ç–æ–º—É –µ–º—É –ª—é–±–æ–ø—ã—Ç–Ω–æ —Ö–æ–¥–∏—Ç—å –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–∏—Ä. </p><br><p> –û–¥–Ω–∞–∫–æ –µ—Å–ª–∏ —É –∞–≥–µ–Ω—Ç–∞ –±—É–¥–µ—Ç –ø—É–ª—å—Ç –æ—Ç —Ç–µ–ª–µ–≤–∏–∑–æ—Ä–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—â–∏–π —Å–ª—É—á–∞–π–Ω—ã–µ –∫–∞–Ω–∞–ª—ã, —Ç–æ –æ–Ω –º–æ–∂–µ—Ç –Ω–∞ –Ω–µ–≥–æ –ø–æ–≤–ª–∏—è—Ç—å! –ò –µ–º—É –±—É–¥–µ—Ç –ª—é–±–æ–ø—ã—Ç–Ω–æ —â–µ–ª–∫–∞—Ç—å –∫–∞–Ω–∞–ª—ã –¥–æ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ (—Ç–∞–∫ –∫–∞–∫ –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –∫–∞–∫–æ–π –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π –∫–∞–Ω–∞–ª, —Ç.–∫. –æ–Ω —Å–ª—É—á–∞–π–Ω—ã–π). –ü–æ–ø—ã—Ç–∫–∞ –æ–±–æ–π—Ç–∏ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç–∞ –≤ Google –≤ —Ä–∞–±–æ—Ç–µ <a href="https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html">Episodic Curiosity through Reachability</a> . </p><br><p> –ù–æ, –ø–æ–∂–∞–ª—É–π, –ª—É—á—à–∏–π state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤—É, –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç OpenAI —Å –∏–¥–µ–µ–π <a href="https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/">Random Network Distillation (RND)</a> . –ï–µ —Å—É—Ç—å –≤ —Ç–æ–º, —á—Ç–æ –±–µ—Ä–µ—Ç—Å—è –≤—Ç–æ—Ä–∞—è, —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ —Å–ª—É—á–∞–π–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Ç—å, –∏ –µ–π –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–µ—Ç—Å—è —Ç–µ–∫—É—â–∏–π state. –ê –Ω–∞—à–∞ –æ—Å–Ω–æ–≤–Ω–∞—è —Ä–∞–±–æ—á–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø—ã—Ç–∞–µ—Ç—Å—è —É–≥–∞–¥–∞—Ç—å –≤—ã—Ö–æ–¥ —ç—Ç–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –í—Ç–æ—Ä–∞—è —Å–µ—Ç—å –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è, –æ–Ω–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –≤—Å–µ –≤—Ä–µ–º—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–∞–∫ –±—ã–ª–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. </p><br><p> –í —á–µ–º —Å–º—ã—Å–ª? –°–º—ã—Å–ª –≤ —Ç–æ–º, —á—Ç–æ –µ—Å–ª–∏ –∫–∞–∫–æ–π-–ª–∏–±–æ state —É–∂–µ –±—ã–ª –ø–æ—Å–µ—â–µ–Ω –∏ –∏–∑—É—á–µ–Ω –Ω–∞—à–µ–π —Ä–∞–±–æ—á–µ–π —Å–µ—Ç—å—é, —Ç–æ –æ–Ω–∞ –±–æ–ª–µ–µ –º–µ–Ω–µ–µ —É—Å–ø–µ—à–Ω–æ —Å–º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –≤—ã—Ö–æ–¥ —Ç–æ–π –≤—Ç–æ—Ä–æ–π —Å–µ—Ç–∏. –ê –µ—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π state, –≥–¥–µ –º—ã –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª–∏, —Ç–æ –Ω–∞—à–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ —Å–º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≤—ã—Ö–æ–¥ —Ç–æ–π RND —Å–µ—Ç–∏. –≠—Ç–∞ –æ—à–∏–±–∫–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –≤—ã—Ö–æ–¥–∞ —Ç–æ–π —Å–ª—É—á–∞–π–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞ (–¥–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ –Ω–∞–≥—Ä–∞–¥—ã, –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –µ–µ –≤—ã—Ö–æ–¥). </p><br><p> –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–µ —Å–æ–≤—Å–µ–º –ø–æ–Ω—è—Ç–Ω–æ. –ù–æ –ø–∏—à—É—Ç, —á—Ç–æ —ç—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–≥–¥–∞ —Ü–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è –∏ –∫–æ–≥–¥–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —Å–∞–º–æ–º—É —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á—Ç–æ –±—É–¥–µ—Ç –¥–∞–ª—å—à–µ (—á—Ç–æ –¥–∞–µ—Ç –±–æ–ª—å—à—É—é –æ—à–∏–±–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –æ–±—ã—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞). –¢–∞–∫ –∏–ª–∏ –∏–Ω–∞—á–µ, –Ω–æ RND —Ä–µ–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–ª –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞ –≤ –∏–≥—Ä–∞—Ö. –ò —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –ø—Ä–æ–±–ª–µ–º–æ–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ç–µ–ª–µ–≤–∏–∑–æ—Ä–∞. </p><br><p> –° –ø–æ–º–æ—â—å—é RND –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞ –≤ OpenAI –≤–ø–µ—Ä–≤—ã–µ —á–µ—Å—Ç–Ω–æ (–∞ –Ω–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫, –∫–∞–∫ –≤ Uber) –ø—Ä–æ—à–ª–∏ –ø–µ—Ä–≤—ã–π —É—Ä–æ–≤–µ–Ω—å Montezuma's Revenge. –ù–µ –∫–∞–∂–¥—ã–π —Ä–∞–∑ –∏ –Ω–µ–Ω–∞–¥–µ–∂–Ω–æ, –Ω–æ –≤—Ä–µ–º—è –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ –ø–æ–ª—É—á–∞–µ—Ç—Å—è. </p><br><p><img src="https://habrastorage.org/webt/iw/jm/kb/iwjmkbze4r8efybc5-01pbqzjf8.png"></p><br><h1 id="chto-v-itoge">  What is the result? </h1><br><p> –ö–∞–∫ –≤–∏–¥–∏—Ç–µ, –≤—Å–µ–≥–æ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç Reinforcement Learning –ø—Ä–æ—à–µ–ª –¥–æ–ª–≥–∏–π –ø—É—Ç—å. –ù–µ –ø—Ä–æ—Å—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π, –∫–∞–∫ –≤ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç—è—Ö, –≥–¥–µ resudal –∏ skip connections –ø–æ–∑–≤–æ–ª–∏–ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ç–∏ –≥–ª—É–±–∏–Ω–æ–π –≤ —Å–æ—Ç–Ω–∏ —Å–ª–æ–µ–≤, –≤–º–µ—Å—Ç–æ –¥–µ—Å—è—Ç–∫–∞ —Å–ª–æ–µ–≤ —Å –æ–¥–Ω–æ–π —Ç–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Relu, –ø–æ–±–æ—Ä–æ–≤—à–µ–π –ø—Ä–æ–±–ª–µ–º—É –∏—Å—á–µ–∑–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤ —Å–∏–≥–º–æ–∏–¥–µ –∏ tanh. –í –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø—Ä–æ–∏–∑–æ—à–µ–ª –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è—Ö –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–∏—á–∏–Ω, –ø–æ—á–µ–º—É –Ω–µ –∑–∞—Ä–∞–±–æ—Ç–∞–ª —Ç–æ—Ç –∏–ª–∏ –∏–Ω–æ–π –Ω–∞–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ "–Ω–µ –∑–∞—Ä–∞–±–æ—Ç–∞–ª". </p><br><p> –ù–æ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –≤—Å–µ –ø–æ –ø—Ä–µ–∂–Ω–µ–º—É —É–ø–∏—Ä–∞–µ—Ç—Å—è –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ —Ç–µ—Ö –∂–µ Q, V –∏–ª–∏ A –∑–Ω–∞—á–µ–Ω–∏–π. –ù–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö, –∫–∞–∫ –≤ –º–æ–∑–≥–µ (Hierarchical Reinforcement Learning –Ω–µ –≤ —Å—á–µ—Ç, —É–∂ –±–æ–ª—å–Ω–æ –ø—Ä–∏–º–∏—Ç–∏–≤–Ω–∞—è –≤ –Ω–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é –≤ –∂–∏–≤–æ–º –º–æ–∑–≥–µ). –ù–∏ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏–¥—É–º–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–µ—Ç–∏, –∑–∞—Ç–æ—á–µ–Ω–Ω—É—é –∏–º–µ–Ω–Ω–æ –ø–æ–¥ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–∞–∫ —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ —Å LSTM –∏ –¥—Ä—É–≥–∏–º–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. Reinforcement Learning –ª–∏–±–æ —Ç–æ–ø—á–µ—Ç—Å—è –Ω–∞ –º–µ—Å—Ç–µ, —Ä–∞–¥—É—è—Å—å –Ω–µ–±–æ–ª—å—à–∏–º —É—Å–ø–µ—Ö–∞–º, –ª–∏–±–æ –¥–≤–∏–∂–µ—Ç—Å—è –≤ –∫–∞–∫–æ–º-—Ç–æ —Å–æ–≤—Å–µ–º —É–∂ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏. </p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I want to believe that one day in the training with reinforcement there will be a breakthrough in the architecture of neural networks, similar to what happened in convolutional networks. </font><font style="vertical-align: inherit;">And we will see truly working reinforcement training. </font><font style="vertical-align: inherit;">Studying on single examples, with a working associative memory and working on different time scales.</font></font></p></div><p>Source: <a href="https://habr.com/ru/post/437020/">https://habr.com/ru/post/437020/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>