<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We launch our neural network detector on the Raspberry Pi using the Neural Compute Stick and OpenVINO</title>
  <meta name="description" content="With the spread and development of neural networks, there is an increasing need to use them on embedded and low-power devices, robots and drones. The ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>We launch our neural network detector on the Raspberry Pi using the Neural Compute Stick and OpenVINO</h1><div class="post__text post__text-html js-mediator-article">  With the spread and development of neural networks, there is an increasing need to use them on embedded and low-power devices, robots and drones.  The Neural Compute Stick device in conjunction with the OpenVINO framework from Intel allows you to solve this problem by taking the heavy calculations of the neural networks.  Thanks to this, you can effortlessly launch a neural network classifier or detector on a low-power device like the Raspberry Pi in almost real time, while not greatly increasing power consumption.  In this post, I‚Äôll explain how to use the OpenVINO framework (in C ++) and the Neural Compute Stick to run a simple face detection system on the Raspberry Pi. <br><br>  As usual, all the code is available on <a href="https://github.com/BeloborodovDS/NCS-face">GitHub</a> . <br><br><img src="https://habrastorage.org/webt/qu/b_/tj/qub_tj1u6ztw9irfy9ivtaaidcc.jpeg"><br><a name="habracut"></a><br><h3>  A bit about Neural Compute Stick and OpenVINO </h3><br>  In the summer of 2017, Intel released the <a href="https://software.intel.com/ru-ru/movidius-ncs">Neural Compute Stick</a> (NCS) device designed to run neural networks on low-power devices, and after a couple of months it could be purchased and tested, which I did.  NCS is a small computing module with an azure-colored body (also playing the role of a radiator), connected to the main device via USB.  Inside, among other things, is the Intel Myriad <abbr title="Vision Processing Unit">VPU</abbr> , which is essentially a 12-core parallel processor, sharpened by operations that often occur in neural networks.  NCS is not suitable for teaching neural networks, but the inference in already trained neural networks is comparable in speed to that on the GPU.  All calculations in NCS are carried out over 16-bit float numbers, which allows to increase the speed.  NCS requires only 1 watt of power to operate, that is, at 5 V on the USB connector, up to 200 mA is consumed - this is even less than the camera for the Raspberry Pi (250 mA). <br><br><img src="https://habrastorage.org/webt/8d/u8/ov/8du8ov7hj1-f3vk8sjbenkyupvm.png"><br><br>  <a href="https://github.com/movidius/ncsdk">Neural Compute SDK</a> (NCSDK) was used to work with the first NCS: it includes tools for compiling neural networks in <a href="http://caffe.berkeleyvision.org/">Caffe</a> and <a href="https://www.tensorflow.org/">TensorFlow formats</a> into NCS format, tools for measuring their performance, as well as Python and C ++ API for interference. <br><br>  Then a new version of the framework for working with NCS was released: <a href="https://github.com/movidius/ncsdk/tree/ncsdk2">NCSDK2</a> .  The API has changed quite a bit, and although some changes seemed strange to me, there were some useful innovations.  In particular, an automatic conversion was added from float 32 bit to float 16 bit to C ++ (previously, for this, you had to insert crutches in the form of code from Numpy).  Also appeared the queue of images and the results of their processing. <br><br>  In May 2018, Intel released <a href="https://software.intel.com/en-us/openvino-toolkit">OpenVINO</a> (previously called the Intel Computer Vision SDK).  This framework is designed to effectively run neural networks on various devices: processors and graphics cards from Intel, <abbr title="Field-Programmable Gate Array">FPGA</abbr> , and the Neural Compute Stick. <br><br>  In November 2018, a new version of the accelerator was released: the <a href="https://software.intel.com/en-us/neural-compute-stick">Neural Compute Stick 2</a> .  The computational power of the device was increased: the description on the website promises an acceleration of up to 8x, but I did not have time to test the new version of the device.  Acceleration is achieved by increasing the number of cores from 12 to 16, as well as adding new computing devices optimized for neural networks.  True, I did not find information about the power consumption. <br><br>  The second version of NCS is already incompatible with NCSDK or NCSDK2: their credentials passed to OpenVINO, which is capable of working with many other devices besides both versions of NCS.  OpenVINO itself has great functionality and includes the following components: <br><br><ol><li>  Model Optimizer: Python script that allows you to convert neural networks from popular frameworks for deep learning into the universal OpenVINO format.  The list of supported frameworks: <a href="http://caffe.berkeleyvision.org/">Caffe</a> , <a href="https://www.tensorflow.org/">TensorFlow</a> , <a href="https://mxnet.apache.org/">MXNET</a> , <a href="https://github.com/kaldi-asr/kaldi">Kaldi</a> (speech recognition framework), <a href="https://onnx.ai/">ONNX</a> (open neural network presentation format). </li><li>  Inference Engine: C ++ and Python API for the inference of neural networks, abstracted from a specific device inference.  The API code will look almost identical for CPU, GPU, FPGA and NCS. </li><li>  A set of plug-ins for different devices.  Plugins are dynamic libraries that are loaded explicitly in the code of the main program.  We are most interested in the plugin for NCS. </li><li>  A set of pre-trained models in the universal format OpenVINO (full list <a href="https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models">here</a> ).  An impressive collection of high-quality neural networks: detectors of persons, pedestrians, objects;  recognition of the orientation of faces, particular points of faces, posture of a person;  super resolution;  other.  It is worth noting that not all of them are supported by NCS / FPGA / GPU. </li><li>  Model Downloader: another script that simplifies downloading models in the OpenVINO format over the network (although you can easily do without it). </li><li>  Library of computer vision <a href="https://opencv.org/">OpenCV</a> , optimized for hardware Intel. </li><li>  Library of computer vision <a href="https://www.khronos.org/openvx/">OpenVX</a> . </li><li>  Intel <a href="https://github.com/intel/clDNN">Compute Library for Deep Neural Networks</a> . </li><li>  Intel <a href="https://github.com/intel/mkl-dnn">Math Kernel Library for Deep Neural Networks</a> . </li><li>  A tool for optimizing neural networks for FPGA (optional). </li><li>  Documentation and sample programs. </li></ol><br>  In my previous articles, I talked about how to run the YOLO face detector on NCS <a href="https://habr.com/ru/post/347438/">(first article)</a> , and also how to train your SSD face detector and run it on Raspberry Pi and NCS <a href="https://habr.com/ru/post/424973/">(second article)</a> .  In these articles, I used NCSDK and NCSDK2.  In this article, I will tell you how to do something similar, but using OpenVINO, I will make a small comparison of both different face detectors and the two frameworks for their launch, and point out some of the pitfalls.  I am writing in C ++, because I believe that this way you can achieve better performance, which will be important in the case of Raspberry Pi. <br><br><h3>  Install OpenVINO </h3><br>  Not the most difficult task, although there are subtleties.  OpenVINO at the time of this writing only supports Ubuntu 16.04 LTS, CentOS 7.4 and Windows 10. I have Ubuntu 18, and for installation it needs <a href="https://github.com/nikhilraghava/OpenVINO-18.04-Support">small crutches</a> .  I also wanted to compare OpenVINO with NCSDK2, with the installation of which there are also problems: in particular, it tightens its versions of Caffe and TensorFlow and may break the environment settings slightly.  In the end, I decided to go the simple way and install both frameworks in a virtual machine with Ubuntu 16 (I use <a href="https://www.virtualbox.org/">VirtualBox</a> ). <br><br>  It is worth noting that in order to successfully connect NCS to a virtual machine, you need to install VirtualBox guest add-ons and enable USB 3.0 support.  I also added a universal filter for USB devices, as a result of which NCS was connected without problems (although the webcam still has to be connected in the virtual machine settings).  To install and compile OpenVINO, you need to create an Intel account, select the framework option (with or without FPGA support) and follow the <a href="https://software.intel.com/en-us/articles/OpenVINO-Install-Linux-FPGA">instructions</a> .  It's even easier with NCSDK: it boots <a href="https://github.com/movidius/ncsdk">from GitHub</a> (don't forget to choose the ncsdk2 branch for the new version of the framework), after which you need to <code>make install</code> . <br><br>  The only problem I encountered when running NCSDK2 in a virtual machine is the following error: <br><br><pre> <code class="plaintext hljs">E: [ 0] dispatcherEventReceive:236 dispatcherEventReceive() Read failed -1 E: [ 0] eventReader:254 Failed to receive event, the device may have reset</code> </pre><br>  It occurs at the end of the correct execution of the program and (like) does not affect anything.  Apparently, this is a <a href="https://ncsforum.movidius.com/discussion/840/how-do-i-test-if-the-ncs-is-in-working-condition">small bug related to the VM</a> (this should not be the case on Raspberry). <br><br>  Installation on the Raspberry Pi is significantly different.  First of all, make sure that you have Raspbian Stretch: both frameworks officially work only on this OS.  NCSDK2 needs to be <a href="https://movidius.github.io/blog/ncs-apps-on-rpi/">compiled in API-only mode</a> , otherwise it will try to install Caffe and TensorFlow, which is unlikely to please your Raspberry.  In the case of OpenVINO, there is an already <a href="https://software.intel.com/en-us/articles/OpenVINO-Install-RaspberryPI">compiled version for Raspberry</a> , which only needs to be unpacked and set up environment variables.  In this version there is only C ++ and Python API, as well as the OpenCV library, all other tools are not available.  This means that for both frameworks, models need to be converted in advance on a machine with Ubuntu.  My <a href="https://github.com/BeloborodovDS/NCS-face">face detection demo</a> works on both Raspberry and desktop, so I just added the converted neural network files to my GitHub repository to make them easier to sync to Raspberry.  I have a Raspberry Pi 2 model B, but it should take off with other models. <br><br>  There is another subtlety regarding the interaction of the Raspberry Pi and the Neural Compute Stick: if in the case of a laptop, simply push NCS to the nearest USB 3.0 port, then you will have to find a USB cable for the Raspberry, otherwise the NSC will block the remaining three USB connectors with its case.  It is also worth remembering that on Raspberry all USB versions are 2.0, so the speed of the inference will be lower due to communication delays (a detailed comparison will be later).  But if you want to connect to the Raspberry two or more NCS, you will most likely have to find a USB hub with additional power. <br><br><h3>  How does the code OpenVINO </h3><br>  Pretty cumbersome.  We need to do a lot of different actions, starting with loading the plug-in and ending with the inference itself - so I wrote a wrapper class for the detector.  The full code can be viewed on GitHub, and here I just list the main points.  Let's start in order: <br><br>  The definitions of all the functions we need are in the <code>inference_engine.hpp</code> file in the <code>InferenceEngine</code> namespace. <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;inference_engine.hpp&gt; using namespace InferenceEngine;</span></span></span></span></code> </pre><br>  The following variables will be needed all the time.  <code>inputName</code> and <code>outputName</code> are needed to address the input and output of the neural network.  Generally speaking, a neural network can have many inputs and outputs, but in our detectors there will be one at a time.  The variable <code>net</code> is the network itself, <code>request</code> is a pointer to the last inverse request, <code>inputBlob</code> is a pointer to the neural network input data array.  The remaining variables speak for themselves. <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">string</span></span> inputName; <span class="hljs-built_in"><span class="hljs-built_in">string</span></span> outputName; ExecutableNetwork net; InferRequest::Ptr request; Blob::Ptr inputBlob; <span class="hljs-comment"><span class="hljs-comment">//input shape int netInputWidth; int netInputHeight; int netInputChannels; //output shape int maxNumDetectedFaces; //return code StatusCode ncsCode;</span></span></code> </pre><br>  Now we will load the necessary plug-in - we need the one that is responsible for NCS and NCS2, it can be obtained by the name "MYRIAD".  Let me remind you that in the context of OpenVINO, a plugin is just a dynamic library connecting via an explicit request.  The parameter of the <code>PluginDispatcher</code> function is the list of directories in which plugins should be searched.  If you set up environment variables according to the instructions, an empty string will suffice.  For reference, the plugins are in <code>[OpenVINO_install_dir]/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/</code> <br><br><pre> <code class="cpp hljs">InferencePlugin plugin = PluginDispatcher({<span class="hljs-string"><span class="hljs-string">""</span></span>}).getPluginByDevice(<span class="hljs-string"><span class="hljs-string">"MYRIAD"</span></span>);</code> </pre><br>  Now we will create an object to load the neural network, read its description and set the size of the batch (the number of simultaneously processed images).  A neural network in the OpenVINO format is defined by two files: .xml with a description of the structure and .bin with weights.  While we will use ready-made detectors from OpenVINO, we will later create our own.  Here <code>std::string filename</code> is the file name without the extension.  You should also keep in mind that NCS only supports a batch size of 1. <br><br><pre> <code class="cpp hljs">CNNNetReader netReader; netReader.ReadNetwork(filename+<span class="hljs-string"><span class="hljs-string">".xml"</span></span>); netReader.ReadWeights(filename+<span class="hljs-string"><span class="hljs-string">".bin"</span></span>); netReader.getNetwork().setBatchSize(<span class="hljs-number"><span class="hljs-number">1</span></span>);</code> </pre><br>  Next, the following happens: <br><br><ol><li>  To enter the neural network, we set the unsigned char 8 bit data type.  This means that we can feed the image in the format in which it comes from the camera, and InferenceEngine will take care of the conversion (NCS performs calculations in the float 16 bit format).  This will speed up a bit on the Raspberry Pi - as I understand it, the conversion is done on the NCS, so the data transfer delay via USB is less. </li><li>  We get the names of the input and output, then to refer to them. </li><li>  We get the description of the outputs (this is the map from the name of the output to the pointer to the data block).  We get a pointer to the data block of the first (single) output. </li><li>  We get its size: 1 x 1 x maximum number of detections x length of the description of detection (7).  About the format of the description of detections - later. </li><li>  Set output format to float 32 bit.  Again, the conversion from float 16 bit takes over the InferenceEngine. </li></ol><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//we can set input type to unsigned char: conversion will be performed on device netReader.getNetwork().getInputsInfo().begin()-&gt;second-&gt;setPrecision(Precision::U8); //get input and output names and their info structures inputName = netReader.getNetwork().getInputsInfo().begin()-&gt;first; outputName = netReader.getNetwork().getOutputsInfo().begin()-&gt;first; OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo()); InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo()); DataPtr &amp;outputData = (outputInfo.begin()-&gt;second); //get output shape: (1 x 1 x maxNumDetectedFaces x faceDescriptionLength(7)) const SizeVector outputDims = outputData-&gt;getTensorDesc().getDims(); maxNumDetectedFaces = outputDims[2]; //set input type to float32: calculations are all in float16, conversion is performed on device outputData-&gt;setPrecision(Precision::FP32);</span></span></code> </pre><br>  Now the most important point: load the neural network into the plugin (that is, in the NCS).  Apparently, the compilation in the desired format occurs on the fly.  If the program crashes on this function, the neural network is probably not suitable for this device. <br><br><pre> <code class="cpp hljs">net = plugin.LoadNetwork(netReader.getNetwork(), {});</code> </pre><br>  And finally - let's make a trial inference and get the dimensions of the entrance (perhaps this can be done more elegantly).  First, we open the query for inferens, then from it we get a link to the input data block, and we already ask it for the size. <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//perform single inference to get input shape (a hack) request = net.CreateInferRequestPtr(); //open inference request //we need the blob size: (batch(1) x channels(3) x H x W) inputBlob = request-&gt;GetBlob(inputName); SizeVector blobSize = inputBlob-&gt;getTensorDesc().getDims(); netInputWidth = blobSize[3]; netInputHeight = blobSize[2]; netInputChannels = blobSize[1]; request-&gt;Infer(); //close request</span></span></code> </pre><br>  Let's try to upload a picture in NCS.  In the same way, we create a request for inference, we get a pointer to a data block from it, and from there we get a pointer to the array itself.  Next, simply copy the data from our image (here it is already reduced to the desired size).  It is worth noting that measurements in <code>cv::Mat</code> and <code>inputBlob</code> are stored in a different order (in OpenCV, the channel index changes the fastest, in OpenVINO - the slowest), so memcpy cannot do with one.  Then we start asynchronous inference. <br><br>  Why asynchronous?  This will optimize the allocation of resources.  While NCS considers the neural network, it is possible to process the next frame - this will lead to a noticeable acceleration on the Raspberry Pi. <br><br><pre> <code class="cpp hljs">cv::Mat data; ... <span class="hljs-comment"><span class="hljs-comment">//get image somehow //create request, get data blob request = net.CreateInferRequestPtr(); inputBlob = request-&gt;GetBlob(inputName); unsigned char* blobData = inputBlob-&gt;buffer().as&lt;unsigned char*&gt;(); //copy from resized frame to network input int wh = netInputHeight*netInputWidth; for (int c = 0; c &lt; netInputChannels; c++) for (int h = 0; h &lt; wh; h++) blobData[c * wh + h] = data.data[netInputChannels*h + c]; //start asynchronous inference request-&gt;StartAsync();</span></span></code> </pre><br>  If you are familiar with neural networks, you might have a question about when we scale the values ‚Äã‚Äãof the input pixels of the neural network (for example, we reduce to <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mo" id="MJXp-Span-2" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-3">0</span><span class="MJXp-mo" id="MJXp-Span-4" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-5">1</span><span class="MJXp-mo" id="MJXp-Span-6" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.653ex" height="2.66ex" viewBox="0 -832 2003.2 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-5B" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-30" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-2C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-31" x="1224" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-5D" x="1724" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-1"> [0,1] </script>  ).  The fact is that in the OpenVINO models this transformation is already included in the description of the neural network, and when using our detector we will do something similar.  And since both the conversion to float and input scaling are done by OpenVINO, we just need to resize the image. <br><br>  Now (after doing some useful work) complete the request for inference.  The program is blocked until the execution results come.  We get a pointer to the result. <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> * output; ncsCode = request-&gt;Wait(IInferRequest::WaitMode::RESULT_READY); output = request-&gt;GetBlob(outputName)-&gt;buffer().as&lt;<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>*&gt;();</code> </pre><br>  Now it's time to think about the format in which NCS returns the result of the detector.  It is worth noting that the format is slightly different from what it was when using NCSDK.  Generally speaking, the detector output is four-dimensional and has a dimension (1 x 1 x maximum number of detections x 7), we can assume that this is an array of size ( <code>maxNumDetectedFaces</code> x 7). <br><br>  The <code>maxNumDetectedFaces</code> parameter is specified in the description of the neural network, and it is easy to change it, for example, in the .prototxt description of the network in the Caffe format.  Earlier we got it from the object representing the detector.  This parameter is related to the specifics of the class of detectors <a href="https://arxiv.org/abs/1512.02325">SSD (Single Shot Detector)</a> , which includes all supported NCS detectors.  SSD always considers the same (and very large) number of bounding boxes for each image, and after screening out detections with a low confidence rating and removing overlapping frames with Non-maximum Suppression, usually 100-200 are usually left.  This is what the parameter is responsible for. <br><br>  The seven values ‚Äã‚Äãin the description of one detection are as follows: <br><br><ol><li>  the number of the image in the batch where the object was detected (in our case it should be zero); </li><li>  object class (0 - background, starting from 1 - other classes, only detections with a positive class are returned); </li><li>  confidence in the presence of detection (in the range <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-7"><span class="MJXp-mo" id="MJXp-Span-8" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-9">0</span><span class="MJXp-mo" id="MJXp-Span-10" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-11">1</span><span class="MJXp-mo" id="MJXp-Span-12" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.653ex" height="2.66ex" viewBox="0 -832 2003.2 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-5B" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-30" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-2C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-31" x="1224" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/436744/&amp;xid=17259,15700023,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhioYCkLMj9aFjp9M09T5csCriSqg#MJMAIN-5D" x="1724" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-2"> [0,1] </script>  ); </li><li>  the normalized x-coordinate of the upper left corner of the bounding box (in the range <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-13"><span class="MJXp-mo" id="MJXp-Span-14" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-15">0</span><span class="MJXp-mo" id="MJXp-Span-16" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-17">1</span><span class="MJXp-mo" id="MJXp-Span-18" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-3"> [0,1] </script>  ); </li><li>  similarly, the y-coordinate; </li><li>  the normalized width of the bounding box (in the range <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-19"><span class="MJXp-mo" id="MJXp-Span-20" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-21">0</span><span class="MJXp-mo" id="MJXp-Span-22" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-23">1</span><span class="MJXp-mo" id="MJXp-Span-24" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-4"> [0,1] </script>  ); </li><li>  similarly - height; </li></ol><br><div class="spoiler">  <b class="spoiler_title">The code to extract the bounding box from the detector output</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_detection_boxes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params">* predictions, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> numPred, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> w, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> h, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> thresh, </span></span><span class="hljs-built_in"><span class="hljs-function"><span class="hljs-params"><span class="hljs-built_in">std</span></span></span></span><span class="hljs-function"><span class="hljs-params">::</span></span><span class="hljs-built_in"><span class="hljs-function"><span class="hljs-params"><span class="hljs-built_in">vector</span></span></span></span><span class="hljs-function"><span class="hljs-params">&lt;</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params">&gt;&amp; probs, </span></span><span class="hljs-built_in"><span class="hljs-function"><span class="hljs-params"><span class="hljs-built_in">std</span></span></span></span><span class="hljs-function"><span class="hljs-params">::</span></span><span class="hljs-built_in"><span class="hljs-function"><span class="hljs-params"><span class="hljs-built_in">vector</span></span></span></span><span class="hljs-function"><span class="hljs-params">&lt;cv::Rect&gt;&amp; boxes)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> score = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> cls = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> id = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-comment"><span class="hljs-comment">//predictions holds numPred*7 values //data format: image_id, detection_class, detection_confidence, //box_normed_x, box_normed_y, box_normed_w, box_normed_h for (int i=0; i&lt;numPred; i++) { score = predictions[i*7+2]; cls = predictions[i*7+1]; id = predictions[i*7 ]; if (id&gt;=0 &amp;&amp; score&gt;thresh &amp;&amp; cls&lt;=1) { probs.push_back(score); boxes.push_back(Rect(predictions[i*7+3]*w, predictions[i*7+4]*h, (predictions[i*7+5]-predictions[i*7+3])*w, (predictions[i*7+6]-predictions[i*7+4])*h)); } } }</span></span></code> </pre><br>  <code>numPred</code> we learn from the detector itself, and <code>w,h</code> - the size of the image for visualization. <br></div></div><br>  Now about how the general scheme of inference in real time looks.  First, we initialize the neural network and the camera, set up <code>cv::Mat</code> for raw frames, and another one for frames reduced to the required size.  We fill our frames with zeros - this will add confidence that at the idle launch the neural network will not find anything.  Then we start the inference cycle: <br><br><ul><li>  We load the current frame into the neural network using an asynchronous request - the NCS has already started working, and at this time we have the opportunity to do useful work as the main processor. </li><li>  We display all previous detections in the previous frame, draw a frame (if necessary). </li><li>  We get a new frame from the camera, compress it to the desired size.  For Raspberry, I recommend using the simplest resizing algorithm ‚Äî in OpenCV, this is the Nearest Neighbors interpolation.  The quality of the detector is not much affected, but can throw a little speed.  I also mirror the frame for easy visualization (optional). </li><li>  Now it's time to get the result from the NCS, completing the inference request.  The program will be blocked until the result. </li><li>  We process new detections, select frames. </li><li>  Else: working out keystrokes, frame counting, etc. </li></ul><br><h3>  How to compile it </h3><br>  In the examples InferenceEngine, I didn‚Äôt like the cumbersome CMake files, and I decided to compactly rewrite everything into my Makefile: <br><br><pre> <code class="bash hljs">g++ $(RPI_ARCH) \ -I/usr/include -I. \ -I$(OPENVINO_PATH)/deployment_tools/inference_engine/include \ -I$(OPENVINO_PATH_RPI)/deployment_tools/inference_engine/include \ -L/usr/lib/x86_64-linux-gnu \ -L/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/lib \ -L$(OPENVINO_PATH)/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64 \ -L$(OPENVINO_PATH_RPI)/deployment_tools/inference_engine/lib/raspbian_9/armv7l \ vino.cpp wrapper/vino_wrapper.cpp \ -o demo -std=c++11 \ `pkg-config opencv --cflags --libs` \ -ldl -linference_engine $(RPI_LIBS)</code> </pre><br>  This team will work on both Ubuntu and Raspbian, thanks to a pair of tricks.  I specified the paths for searching for headers and dynamic libraries for both Raspberry and Ubuntu machines.  From libraries, in addition to OpenCV, you also need to connect <code>libinference_engine</code> and <code>libdl</code> - a library for dynamically linking other libraries, it is needed for the plug-in to load.  In this case, the <code>libmyriadPlugin</code> itself is not necessary.  Among other things, for Raspberry, I also connect the <a href="http://www.uco.es/investiga/grupos/ava/node/40">Raspicam</a> library to work with the camera (this is <code>$(RPI_LIBS)</code> ).  Also had to use the standard C ++ 11. <br><br>  Separately, it is worth noting that when compiling on Raspberry you need the <code>-march=armv7-a</code> flag (this is <code>$(RPI_ARCH)</code> ).  If you do not specify it, the program will compile, but will fall with a quiet segment.  You can also add optimization with <code>-O3</code> , this will add speed. <br><br><h3>  What are the detectors </h3><br>  NCS supports only Caffe SSD detectors out of the box, although with the help of a pair of dirty tricks I managed to run <a href="https://habr.com/ru/post/347438/">YOLO from Darknet</a> on it.  <a href="https://arxiv.org/abs/1512.02325">Single Shot Detector (SSD)</a> is a popular architecture among lightweight neural networks, and with the help of different encoders (or backbone networks) you can quite flexibly vary the ratio of speed and quality. <br><br>  I will experiment with different face detectors: <br><br><ul><li>  YOLO, taken <a href="https://github.com/dannyblueliu/YOLO-Face-detection">from here</a> , converted first to Caffe format, then to NCS format (only with NCSDK).  The image is 448 x 448. </li><li>  Own <a href="https://arxiv.org/abs/1704.04861">Mobilenet</a> + SSD detector, the training of which I told in a <a href="https://habr.com/ru/post/424973/">previous publication</a> .  I still have a clipped version of this detector, which sees only small faces, and at the same time a little faster.  I will check the full version of my detector on both NCSDK and OpenVINO.  The image is 300 x 300. </li><li>  Detector face-detection-adas-0001 from OpenVINO: MobileNet + SSD.  The image is 384 x 672. </li><li>  OpenVINO face-detection-retail-0004 detector: lightweight <a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a> + SSD.  The image is 300 x 300. </li></ul><br>  For detectors from OpenVINO, there are no scales in either the Caffe format or the NCSDK format, so I can only run them in OpenVINO. <br><br><h3>  Transform your detector in the format OpenVINO </h3><br>  I have two files in the Caffe format: .prototxt with the description of the network and .caffemodel with weights.  I need to get two of them in the format OpenVINO: .xml and .bin with the description and weights, respectively.  To do this, you need to use the mo.py script from OpenVINO (aka Model Optimizer): <br><br><pre> <code class="bash hljs">mo.py \ --framework caffe \ --input_proto models/face/ssd-face.prototxt \ --input_model models/face/ssd-face.caffemodel \ --output_dir models/face \ --model_name ssd-vino-custom \ --mean_values [127.5,127.5,127.5] \ --scale_values [127.5,127.5,127.5] \ --data_type FP16</code> </pre><br>  <code>output_dir</code> sets the directory in which new files will be created, <code>model_name</code> is the name for new files without extension, <code>data_type (FP16/FP32)</code> is the type of weights in the neural network (NCS supports only FP16).  The parameters <code>mean_values, scale_values</code> set the average and scale for preprocessing images before launching them into the neural network.  The specific conversion looks like this: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-25"><span class="MJXp-mo" id="MJXp-Span-26" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-28">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29">x</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31">l</span><span class="MJXp-msubsup" id="MJXp-Span-32"><span class="MJXp-mtext" id="MJXp-Span-33" style="margin-right: 0.05em;">&nbsp;</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-34" style="vertical-align: -0.4em;">v</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-35">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-39">s</span><span class="MJXp-mo" id="MJXp-Span-40" style="margin-left: 0em; margin-right: 0.222em;">‚Äã</span><span class="MJXp-mo" id="MJXp-Span-41" style="margin-left: 0em; margin-right: 0.222em;">‚Äã</span><span class="MJXp-mo" id="MJXp-Span-42" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46">n</span><span class="MJXp-msubsup" id="MJXp-Span-47"><span class="MJXp-mtext" id="MJXp-Span-48" style="margin-right: 0.05em;">&nbsp;</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-49" style="vertical-align: -0.4em;">v</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-50">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-51">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54">s</span><span class="MJXp-mo" id="MJXp-Span-55" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mrow" id="MJXp-Span-56"><span class="MJXp-mo" id="MJXp-Span-57" style="margin-left: 0.111em; margin-right: 0.111em;">/</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-62">e</span><span class="MJXp-msubsup" id="MJXp-Span-63"><span class="MJXp-mtext" id="MJXp-Span-64" style="margin-right: 0.05em;">&nbsp;</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-65" style="vertical-align: -0.4em;">v</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-66">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-68">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-69">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70">s</span><span class="MJXp-mo" id="MJXp-Span-71" style="margin-left: 0em; margin-right: 0.222em;">‚Äã</span><span class="MJXp-mo" id="MJXp-Span-72" style="margin-left: 0em; margin-right: 0.222em;">‚Äã</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> (pixel \ _values ‚Äã‚Äã- mean \ _values) / scale \ _values ‚Äã‚Äã</script></p><br><br>  In this case, the values ‚Äã‚Äãare brought from the range <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-73"><span class="MJXp-mo" id="MJXp-Span-74" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-75">0</span><span class="MJXp-mo" id="MJXp-Span-76" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-77">255</span><span class="MJXp-mo" id="MJXp-Span-78" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-6"> [0,255] </script>  in range <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-79"><span class="MJXp-mo" id="MJXp-Span-80" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-81">0</span><span class="MJXp-mo" id="MJXp-Span-82" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-83">1</span><span class="MJXp-mo" id="MJXp-Span-84" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-7"> [0,1] </script>  .  In general, this script has a lot of parameters, some of which are specific for individual frameworks, I recommend to look at the manual for the script. <br><br>  There are no ready-made models in the OpenVINO distribution for Raspberry, but you can simply download them. <br><br><div class="spoiler">  <b class="spoiler_title">For example, like this.</b> <div class="spoiler_text"><pre> <code class="bash hljs"> wget --no-check-certificate \ https://download.01.org/openvinotoolkit/2018_R4/open_model_zoo/face-detection-retail-0004/FP16/face-detection-retail-0004.xml \ -O ./models/face/vino.xml; \ wget --no-check-certificate \ https://download.01.org/openvinotoolkit/2018_R4/open_model_zoo/face-detection-retail-0004/FP16/face-detection-retail-0004.bin \ -O ./models/face/vino.bin</code> </pre><br></div></div><br><h3>  Comparison of detectors and frameworks </h3><br>  I used three comparison options: 1) NCS + Virtual Machine with Ubuntu 16.04, Core i7 processor, USB 3.0 connector;  2) NCS + Same machine, USB 3.0 connector + USB 2.0 cable (there will be more exchange delays with the device);  3) NCS + Raspberry Pi 2 model B, Raspbian Stretch, USB 2.0 connector + USB 2.0 cable. <br><br>  I ran my detector with both OpenVINO and NCSDK2, detectors from OpenVINO only with their native framework, YOLO only with NCSDK2 (most likely, it can be run on OpenVINO). <br><br>  The FPS table for different detectors looks like this (approximate numbers): <br><br><table><tbody><tr><th>  Model </th><th>  USB 3.0 </th><th>  USB 2.0 </th><th>  Raspberry pi </th></tr><tr><td>  Custom SSD with NCSDK2 </td><td>  10.8 </td><td>  9.3 </td><td>  7.2 </td></tr><tr><td>  Custom longrange SSD with NCSDK2 </td><td>  11.8 </td><td>  10.0 </td><td>  7.3 </td></tr><tr><td>  YOLO v2 with NCSDK2 </td><td>  5.3 </td><td>  4.6 </td><td>  3.6 </td></tr><tr><td>  Custom SSD with OpenVINO </td><td>  10.6 </td><td>  9.9 </td><td>  7.9 </td></tr><tr><td>  OpenVINO face-detection-retail-0004 </td><td>  15.6 </td><td>  14.2 </td><td>  9.3 </td></tr><tr><td>  OpenVINO face-detection-adas-0001 </td><td>  5.8 </td><td>  5.5 </td><td>  3.9 </td></tr></tbody></table><br><br>  <em>Note: performance was measured for the entire demo program entirely, including frame processing and visualization.</em> <br><br>  YOLO was the slowest and most unstable of all.  He very often skips the detection and can not work with the illuminated frames. <br><br>  The detector, which I trained, works twice as fast, is more resistant to distortions in frames and detects even small faces.  However, he still sometimes skips the detections, and sometimes he detects false ones.  If you cut off the last few layers from it, it will become slightly faster, but it will cease to see large faces.  The same detector, launched via OpenVINO, becomes slightly faster when using USB 2.0, the quality does not change visually. <br><br>  The detectors from OpenVINO, of course, are far superior to both YOLO and my detector.  (I would not even train my detector if OpenVINO existed in its current form at that time).  The retail-0004 model is significantly faster and at the same time practically does not miss the face, but I managed to slightly deceive her (although the confidence of these detections is low): <br><br><img src="https://habrastorage.org/webt/uj/ap/nl/ujapnlbjzkipljlzklgyvjzked4.png"><br>  <em>Competitive attack of natural intelligence on artificial</em> <br><br>  The adas-0001 detector is significantly slower, but it works with images of large size and should be more accurate.  I did not notice the difference, but I checked it on pretty simple frames. <br><br><h4>  Conclusion </h4><br>  In general, it is very nice that on a low-power device like the Raspberry Pi you can use neural networks, and even almost in real time.  OpenVINO provides very extensive functionality for the inference of neural networks on many different devices - much broader than I described in the article.  I think Neural Compute Stick and OpenVINO will be very useful in my robotic research. </div><p>Source: <a href="https://habr.com/ru/post/436744/">https://habr.com/ru/post/436744/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>