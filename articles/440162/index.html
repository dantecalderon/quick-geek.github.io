<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>An example of a simple neural network in C / C ++</title>
  <meta name="description" content="Hello. 

 I decided to share a simple and capacious in my opinion solution of a neural network in C ++. 

 Why this information should be interesting?...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>An example of a simple neural network in C / C ++</h1><div class="post__text post__text-html js-mediator-article">  Hello. <br><br>  I decided to share a simple and capacious in my opinion solution of a neural network in C ++. <br><br>  <b>Why this information should be interesting?</b> <br><br>  <b>Answer:</b> I tried in the minimum set to program the work of the multilayer perceptron, so that it could be configured as you please in just a few lines of code, and the implementation of the basic algorithms of work on ‚ÄúC‚Äù will allow to easily transfer oriented languages ‚Äã‚Äãto ‚ÄúC‚Äù ( and any other) <b><u>without the use of third-party libraries!</u></b> <br><br><h4>  Please take a look at what came of it. </h4><br>  I will not tell you about the <b>purpose of neural networks</b> , I hope you are not banned from <b>google</b> and you can find the information you are interested in (purpose, possibilities, areas of application, and so on). <br><br>  You will find the <b>source code</b> at the end of the article, but for now in order. <br><br><h3>  Let's start the analysis </h3><br><h4>  1) Architecture and technical details </h4><br>  - <b>multi-layer perceptron</b> with the ability to configure any number of layers with a given width.  Below is <br><br><div class="spoiler">  <b class="spoiler_title">configuration example</b> <div class="spoiler_text">  <b>myNeuero.cpp</b> <br><br><pre><code class="cpp hljs">inputNeurons = <span class="hljs-number"><span class="hljs-number">100</span></span>; <span class="hljs-comment"><span class="hljs-comment">//—à–∏—Ä–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è outputNeurons =2; //—à–∏—Ä–∏–Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è nlCount = 4; //–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ ( –ø–æ —Ñ–∞–∫—Ç—É –∏—Ö 3, —É–∫–∞–∑—ã–≤–∞–µ–º–æ–µ —á–∏—Å–ª–æ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–µ–Ω–æ –Ω–∞ 1 list = (nnLay*) malloc((nlCount)*sizeof(nnLay)); inputs = (float*) malloc((inputNeurons)*sizeof(float)); targets = (float*) malloc((outputNeurons)*sizeof(float)); list[0].setIO(100,20); //—É—Å—Ç–∞–Ω–æ–≤–∫–∞ —à–∏—Ä–∏–Ω—ã INPUTS/OUTPUTS –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è list[1].setIO(20,6); // -//- list[2].setIO(6,3); // -//- list[3].setIO(3,2); // -//- –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π</span></span></code> </pre> <br></div></div><br>  Please note that setting the input and output widths for each layer is performed according to a certain rule - the input of the current layer = the output of the previous one.  The exception is the input layer. <br><br>  Thus, you have the opportunity to customize any configuration manually or according to a predetermined rule before compiling or after compiling to read data from source files. <a name="habracut"></a><br><br>  - implementation of the mechanism of <b>back propagation of errors</b> with the ability to set the learning rate <br><br>  <b>myNeuero.h</b> <br><br><pre> <code class="cpp hljs"> <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> learnRate 0.1</span></span></code> </pre> <br>  - installation of <b>initial scales</b> <br><br>  <b>myNeuero.h</b> <br><br><pre> <code class="cpp hljs"> <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> randWeight (( ((float)qrand() / (float)RAND_MAX) - 0.5)* pow(out,-0.5))</span></span></code> </pre> <br>  <b>Note</b> : if there are more than three layers (nlCount&gt; 4), then pow (out, -0.5) needs to be increased so that with direct signal passing its energy does not reduce to 0. Example of pow (out, -0.2) <br><br>  - the <b>basis of the code in C.</b> The basic algorithms and storage of weight coefficients are implemented as a structure in the C language, everything else is a shell of the calling function of this structure, it is also a display of any of the layers taken separately <br><br><div class="spoiler">  <b class="spoiler_title">Layer structure</b> <div class="spoiler_text">  <b>myNeuero.h</b> <br><br><pre> <code class="cpp hljs"> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">nnLay</span></span></span><span class="hljs-class">{</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> in; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> out; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>** matrix; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>* hidden; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>* errors; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getInCount</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{<span class="hljs-keyword"><span class="hljs-keyword">return</span></span> in;} <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getOutCount</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{<span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out;} <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> **</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getMatrix</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{<span class="hljs-keyword"><span class="hljs-keyword">return</span></span> matrix;} <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">updMatrix</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *enteredVal)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ou =<span class="hljs-number"><span class="hljs-number">0</span></span>; ou &lt; out; ou++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> hid =<span class="hljs-number"><span class="hljs-number">0</span></span>; hid &lt; in; hid++) { matrix[hid][ou] += (learnRate * errors[ou] * enteredVal[hid]); } matrix[in][ou] += (learnRate * errors[ou]); } }; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">setIO</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> inputs, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> outputs)</span></span></span><span class="hljs-function"> </span></span>{ in=inputs; out=outputs; hidden = (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>*) <span class="hljs-built_in"><span class="hljs-built_in">malloc</span></span>((out)*<span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)); matrix = (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>**) <span class="hljs-built_in"><span class="hljs-built_in">malloc</span></span>((in+<span class="hljs-number"><span class="hljs-number">1</span></span>)*<span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> inp =<span class="hljs-number"><span class="hljs-number">0</span></span>; inp &lt; in+<span class="hljs-number"><span class="hljs-number">1</span></span>; inp++) { matrix[inp] = (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>*) <span class="hljs-built_in"><span class="hljs-built_in">malloc</span></span>(out*<span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)); } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> inp =<span class="hljs-number"><span class="hljs-number">0</span></span>; inp &lt; in+<span class="hljs-number"><span class="hljs-number">1</span></span>; inp++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> outp =<span class="hljs-number"><span class="hljs-number">0</span></span>; outp &lt; out; outp++) { matrix[inp][outp] = randWeight; } } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">makeHidden</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *inputs)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> hid =<span class="hljs-number"><span class="hljs-number">0</span></span>; hid &lt; out; hid++) { <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> tmpS = <span class="hljs-number"><span class="hljs-number">0.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> inp =<span class="hljs-number"><span class="hljs-number">0</span></span>; inp &lt; in; inp++) { tmpS += inputs[inp] * matrix[inp][hid]; } tmpS += matrix[in][hid]; hidden[hid] = sigmoida(tmpS); } }; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function">* </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getHidden</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hidden; }; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calcOutError</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *targets)</span></span></span><span class="hljs-function"> </span></span>{ errors = (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>*) <span class="hljs-built_in"><span class="hljs-built_in">malloc</span></span>((out)*<span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ou =<span class="hljs-number"><span class="hljs-number">0</span></span>; ou &lt; out; ou++) { errors[ou] = (targets[ou] - hidden[ou]) * sigmoidasDerivate(hidden[ou]); } }; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calcHidError</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *targets,</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> **outWeights,</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> inS, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> outS)</span></span></span><span class="hljs-function"> </span></span>{ errors = (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>*) <span class="hljs-built_in"><span class="hljs-built_in">malloc</span></span>((inS)*<span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> hid =<span class="hljs-number"><span class="hljs-number">0</span></span>; hid &lt; inS; hid++) { errors[hid] = <span class="hljs-number"><span class="hljs-number">0.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ou =<span class="hljs-number"><span class="hljs-number">0</span></span>; ou &lt; outS; ou++) { errors[hid] += targets[ou] * outWeights[hid][ou]; } errors[hid] *= sigmoidasDerivate(hidden[hid]); } }; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function">* </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getErrors</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> errors; }; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sigmoida</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> val)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (<span class="hljs-number"><span class="hljs-number">1.0</span></span> + <span class="hljs-built_in"><span class="hljs-built_in">exp</span></span>(-val))); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sigmoidasDerivate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> val)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (val * (<span class="hljs-number"><span class="hljs-number">1.0</span></span> - val)); }; };</code> </pre><br></div></div><br><h4>  2) Application </h4><br>  Testing the project with the mnist set was successful, we managed to achieve the conditional probability of handwriting recognition 0.9795 (nlCount = 4, learnRate = 0.03 and several epochs).  The main purpose of the test was to test the performance of the neural network, with which it coped. <br><br>  Below we consider the work on the <b>"conditional task</b> . <b>"</b> <br><br>  <b>Initial data:</b> <br><br>  -2 random input vectors of 100 values <br>  - neural network with random generation of scales <br>  -2 set goals <br><br>  <b>Code</b> in main () function <br><br><pre> <code class="cpp hljs">{ <span class="hljs-comment"><span class="hljs-comment">//!!!________ –î–õ–Ø –í–´–í–û–î–ê –í–ú–ï–°–¢–û qDebug() –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å std::cout –∏–ª–∏ std::cerr myNeuro *bb = new myNeuro(); //----------------------------------INPUTS----GENERATOR------------- /! —Å–æ–∑–¥–∞—ë–º 2 —Å–ª—É—á–∞–π–Ω–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞ qsrand((QTime::currentTime().second())); float *abc = new float[100]; for(int i=0; i&lt;100;i++) { abc[i] =(qrand()%98)*0.01+0.01; } float *cba = new float[100]; for(int i=0; i&lt;100;i++) { cba[i] =(qrand()%98)*0.01+0.01; } //---------------------------------TARGETS----GENERATOR------------- // —Å–æ–∑–¥–∞–µ–º 2 —Ü–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è float *tar1 = new float[2]; tar1[0] =0.01; tar1[1] =0.99; float *tar2 = new float[2]; tar2[0] =0.99; tar2[1] =0.01; //--------------------------------NN---------WORKING--------------- // –ø–µ—Ä–≤–∏—á–Ω—ã–π –æ–ø—Ä–æ—Å —Å–µ—Ç–∏ bb-&gt;query(abc); qDebug()&lt;&lt;"_________________________________"; bb-&gt;query(cba); // –æ–±—É—á–µ–Ω–∏–µ int i=0; while(i&lt;100000) { bb-&gt;train(abc,tar1); bb-&gt;train(cba,tar2); i++; } //–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ä–æ—Å —Å–µ—Ç–∏ –≤—Ç–æ—Ä–æ–π —Ä–∞–∑) qDebug()&lt;&lt;"___________________RESULT_____________"; bb-&gt;query(abc); qDebug()&lt;&lt;"______"; bb-&gt;query(cba); }</span></span></code> </pre> <br>  <b>The result of the neural network</b> <br><br><img src="https://habrastorage.org/webt/gt/oe/vc/gtoevca428fe3i7wmvkkupaayq0.png" alt="image"><br><br><h3>  <b>Results</b> </h3><br>  As you can see, calling the query (inputs) function before learning for each of the vectors does not allow us to judge their differences.  Further, by calling the train (input, target) function, for training with the goal of arranging weights so that the neural network can later distinguish the input vectors. <br><br>  After completing the training, we observe that the attempt to compare the vector ‚Äúabc‚Äù - ‚Äútar1‚Äù, and ‚Äúcba‚Äù - ‚Äútar2‚Äù succeeded. <br><br>  <b>You are given the opportunity using the source code to independently test performance and experiment with the configuration!</b> <br><br>  PS: this code was written from QtCreator, I hope to ‚Äúreplace the output‚Äù you will have no difficulty, leave your comments and observations. <br><br>  PPS: if anyone is interested in a detailed analysis of the work struct nnLay {} write, there will be a new post. <br><br>  PPPS: I hope someone will come in handy "C" oriented code for transfer to other tools. <br><br><div class="spoiler">  <b class="spoiler_title">Sources</b> <div class="spoiler_text">  <b>main.cpp</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;QCoreApplication&gt; #include &lt;QDebug&gt; #include &lt;QTime&gt; #include "myneuro.h" int main(int argc, char *argv[]) { QCoreApplication a(argc, argv); myNeuro *bb = new myNeuro(); //----------------------------------INPUTS----GENERATOR------------- qsrand((QTime::currentTime().second())); float *abc = new float[100]; for(int i=0; i&lt;100;i++) { abc[i] =(qrand()%98)*0.01+0.01; } float *cba = new float[100]; for(int i=0; i&lt;100;i++) { cba[i] =(qrand()%98)*0.01+0.01; } //---------------------------------TARGETS----GENERATOR------------- float *tar1 = new float[2]; tar1[0] =0.01; tar1[1] =0.99; float *tar2 = new float[2]; tar2[0] =0.99; tar2[1] =0.01; //--------------------------------NN---------WORKING--------------- bb-&gt;query(abc); qDebug()&lt;&lt;"_________________________________"; bb-&gt;query(cba); int i=0; while(i&lt;100000) { bb-&gt;train(abc,tar1); bb-&gt;train(cba,tar2); i++; } qDebug()&lt;&lt;"___________________RESULT_____________"; bb-&gt;query(abc); qDebug()&lt;&lt;"______"; bb-&gt;query(cba); qDebug()&lt;&lt;"_______________THE____END_______________"; return a.exec(); }</span></span></span></span></code> </pre><br>  <b>myNeuro.cpp</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"myneuro.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;QDebug&gt; myNeuro::myNeuro() { //--------–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π inputNeurons = 100; outputNeurons =2; nlCount = 4; list = (nnLay*) malloc((nlCount)*sizeof(nnLay)); inputs = (float*) malloc((inputNeurons)*sizeof(float)); targets = (float*) malloc((outputNeurons)*sizeof(float)); list[0].setIO(100,20); list[1].setIO(20,6); list[2].setIO(6,3); list[3].setIO(3,2); //--------–æ–¥–Ω–æ—Å–ª–æ–π–Ω—ã–π--------- // inputNeurons = 100; // outputNeurons =2; // nlCount = 2; // list = (nnLay*) malloc((nlCount)*sizeof(nnLay)); // inputs = (float*) malloc((inputNeurons)*sizeof(float)); // targets = (float*) malloc((outputNeurons)*sizeof(float)); // list[0].setIO(100,10); // list[1].setIO(10,2); } void myNeuro::feedForwarding(bool ok) { list[0].makeHidden(inputs); for (int i =1; i&lt;nlCount; i++) list[i].makeHidden(list[i-1].getHidden()); if (!ok) { qDebug()&lt;&lt;"Feed Forward: "; for(int out =0; out &lt; outputNeurons; out++) { qDebug()&lt;&lt;list[nlCount-1].hidden[out]; } return; } else { // printArray(list[3].getErrors(),list[3].getOutCount()); backPropagate(); } } void myNeuro::backPropagate() { //-------------------------------ERRORS-----CALC--------- list[nlCount-1].calcOutError(targets); for (int i =nlCount-2; i&gt;=0; i--) list[i].calcHidError(list[i+1].getErrors(),list[i+1].getMatrix(), list[i+1].getInCount(),list[i+1].getOutCount()); //-------------------------------UPD-----WEIGHT--------- for (int i =nlCount-1; i&gt;0; i--) list[i].updMatrix(list[i-1].getHidden()); list[0].updMatrix(inputs); } void myNeuro::train(float *in, float *targ) { inputs = in; targets = targ; feedForwarding(true); } void myNeuro::query(float *in) { inputs=in; feedForwarding(false); } void myNeuro::printArray(float *arr, int s) { qDebug()&lt;&lt;"__"; for(int inp =0; inp &lt; s; inp++) { qDebug()&lt;&lt;arr[inp]; } }</span></span></span></span></code> </pre> <br>  <b>myNeuro.h</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">ifndef</span></span></span><span class="hljs-meta"> MYNEURO_H #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> MYNEURO_H #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;math.h&gt; #include &lt;QtGlobal&gt; #include &lt;QDebug&gt; #define learnRate 0.1 #define randWeight (( ((float)qrand() / (float)RAND_MAX) - 0.5)* pow(out,-0.5)) class myNeuro { public: myNeuro(); struct nnLay{ int in; int out; float** matrix; float* hidden; float* errors; int getInCount(){return in;} int getOutCount(){return out;} float **getMatrix(){return matrix;} void updMatrix(float *enteredVal) { for(int ou =0; ou &lt; out; ou++) { for(int hid =0; hid &lt; in; hid++) { matrix[hid][ou] += (learnRate * errors[ou] * enteredVal[hid]); } matrix[in][ou] += (learnRate * errors[ou]); } }; void setIO(int inputs, int outputs) { in=inputs; out=outputs; hidden = (float*) malloc((out)*sizeof(float)); matrix = (float**) malloc((in+1)*sizeof(float)); for(int inp =0; inp &lt; in+1; inp++) { matrix[inp] = (float*) malloc(out*sizeof(float)); } for(int inp =0; inp &lt; in+1; inp++) { for(int outp =0; outp &lt; out; outp++) { matrix[inp][outp] = randWeight; } } } void makeHidden(float *inputs) { for(int hid =0; hid &lt; out; hid++) { float tmpS = 0.0; for(int inp =0; inp &lt; in; inp++) { tmpS += inputs[inp] * matrix[inp][hid]; } tmpS += matrix[in][hid]; hidden[hid] = sigmoida(tmpS); } }; float* getHidden() { return hidden; }; void calcOutError(float *targets) { errors = (float*) malloc((out)*sizeof(float)); for(int ou =0; ou &lt; out; ou++) { errors[ou] = (targets[ou] - hidden[ou]) * sigmoidasDerivate(hidden[ou]); } }; void calcHidError(float *targets,float **outWeights,int inS, int outS) { errors = (float*) malloc((inS)*sizeof(float)); for(int hid =0; hid &lt; inS; hid++) { errors[hid] = 0.0; for(int ou =0; ou &lt; outS; ou++) { errors[hid] += targets[ou] * outWeights[hid][ou]; } errors[hid] *= sigmoidasDerivate(hidden[hid]); } }; float* getErrors() { return errors; }; float sigmoida(float val) { return (1.0 / (1.0 + exp(-val))); } float sigmoidasDerivate(float val) { return (val * (1.0 - val)); }; }; void feedForwarding(bool ok); void backPropagate(); void train(float *in, float *targ); void query(float *in); void printArray(float *arr,int s); private: struct nnLay *list; int inputNeurons; int outputNeurons; int nlCount; float *inputs; float *targets; }; #endif // MYNEURO_H</span></span></span></span></code> </pre> <br></div></div><br><br><h3>  <b>UPD:</b> </h3>  Sources to check for mnist are <div class="spoiler">  <b class="spoiler_title">reference</b> <div class="spoiler_text">  1) Project <br>  " <a href="https://github.com/mamkin-itshnik/simple-neuro-network">Github.com/mamkin-itshnik/simple-neuro-network</a> " <br>  There is also a graphic description of the work.  Briefly, when polling the network with test data, you are given the value of each of the output neurons (10 neurons correspond to numbers from 0 to 9).  To make a decision about the figure shown, you need to know the maximum neuron index.  Digit = index + 1 (do not forget where the numbers in the arrays are numbered from) <br>  2) MNIST <br>  " <a href="https://www.kaggle.com/oddrationale/mnist-in-csv">Www.kaggle.com/oddrationale/mnist-in-csv</a> " (if you need to use a smaller dataset, just limit the while counter when reading the CSV PS file: there is an example for git) <br></div></div></div><p>Source: <a href="https://habr.com/ru/post/440162/">https://habr.com/ru/post/440162/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>