<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A sober look at Helm 2: ‚ÄúThis is what it is ...‚Äù</title>
  <meta name="description" content="Like any other solution, Helm - the package manager for Kubernetes - has pros, cons and scope, so using it you should correctly evaluate your expectat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-6974184241884155",
      enable_page_level_ads: true
    });
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>A sober look at Helm 2: ‚ÄúThis is what it is ...‚Äù</h1><div class="post__text post__text-html js-mediator-article">  Like any other solution, Helm - the package manager for Kubernetes - has pros, cons and scope, so using it you should correctly evaluate your expectations ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ix/5j/fm/ix5jfmapr_v4yignapctr-x7_jc.jpeg"></div><br>  We use Helm in our arsenal of continuous roll-out tools.  At the time of this writing, there are <b>over a thousand applications</b> in our clusters and about 4000 installations of these applications in various variations.  Periodically, we encounter problems, but in general we are satisfied with the decision; we have no downtime and data loss. <br><br>  The main motive for writing this article is to provide the user with an <b>objective assessment of the</b> main problems of Helm 2 without categorical conclusions, as well as the desire to share experiences and our solutions. <a name="habracut"></a><br><br><h2>  [BUG] After rollout, the status of the release resources in the cluster does not match the described Helm-chart </h2><br>  When running, Helm does not take into account the state of the release resources in the cluster.  When reinstalling the result is determined only by the current and saved configurations.  Thus, the state of the resource in the cluster and the Helm registry is different, and Helm does not take into account. <br><br>  Consider how this problem manifests itself: <br><br><ol><li>  The resource template on the chart corresponds to state X. </li><li>  The user installs the chart (Tiller saves the state of resource X). </li><li>  Then the user manually changes the resource in the cluster (the state changes from X to Y). </li><li>  Without making any changes, he performs the <code>helm upgrade</code> ... And the resource is still in the Y state, although the user expects X. </li></ol><br>  And that is not all.  At some point, the user changes the resource template in the chart (the new state W) - then we have two scenarios after running the <code>helm upgrade</code> : <br><br><ul><li>  Application of the XW patch drops. </li><li>  After applying the patch, the resource goes to the Z state, which does not correspond to the desired one. </li></ul><br>  To avoid such a problem, it is proposed to organize work with releases as follows: <b>no one should change the resources manually</b> , Helm is the only tool for working with release resources.  Ideally, chart changes are versioned in a Git repository and applied <b>exclusively</b> within the CD. <br><br>  If this option is not suitable, then you can <b>follow the synchronization of</b> release <b>resource states</b> .  Manual synchronization might look like this: <br><br><ol><li>  We learn the status of release resources through <code>helm get</code> . </li><li>  We learn the state of resources in Kubernetes through <code>kubectl get</code> . </li><li>  If the resources are different, then we synchronize Helm with Kubernetes: <br><ol><li>  We create a separate branch. </li><li>  We update the manifests of the chart.  Templates must match resource states in Kubernetes. </li><li>  We execute warm.  Synchronize the state in the registry Helm and cluster. </li><li>  After that, the branch can be removed and continue full-time work. </li></ol></li></ol><br>  When applying patches using the <code>kubectl apply</code> , the so-called <b>3-way-merge</b> is performed, i.e.  takes into account the real state of the updated resource.  You can see the code of the algorithm <a href="">here</a> , and read about it <a href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/">here</a> . <br><br>  At the time of this writing, the Helm developers are looking for ways to implement 3-way-merge in Helm 3. With Helm 2, not everything is so rosy: 3-way-merge is not planned to be implemented, but there is a PR to fix the way of creating resources - you can learn more or even participate under the <a href="https://github.com/helm/helm/issues/3805">relevant issue</a> . <br><br><h2>  [BUG] Error: no RESOURCE with the name NAME found </h2><br>  The problem manifests itself when <b>new resources</b> are successfully created when you re-roll <b>out</b> , and the roll-out itself ends up with an error.  Under the <b>new resources are</b> meant those that were not in the last installation of the chart. <br><br>  In case of a failed rollout, the release is saved in the registry marked <b>FAILED</b> , and during the installation Helm relies on the state of the last release of <b>DEPLOYED</b> , which in this case does not know anything about new resources.  As a result, Helm tries to re-create these resources and fails with the error "no RESOURCE with the name NAME found" (the error says the opposite, but this is exactly the problem).  Part of the problem stems from the fact that Helm does not take into account the state of the release resources in the cluster when creating the patch, as described in the previous section. <br><br>  At the moment, the only solution would be to delete new resources manually. <br><br>  In order to avoid such a state, you can automatically delete new resources created in the current upgrade / rollback, if the command ends with an error.  After a long discussion with the developers in Helm for the upgrade / rollback commands, they added the option - <code>--cleanup-on-fail</code> , which activates automatic cleanup when a rollout fails.  Our <a href="https://github.com/helm/helm/pull/4871">PR</a> is under discussion, finding the best solution. <br><br>  Starting with Helm 2.13, the <code>--atomic</code> option appears in the <code>helm install/upgrade</code> <code>--atomic</code> , which activates cleaning and rollback during a failed installation (for more details, see <a href="https://github.com/helm/helm/pull/5143">PR</a> ). <br><br><h2>  [BUG] Error: watch closed before Until timeout </h2><br>  The problem may occur when the Helm hook runs too long (for example, during migrations) - even though the specified timeout for <code>helm install/upgrade</code> , as well as the <code>spec.activeDeadlineSeconds</code> for the corresponding Job are not exceeded. <br><br>  This error is generated by the Kubernetes API server while the hook job is waiting.  Helm does not handle this error and immediately crashes - instead of repeating the pending request. <br><br>  As a solution, you can increase the timeout in the api-server: <code>--min-request-timeout=xxx</code> in the <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> file. <br><br><h2>  [BUG] Error: UPGRADE FAILED: "foo" has no deployed releases </h2><br>  If the first release via <code>helm install</code> ended with an error, the subsequent <code>helm upgrade</code> will return a similar error. <br><br>  It would seem that the solution is quite simple: you need to manually execute <code>helm delete --purge</code> after a failed first installation, but this manual action breaks the CI / CD automatics.  In order not to interrupt the execution of manual commands, you can use the <a href="https://github.com/flant/werf">werf</a> features to <a href="https://github.com/flant/werf">roll</a> it <a href="https://github.com/flant/werf">out</a> .  When using werf, the problematic release will be automatically recreated upon re-installation. <br><br>  In addition, starting with Helm version 2.13 in the <code>helm install</code> and <code>helm upgrade --install</code> enough to specify the - <code>--atomic</code> option and after a failed installation, the release will automatically be removed (see <a href="https://github.com/helm/helm/pull/5143">PR for</a> details). <br><br><h2>  Autorollback </h2><br>  Helm lacks the <code>--autorollback</code> option, which, when <code>--autorollback</code> , will remember the current successful revision (will fall if the last revision is not successful) and, after an unsuccessful attempt, the deploy will rollback to the saved revision. <br><br>  Since it is critical for a product that everything works without interruptions, it is necessary to look for solutions, the rollout must be predictable.  In order to minimize the likelihood of production downtime, a <b>multi-path approach</b> is often used (for example, staging, qa and production), which consists of sequentially rolling out the contours.  With this approach, most of the problems are fixed until they roll back into production and, in conjunction with autorolback, can achieve good results. <br><br>  To organize autorollback, you can use the <a href="https://github.com/ContainerSolutions/helm-monitor">helm-monitor</a> plugin, which allows you to bind rollback to metrics from Prometheus.  A nice article describing this approach is available <a href="https://container-solutions.com/automated-rollback-helm-releases-based-logs-metrics/">here</a> . <br><br>  For some of our projects, a fairly simple approach is used: <br><br><ol><li>  Before deploy we remember the current revision (we believe that in a normal situation, if a release exists, then it is necessarily in the DEPLOYED state): <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> _RELEASE_NAME=myrelease <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> _LAST_DEPLOYED_RELEASE=$(helm list -adr | \ grep <span class="hljs-variable"><span class="hljs-variable">$_RELEASE_NAME</span></span> | grep DEPLOYED | head -n2 | awk <span class="hljs-string"><span class="hljs-string">'{print $2}'</span></span>)</code> </pre> </li><li>  Run install or upgrade: <br><br><pre> <code class="bash hljs">helm install/upgrade ... || <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> _DEPLOY_FAILED=1</code> </pre> </li><li>  Check the deployment status and rollback to the saved state: <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> [ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$_DEPLOY_FAILED</span></span></span><span class="hljs-string">"</span></span> == <span class="hljs-string"><span class="hljs-string">"1"</span></span> ] &amp;&amp; [ <span class="hljs-string"><span class="hljs-string">"x</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$_LAST_DEPLOYED_RELEASE</span></span></span><span class="hljs-string">"</span></span> != <span class="hljs-string"><span class="hljs-string">"x"</span></span> ] ; <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> helm rollback <span class="hljs-variable"><span class="hljs-variable">$_RELEASE_NAME</span></span> <span class="hljs-variable"><span class="hljs-variable">$_LAST_DEPLOYED_RELEASE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">fi</span></span></code> </pre> </li><li>  We end the pipeline with an error if the deployment was unsuccessful: <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> [ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$_DEPLOY_FAILED</span></span></span><span class="hljs-string">"</span></span> == <span class="hljs-string"><span class="hljs-string">"1"</span></span> ] ; <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span> 1 ; <span class="hljs-keyword"><span class="hljs-keyword">fi</span></span></code> </pre> </li></ol><br>  Again, starting with Helm version 2.13, by calling <code>helm upgrade</code> enough to specify the - <code>--atomic</code> option and after a failed installation, rollback will be automatically executed (see <a href="https://github.com/helm/helm/pull/5143">PR for</a> details). <br><br><h2>  Waiting for readiness of release resources and feedback at the time of rollout </h2><br>  As planned, Helm should monitor the performance of relevant liveness and readiness samples when using the <code>--wait</code> option: <br><br><pre> <code class="bash hljs">--<span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>, will <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> until all Pods, PVCs, Services, and minimum number of Pods of a Deployment are <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a ready state before marking the release as successful. It will <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> as long as --timeout</code> </pre> <br>  This function is not working properly now: not all resources and not all versions of the API are supported.  Yes, and the stated process of waiting does not satisfy our needs. <br><br>  As in the case of using <code>kubectl wait</code> , there is no quick feedback and there is no possibility to regulate this behavior.  If the rollout ends with an error, then we will know about it <b>only after the timeout expires</b> .  In case of a problem installation, you should complete the roll-out process as soon as possible, finish the CI / CD pipeline, roll back the release to the working version and proceed to debugging. <br><br>  If the problematic release was rolled back, and Helm does not return any information during the rollout process, what does debugging amount to?!  In the case of <code>kubectl wait</code> you can organize a separate process to display logs, which will require the names of the release resources.  How to organize a simple and working solution is not immediately clear.  And in addition to the logs of pods, useful information can be contained in the process of rolling out, resource events ... <br><br>  Our <a href="https://github.com/flant/werf">werf</a> CI / CD utility can deploy a Helm chart and monitor resource readiness, as well as display related information.  All data are combined into a single stream and are displayed in the log. <br><br>  This logic is made in a separate decision <a href="https://github.com/flant/kubedog">kubedog</a> .  With the help of the utility, you can subscribe to the resource and receive events and logs, as well as timely learn about the failed rollout.  Those.  as a solution, after calling <code>helm install/upgrade</code> without the <code>--wait</code> option, <code>--wait</code> can call kubedog for each release resource. <br><br>  We sought to make a tool that will provide all the necessary information for debugging in the output of the CI / CD pipeline.  You can read more about the utility in <a href="https://habr.com/ru/company/flant/blog/434160/">our recent article</a> . <br><br>  Perhaps in Helm 3 sometime a similar solution will appear, but so far our <a href="https://github.com/helm/helm/issues/3481">issue</a> is in a hung state. <br><br><h2>  Security when using helm init by default </h2><br>  By default, when the <code>helm init</code> command is executed, the server component is installed in the cluster with rights similar to the superuser, which can lead to undesirable consequences when accessing third parties. <br><br>  To ensure cluster security, it is necessary to limit Tiller's capabilities, as well as take care of the connection - the network security over which communication takes place between the Helm components. <br><br>  The first can be achieved through the use of the standard Kubernetes RBAC mechanism, which will limit the tiller actions, and the second - by configuring SSL.  Read more in the Helm: <a href="https://docs.helm.sh/using_helm/">Securing your Helm Installation</a> documentation. <br><br>  <i>There is an opinion that the presence of the server component - Tiller - is a <b>serious architectural error</b> , literally a foreign resource with root privileges in the Kubernetes ecosystem.</i>  <i>In part, we agree: the implementation is not perfect, but <b>let's take a look at it from the other side</b> .</i>  <i>If you interrupt the deployment process and kill the Helm client, the system will not remain in an uncertain state, i.e.</i>  <i>Tiller will bring the release status to valid.</i>  <i>You also need to understand that despite the fact that in Helm 3 they refuse Tiller, these functions will somehow be performed by the CRD controller.</i> <br><br><h2>  Martian Go Patterns </h2><br>  Go-templates have a large threshold of entry, but the technology has no limitations on the possibilities and problems with DRY.  The basic principles, syntax, functions and operators are discussed in our <a href="https://habr.com/ru/company/flant/blog/423239/">previous article</a> in the Helm series. <br><br><h2>  No secrets out of the box </h2><br>  It is convenient to store and accompany the application code, infrastructure, and templates for rollout, when they are located in one place.  And secrets are no exception. <br><br>  Helm does not support out-of-the-box secrets, but the <a href="https://github.com/futuresimple/helm-secrets">helm-secrets</a> plugin is available, which is essentially a layer between <a href="https://github.com/mozilla/sops">sops</a> , the secret manager of Mozilla, and Helm. <br><br>  When working with secrets, we use our own solution implemented in <a href="https://github.com/flant/werf">werf</a> ( <a href="https://flant.github.io/werf/reference/deploy/secrets.html">secrets documentation</a> ).  Of the features: <br><br><ul><li>  Ease of implementation. </li><li>  Keeping a secret in a file, not just in YAML.  Convenient when storing certificates, keys. </li><li>  Recreation of secrets with the new key. </li><li>  Roll out without a secret key (when using werf).  It can be useful for those cases when the developer does not have this secret key, but there is a need to run the deployment on a test or local circuit. </li></ul><br><h2>  Conclusion </h2><br>  Helm 2 is positioned as a stable product, but at the same time there are many bugs that hang in limbo (some of them are several years old!).  Instead of making decisions or at least patches, all forces are thrown on the development of Helm 3. <br><br>  Despite the fact that MR and issue may hang for months ( <a href="https://github.com/helm/helm/pull/3540">here is an example</a> of how we added a <code>before-hook-creation policy</code> for hooks for several months), it is still possible to participate in the development of the project.  Every Thursday a half-hour Helm developer rally takes place, where you can learn about the priorities and current directions of the team, ask questions and boost their work.  About me and other channels of communication is described in detail <a href="https://github.com/helm/helm">here</a> . <br><br>  To use Helm or not is up to you, of course.  Today we ourselves adhere to such a position that, despite the shortcomings, Helm is an acceptable solution for deployment and participating in its development is beneficial for the whole community. <br><br><h2>  PS </h2><br>  Read also in our blog: <br><br><ul><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/423239/">Creating packages for Kubernetes with Helm: chart structure and template making</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/420437/">Practical acquaintance with the package manager for Kubernetes - Helm</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/417079/">Package Manager for Kubernetes - Helm: Past, Present, Future</a> ‚Äù; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/336170/">Practice with dapp.</a>  <a href="https://habr.com/ru/company/flant/blog/336170/">Part 2. Deploying Docker images in Kubernetes using Helm</a> . ‚Äù </li></ul></div><p>Source: <a href="https://habr.com/ru/post/438814/">https://habr.com/ru/post/438814/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container">Waiting for the list from <a href="../../index.html">here</a>...</nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52319614 = new Ya.Metrika({
                  id:52319614,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52319614" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>