<div class="post__text post__text-html js-mediator-article">  It is assumed that as a result of this method we save the sequence in which the disks are displayed using the ceph osd tree command.  If they are there in order, then it is easier to read and is considered, if necessary. <br><br>  Lyrical digression on the topic.  The official method of replacing a disk in ceph involves deleting all the logical entities associated with this disk from the cluster and then re-creating them.  As a result, a newly-created osd (under some circumstances) can change its number (the number in the entity name, which is osd. The number) and location in the crush map and will naturally appear in another dream on the ceph osd tree and others.  Change its sequence number. <br><br>  The idea of ​​this method is that we will not change any logical entities, but simply slip a new disk to the “old” place in the cluster.  To do this, on this new disk you need to (re) create the correct data structures: all sorts of id, symlinks, and keys. <br><a name="habracut"></a><br>  <b>Mark up the new disk.</b> <br><br><pre><code class="plaintext hljs">parted /dev/диск_с_данными mklabel gpt</code> </pre> <br>  <b>Create a new section on our partition.</b> <br><br><pre> <code class="plaintext hljs">parted /dev/sdaa mkpart primary ext2 0% 100% /sbin/sgdisk --change-name=1:'ceph data' -- /dev/sda1</code> </pre> <br>  <b>We get the UUID of the dead osd</b> <br><br><pre> <code class="plaintext hljs">ceph osd dump|grep 'osd.Номер'</code> </pre> <br>  <b>We put PARTUUID on the data disk</b> <br><br><pre> <code class="plaintext hljs">/sbin/sgdisk --typecode=1:99886b14-7904-4396-acef-c031095d4b62 -- /dev/Диск_с_данными</code> </pre> <br>  <b>Find a section with a magazine</b> <br><br><pre> <code class="plaintext hljs">ceph-disk list | grep for | sort</code> </pre> <br>  <b>Create a file system disk</b> <br><br><pre> <code class="plaintext hljs">/sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdaa1</code> </pre> <br>  <b>Mount FS</b> <br><br><pre> <code class="plaintext hljs">mount -o rw,noatime,attr2,inode64,noquota /dev/Партиция_на_диске_с_данными /var/lib/ceph/osd/ceph-номер_OSD</code> </pre> <br>  <b>We copy data from the next OSD</b> <br><br>  <i>In fact, this is the most disgusting part of the procedure, you need to do everything carefully.</i> <br><br>  When copying, you must skip the / var / lib / ceph / osd / ceph-NUMBER / current directory, this is the data directory.  We will create a symlink to journal later. <br><br>  <b>Copying</b> <br><br><pre> <code class="plaintext hljs">for i in activate.monmap active ceph_fsid fsid journal_uuid keyring magic ready store_version superblock systemd type whoami; do cp /var/lib/ceph/osd/ceph-НОМЕР_СОСЕДА/${i} /var/lib/ceph/osd/ceph-НОМЕР; done</code> </pre> <br>  <b>Looking for a magazine</b> <br><br><pre> <code class="plaintext hljs">ceph-disk list | grep for | sort</code> </pre> <br>  accordingly, we find the partition, and do <br><br><pre> <code class="plaintext hljs">ls -l /dev/disk/by-partuuid | grep Партиция_Номер</code> </pre> <br>  <b>We make a symlink for this UUID</b> <br><br><pre> <code class="plaintext hljs">ln -s /dev/disk/by-partuuid/UUID /var/lib/ceph/osd/ceph-НОМЕР/journal</code> </pre> <br>  <b>We fill fsid with the correct value</b> <br><br>  This fsid is actually a unique id, under which the osd scale is listed in the cluster, it is important because  if you do not guess with id, then the osd-scale itself will not see the cluster and it will be mutual. <br><br>  And the value must be taken from the partuuid partition on the log with the data. <br><br><pre> <code class="plaintext hljs">echo -n UUID &gt;/var/lib/ceph/osd/ceph-НОМЕР/fsid</code> </pre><br>  <b>Fill in the keyring</b> <br><br>  With this, the osd scale is authorized in the cluster. <br><br><pre> <code class="plaintext hljs">ceph auth list|grep --after-context=1 'osd.НОМЕР'</code> </pre> <br>  It is recorded in a file in the format <br><br><pre> <code class="plaintext hljs">[osd.НОМЕР] key = СТРОКА_С_КЛЮЧОМ</code> </pre> <br>  <b>Fill whoami</b> <br><br>  Just write in this file the number of OSD-shki, which we want to revive. <br><br>  <b>We hammer in magazine</b> <br><br><pre> <code class="plaintext hljs">dd bs=32M oflag=direct if=/dev/zero of=/var/lib/ceph/osd/ceph-НОМЕР/journal</code> </pre><br>  <b>Create log metadata and osd-shki</b> <br><br><pre> <code class="plaintext hljs">ceph-osd --mkfs -i Номер_OSD ceph-osd --mkjournal -i Номер_OSD</code> </pre> <br>  <b>Change data owner</b> <br><br><pre> <code class="plaintext hljs">chown -R ceph:ceph /var/lib/ceph/osd/ceph-НОМЕР</code> </pre> <br>  <b>We start ceph-osd</b> <br><br>  Attention: Immediately after the launch of ceph-osd, rebuild will start if, until the moment when the disk exited the cliter, the ceph osd out NUMBER command was given. <br><br><pre> <code class="plaintext hljs">systemctl start ceph-osd.НОМЕР</code> </pre> </div>