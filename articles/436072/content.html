<div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/k4/rl/pc/k4rlpcs5lwl2dsvt97pez6qhfgw.png"></p><br><p><img src="https://habrastorage.org/webt/mp/4c/kh/mp4ckhvyt2k9kydafhv3vxa0its.png" align="left">  <strong>Anton Chaynikov, developer Data Science, Redmadrobot</strong> <br>  <em>Hi, Habr!</em>  <em>Today I will talk about thorns on the way to chatbot, which facilitates the work of the insurance company chat operators.</em>  <em>Or rather, as we taught the bot to distinguish requests from one another through machine learning.</em>  <em>What models they experimented with and what results they got.</em>  <em>How did four approaches to cleaning and enriching the data of decent quality and five attempts to clean the data quality "indecent".</em> <a name="habracut"></a></p><br><h3 id="zadacha">  Task </h3><br><p>  +100500 customer calls per day come to the insurance company chat.  Most of the questions are simple and repetitive, but the operators are no better at it, and customers still have to wait five to ten minutes.  How to improve the quality of service and optimize labor costs so that operators have less routine work, and users have more pleasant sensations from quickly solving their questions? </p><br><p>  And we will make chatbot.  Let him read the messages of users, give simple instructions for simple cases, and ask standard questions for complex cases in order to get the information the operator needs.  A live operator has a script tree — a script (or a block diagram) that says what questions users can ask and how to respond to them.  We would take this scheme and put it in the chatbot, but what a bad luck - the chatbot does not understand humanly and does not know how to relate the user's question to the script branch. </p><br><p>  So, we will teach him with the help of the good old machine learning.  But you can not just take a piece of data generated by users, and teach him a model of decent quality.  To do this, you need to experiment with the architecture of the model, to clean the data, and sometimes to collect it again. </p><br><p>  <strong>How to learn bot:</strong> </p><br><ul><li>  Let us consider the variants of the models: how the dataset size, text vectorization details, dimension reduction, classifier and final accuracy are combined. </li><li>  Clean up decent data: find the classes that you can safely throw;  find out why the last six months of markup better than the previous three;  determine where the model is lying, and where the markup;  find out what typos can be useful for. </li><li>  Let's clean up the “indecent” data: let's see when clustering is useful and useless, how users and operators talk, when it's time to stop suffering and go collect the markup. </li></ul><br><h3 id="faktura">  Texture </h3><br><p>  We had two clients (insurance companies with online chat rooms) and chatboat training projects (we will not call them, it doesn’t matter), with dramatically different data quality.  Well, if half of the problems of the second project were solved by manipulations from the first.  Details below. </p><br><p>  From a technical point of view, our task is to classify texts.  This is done in two stages: first, the texts are vectorized (with the help of tf-idf, doc2vec, etc.), then the classifying model is studied on the vectors (and classes) - random forest, SVM, neural network, and so on.  and so on </p><br><p>  Where does the data come from: </p><br><ul><li>  Sql-upload history messages in the chat.  Relevant upload fields: text messages;  author (client or operator);  group messages in dialogs;  timestamp;  category of client’s application (questions about CTP, CASCO, LCA; questions about the site’s work; questions about loyalty programs; questions about changing insurance conditions, etc.). </li><li>  The tree of scenarios, or the sequence of questions and answers of operators to customers with different requests. </li></ul><br><p>  Without validation, of course, nowhere.  All models were trained on 70% of the data and evaluated on the results for the remaining 30%. </p><br><p>  Quality metrics for the models we used: </p><br><ul><li>  When training: logloss, for differentiability; </li><li>  When writing reports: classification accuracy on a test sample, for simplicity and clarity (including for the customer); </li><li>  When choosing a direction for further action: the intuition of the data scientist, who is closely looking at the results. </li></ul><br><h3 id="eksperimenty-s-modelyami">  Experiments with models </h3><br><p>  Rarely, when the task is immediately clear which model will give the best results.  So here: no experiments anywhere. </p><br><p>  We will try vectorization options: </p><br><ul><li>  tf-idf on separate words; </li><li>  tf-idf on triplets of characters (hereinafter: 3-grams); </li><li>  tf-idf on 2-, 3-, 4-, 5-grams separately; </li><li>  tf-idf on 2, 3, 4, 5 grams, taken all together; </li><li>  All of the above + reduction of words in the source text to the dictionary form; </li><li>  All of the above + dimension reduction using the Truncated SVD method; </li><li>  With the number of measurements: 10, 30, 100, 300; </li><li>  doc2vec, trained on the corpus of texts from the task. </li></ul><br><p>  The classification options on this background look rather poor: SVM, XGBoost, LSTM, random forests, naive bayes, random forest on top of the SVM and XGB predictions. </p><br><p>  And although we checked the reproducibility of the results on three independently assembled datasets and their fragments, we will only vouch for the wide applicability. </p><br><p>  <strong>The results of the experiments:</strong> </p><br><ul><li>  In the chain of “preprocessing-vectorization-lowering dimensionality-classification”, the effect of the choice at each step is almost independent of the other steps.  What is very convenient, you can not go through a dozen options for each new idea and use the best known option at every step. </li><li>  tf-idf in words loses to 3 grams (accuracy 0.72 vs 0.78).  2-, 4-, 5-grams lose to 3-grams (0.75–0.76 vs 0.78).  {2; 5} -grams all together gain very little from 3-grams.  Taking into account the sharp increase in the required memory, for training, we decided to neglect the gain of 0.4% accuracy. </li><li>  Compared to all varieties tf-idf, doc2vec was helpless (accuracy of 0.4 and lower).  It would be worthwhile to try to train him not on the corpus of the task (~ 250000 texts), but on a much larger one (2.5–25 million texts), but so far, alas, have not reached the hands. </li><li>  Truncated SVD did not help.  Accuracy increases monotonically with increasing measurements, smoothly reaching accuracy without TSVD. </li><li>  Among the classifiers, XGBoost wins by a significant margin (+ 5–10%).  The closest competitors are SVM and random forests.  Naive Bayes is not a competitor even to random forests. </li><li>  The success of LSTM is strongly dependent on the size of the dataset: on a sample of 100,000 objects, it is able to compete with XGB.  On a sample of 6000 - lagging behind along with the Bayes. </li><li>  A random forest over SVM and XGB either always agrees with XGB, or makes more mistakes.  This is very sad, we hoped that the SVM would find in the data at least some regularities inaccessible to XGB, but alas. </li><li>  At XGBoost everything is difficult with stability.  For example, updating it from version 0.72 to 0.80 inexplicably reduced the accuracy of the models being trained by 5–10%.  And one more thing: XGBoost supports changing the parameters of training in the course of training and compatibility with the standard scikit-learn API, but strictly separately.  You cannot do both together.  I had to <a href="https://github.com/dmlc/xgboost/pull/3682">fix it.</a> </li><li>  If you bring words to vocabulary form, this slightly improves the quality, in combination with tf-idf in words, but it is useless in all other cases.  In the end, we turned it off to save time. </li></ul><br><h3 id="opyt-1-chistka-dannyh-ili-chto-delat-s-razmetkoy">  Experience 1. Data cleaning, or what to do with markup </h3><br><p>  Chat operators are just people.  In determining the category of user query, they often make mistakes and differently understand the boundaries between categories.  Therefore, the source data must be ruthlessly and intensively cleaned. </p><br><p>  Our data on the training model on the first project: </p><br><ul><li>  The history of online chat messages for several years.  This is 250,000 messages in 60,000 dialogs.  At the end of the dialogue, the operator chose the category to which the user’s reference relates.  In this dataset about 50 categories. </li><li>  Script tree.  In our case, the operators did not have working scripts. </li></ul><br><p>  What exactly the data is bad, we formulated as hypotheses, then checked and, where we could, corrected.  Here's what happened: </p><br><p>  <strong>Approach the first.</strong>  From the whole huge list of classes you can safely leave 5-10. <br>  We reject small classes (&lt;1% of the sample): little data + small impact.  We combine difficult classes to which the operators still react the same way.  For example: <br>  'dms' + 'how to sign up for a doctor' + 'question about filling the program' <br>  'cancellation' + 'status of cancellation' + 'cancellation of paid policy' <br>  'question to extend the' + 'how to extend the policy?' </p><br><p>  Then we throw out classes like “other”, “other” and the like: they are useless for chatbot (redirecting to the operator anyway), and at the same time spoil the accuracy, because 20% (30, 50, 90) of requests are not categorized and here.  Now we throw out the class that the chatbot cannot work with (yet). </p><br><p>  Result: in one case - growth from accuracy 0.40 to 0.69, in the other - from 0.66 to 0.77. </p><br><p>  <strong>Approach the second.</strong>  At the beginning of the chat, the operators themselves are poorly aware of how to choose a class for the user's appeal, therefore there is a lot of “noise” and errors in the data. </p><br><p>  Experiment: we take only the last two (three, six, ...) months of dialogues and train the model on <br>  of them. </p><br><p>  Result: in one remarkable case, the accuracy increased from 0.40 to 0.60, in the other - from 0.69 to 0.78. </p><br><p>  <strong>Approach the third.</strong>  Sometimes an accuracy of 0.70 means not “in 30% of cases the model is mistaken”, but “in 30% of cases the marking is lying, and the model corrects it very reasonably”. </p><br><p>  You cannot test this hypothesis with metrics such as accuracy or logloss.  For the purposes of the experiment, we limited ourselves to the data scientist's gaze, but in the ideal case, it is necessary here to qualitatively redefine datasets, not forgetting about cross-training. </p><br><p>  To work with such samples, we came up with the process of "iterative enrichment": </p><br><ol><li>  Split datasets into 3-4 fragments. </li><li>  To train the model on the first fragment. </li><li>  Predict the trained model classes second. </li><li>  Closely look at the predicted classes and the degree of confidence of the model, choose the boundary value of confidence. </li><li>  Remove from the second fragment texts (objects), predicted with certainty below the boundary, to train the model on this. </li><li>  Repeat until you get bored or fragments run out. </li></ol><br><p>  On the one hand, the results are excellent: the model of the first iteration has an accuracy of 70%, the second - 95%, the third - 99 +%.  A close look at the results of the predictions quite confirm this accuracy. </p><br><p>  On the other hand, how to systematically make sure in this process that subsequent models are not learned from the errors of the previous ones?  There is an idea to test the process on manually “noisy” datasets with high-quality source markup, such as MNIST.  But time for this, alas, was not enough.  And without verification, we did not dare to launch iterative enrichment and the resulting models in production. </p><br><p>  <strong>Approach the fourth.</strong>  Dataset can be expanded - and thus increase the accuracy and reduce retraining, adding to the existing texts a lot of typos. <br>  Types of typos - doubling a letter, skipping a letter, rearranging adjacent letters in places, replacing a letter with an adjacent one on the keyboard. </p><br><p>  Experiment: The proportion of p letters in which a typo will occur: 2%, 4%, 6%, 8%, 10%, 12%.  Increase dataset: usually up to 60,000 replicas.  Depending on the initial size (after the filters), this meant an increase of 3–30 times. </p><br><p>  Result: depends on dataset.  On a small dataset (~ 300 replicas) 4–6% of typos give a stable and significant increase in accuracy (0.40 → 0.60).  On big it is worse.  With a share of misprints of 8% or more, the texts turn into nonsense and the accuracy drops.  With an error rate of 2–8%, accuracy varies in the range of a few percent, very rarely exceeds accuracy without typing errors and, by sensation, it is not worth increasing the training time several times. </p><br><p>  As a result, we obtain a model that distinguishes 5 classes of references with an accuracy of 0.86.  We coordinate with the client the texts of questions and answers for each of the five forks, fasten the texts to the chatbot, send it to QA. </p><br><h3 id="opyt-2-po-koleno-v-dannyh-ili-chto-delat-bez-razmetki">  Experience 2. Knee-high in the data, or what to do without markup </h3><br><p>  Having obtained good results on the first project, we approached the second with all confidence.  But, fortunately, we have not forgotten how to be surprised. </p><br><p>  What we met: </p><br><ul><li>  Tree of scenarios with five branches, agreed with the client about a year ago. </li><li> A tagged sample of 500 messages and 11 classes of unknown origin. </li><li>  A selection of 220,000 messages, 21,000 dialogs and 50 other classes, marked up by chat operators. </li><li>  SVM-model, trained on the first sample, with an accuracy of 0.69, which was inherited from the previous team of data scientists.  Why SVM, history is silent. </li></ul><br><p>  First of all, we look at the classes: in the script tree, in the sample of the SVM model, in the main sample.  And that's what we see: </p><br><ul><li>  The classes of the SVM models roughly correspond to the script branches, but they do not correspond at all to the classes from the large sample. </li><li>  The script tree was written on business processes a year ago, and became outdated almost to no avail.  The SVM model is outdated with it. </li><li>  The two largest classes in the large sample are Sales (50%) and Others (45%). </li><li>  Of the five following classes in size, three are as common as Sales. </li><li>  The remaining 45 classes contain less than 30 dialogues each.  Those.  We do not have a script tree, no list of classes, and no markup. </li></ul><br><p>  What to do in such cases?  We rolled up our sleeves and went on our own to pull out classes and markup from the data. </p><br><p>  <strong>Attempt the first.</strong>  Let's try to cluster user questions, i.e.  The first messages in the dialogue, with the exception of greetings. </p><br><p>  We are checking.  We vector replicas counting 3-gram.  Reduce the dimension to the first ten measurements of TSVD.  Cluster agglomerative clustering with Euclidean distance and the target Ward function.  Once again, we lower the dimension using t-SNE (to two dimensions so that the results can be viewed with the eyes).  We draw replica points on the plane, painting in the color of clusters. </p><br><p>  Result: fear and horror.  Sane clusters, we can assume that there is not: </p><br><img src="https://habrastorage.org/webt/cl/rm/-s/clrm-sda0crxfeyq8ynskurotkk.png"><br><p>  Almost not - there is one, orange on the left, this is because all the messages in it contain a 3-gram "@".  This 3G is a preprocessing artifact.  Somewhere in the process of filtering punctuation marks “@” was not only not filtered out, but also overgrown with spaces.  But the artifact is useful.  In this cluster are users who first write their email.  Unfortunately, only by the presence of mail it is not at all clear what the user's request is.  Moving on. </p><br><p>  <strong>Attempt the second.</strong>  What if operators often respond with more or less standard links? <br>  We are checking.  We pull out link-like substrings from operator messages, we slightly edit the links, differing in spelling, but identical in meaning (http / https, / search? City =% city%), we consider the link frequencies. </p><br><p>  Result: unpromising.  First, operators respond with links only to a small proportion of requests (&lt;10%).  Secondly, even after manual cleansing and filtering out the links that have been met once, there are more than thirty of them left.  Thirdly, there is no particular similarity in the behavior of users who end the dialogue with a link. </p><br><p>  <strong>Attempt the third.</strong>  Let's look for the standard answers of the operators - what if they are indicators of some sort of message classification? </p><br><p>  We are checking.  In each dialogue, we take the last remark of the operator (not counting the farewells: “I can help with something else,” etc.) and consider the frequency of the unique replicas. </p><br><p>  Result: promising, but inconvenient.  50% of operators' responses are unique, another 10–20% are met twice, the remaining 30–40% are covered by a relatively small number of popular templates.  Relatively small - about three hundred.  A close look at these templates shows that many of them are variants of the same meaning of the answer - where they differ by one letter, where by one word, where by one paragraph.  I would like to group these similar answers. </p><br><p>  <strong>Attempt fourth.</strong>  We cluster the last replicas of the operators.  These clusters are much better: </p><br><img src="https://habrastorage.org/webt/cb/to/xh/cbtoxhphxhmfo-94bpelgw1dalq.png"><br><p>  With this you can already work. </p><br><p>  We cluster and draw replicas on the plane, as in the first attempt, manually determine the most clearly separated clusters, remove them from the dataset and cluster anew.  After the separation of approximately half of the dataset, the clear clusters end, and we begin to think about which classes to assign to them.  We scatter clusters according to the original five classes - the sample is skewed, and three of the five original classes do not receive a single cluster.  Poorly.  We scatter clusters in five classes, which we plan randomly, to: “call”, “come”, “wait for an answer a day”, “problems with captcha”, “other”.  The skew is smaller, but the accuracy is only 0.4–0.5.  Bad again.  We assign each of the 30+ clusters its own class.  The sample is again “skewed”, and the accuracy is again 0.5, although about five selected classes have decent accuracy and completeness (0.8 and higher).  But the result is still not impressive. </p><br><p>  <strong>Attempt the fifth.</strong>  We need all the ins and outs of clustering.  Extract the full clustering dendrogram instead of the top thirty clusters.  We save it in a format that is accessible to client analysts, and help them to make markup - we sketch the list of classes. </p><br><p>  For each message, we compute a chain of clusters that include each message, starting from the root.  We build a table with columns: text, id of the first cluster in the chain, id of the second cluster in the chain, ..., id of the cluster corresponding to the text.  We save the table in csv / xls.  Then you can work with it with office tools. </p><br><p>  We give the data and a sketch of the list of classes for marking the client.  Client analysts have redefined ~ 10,000 first user posts.  We have already learned from experience, asked to mark each message at least twice.  And for good reason - 4,000 of these 10,000 have to be thrown away, because the two analysts have marked out differently.  On the remaining 6,000, we rather quickly repeated the successes of the first project: </p><br><ul><li>  Baseline: we do not filter in any way - the accuracy is 0.66. </li><li>  We unite the classes identical from the point of view of the operator.  We get the accuracy of 0.73. </li><li>  We remove the class "Other" - the accuracy increases to 0.79. </li></ul><br><p>  The model is ready, now you need to draw a script tree.  For reasons that we will not explain, we did not have access to the scripts of the operators' responses.  We were not confused, pretended to be users, and for a couple of hours in the field we collected response templates and clarifying questions from operators for all occasions.  Decorated them in a tree, packed in a bot and went to test.  Customer approved. </p><br><h3 id="vyvody-ili-chto-pokazal-opyt">  Conclusions, or that experience has shown: </h3><br><ul><li>  You can experiment with parts of the model (preprocessing, vectorization, classification, etc.) separately. </li><li>  XGBoost still reigns, although if you need something unusual from him, you have problems. </li><li>  The user is a peripheral device of random input, so it is necessary to clean user data. </li><li>  Iterative enrichment is cool, albeit dangerous. </li><li>  Sometimes it is necessary to give the data back to the markup client.  But do not forget to help him get a quality result. </li></ul><br><p>  To be concluded. <cut></cut></p></div>