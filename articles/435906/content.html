<div class="post__text post__text-html js-mediator-article">  Good day, habrovchane.  The task was received - to deploy a fault-tolerant High Available storage using pacamaker + drbd (in dual primary mode) + clvmd + ctdb, which will be mounted on the server.  I will make a reservation that I come across all these tools for the first time and will be happy with criticism and additions / corrections.  Online instructions specifically for this bundle or not, or the information is outdated.  This is a working one at the moment, but there is one problem whose solution I hope to find soon.  All actions must be performed on both nodes, unless otherwise indicated. <br><br><a name="habracut"></a>  Let's get started  We have two virtual machines on CentOS 7. <br><br>  1) For reliability, we introduce them to / etc / hosts <br><br><pre><code class="bash hljs">192.168.0.1 node1 192.168.0.2 node2</code> </pre> <br>  2) There is no DRBD in the standard repositories, so you need to connect a third-party. <br><br><pre> <code class="bash hljs">rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh https://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</code> </pre><br>  3) Install drbd version 8.4 (I didn’t manage to start 9.0 in dual primary mode) <br><br><pre> <code class="bash hljs">yum install -y kmod-drbd84 drbd84-utils</code> </pre><br>  4) Activate and enable drbd kernel module in autoload <br><br><pre> <code class="bash hljs">modprobe drbd <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> drbd &gt; /etc/modules-load.d/drbd.conf</code> </pre><br>  5) Create drbd resource configuration file /etc/drbd.d/r0.res <br><br><pre> <code class="bash hljs">resource r0 { protocol C; device /dev/drbd0; meta-disk internal; disk /dev/sdb; net { allow-two-primaries; } disk { fencing resource-and-stonith; } handlers { fence-peer <span class="hljs-string"><span class="hljs-string">"/usr/lib/drbd/crm-fence-peer.sh"</span></span>; after-resync-target <span class="hljs-string"><span class="hljs-string">"/usr/lib/drbd/crm-unfence-peer.sh"</span></span>; } on node1 { address 192.168.0.1:7788; } on node2 { address 192.168.0.2:7788; }</code> </pre><br>  6) Turn off the drbd unit (the pacemaker will answer for it later), create metadata for the drbd disk, raise the resource <br><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">disable</span></span> drbd drbdadm create-md r0 drbdadm up r0</code> </pre><br>  7) At the first node we make the resource primary <br><br><pre> <code class="bash hljs">drbdadm primary --force r0</code> </pre><br>  8) Put the pacemaker <br><br><pre> <code class="bash hljs">yum install -y pacemaker pcs resource-agents</code> </pre><br>  9) Set a password for the hacluster user for authorization on the nodes <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> CHANGEME | passwd --stdin hacluster</code> </pre><br>  10) Run the pacemaker on both nodes. <br><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> pcsd systemctl start pcsd</code> </pre><br>  11) Log in to the cluster.  From this stage we do everything on one node. <br><br><pre> <code class="bash hljs">pcs cluster auth node1 node2 -u hacluster</code> </pre><br>  12) Create a cluster named samba_cluster <br><br><pre> <code class="bash hljs">pcs cluster setup --force --name samba_cluster node1 node2</code> </pre><br>  13) activate the nodes <br><br><pre> <code class="bash hljs">pcs cluster <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> --all pcs cluster start --all</code> </pre><br>  14) Since we use virtual machines as servers, we disable the STONITH mechanism <br><br><pre> <code class="bash hljs">pcs property <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> stonith-enabled=<span class="hljs-literal"><span class="hljs-literal">false</span></span> pcs property <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> no-quorum-policy=ignore</code> </pre><br>  15) Create a VIP <br><br><pre> <code class="bash hljs">pcs resource create virtual_ip ocf:heartbeat:IPaddr2 ip=192.168.0.10 cidr_netmask=24 op monitor interval=60s</code> </pre><br>  16) Create drbd resource <br><br><pre> <code class="bash hljs">pcs cluster cib drbd_cfg pcs -f drbd_cfg resource create DRBD ocf:linbit:drbd drbd_resource=r0 op monitor interval=60s pcs -f drbd_cfg resource master DRBDClone DRBD master-max=2 master-node-max=1 <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span>-node-max=1 <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span>-max=2 notify=<span class="hljs-literal"><span class="hljs-literal">true</span></span> interleave=<span class="hljs-literal"><span class="hljs-literal">true</span></span> pcs cluster cib-push drbd_cfg</code> </pre><br>  17) Install the necessary clvm packages and prepare clvm <br><br><pre> <code class="bash hljs">yum install -y lvm2-cluster gfs2-utils /sbin/lvmconf --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-cluster</code> </pre> <br>  18) Add the dlm and clvd resource in pacemaker <br><br><pre> <code class="bash hljs">pcs resource create dlm ocf:pacemaker:controld op monitor interval=30s on-fail=fence <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> interleave=<span class="hljs-literal"><span class="hljs-literal">true</span></span> ordered=<span class="hljs-literal"><span class="hljs-literal">true</span></span> pcs resource create clvmd ocf:heartbeat:clvm op monitor interval=30s on-fail=fence <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> interleave=<span class="hljs-literal"><span class="hljs-literal">true</span></span> ordered=<span class="hljs-literal"><span class="hljs-literal">true</span></span> pcs constraint colocation add clvmd-clone with dlm-clone</code> </pre> <br>  19) At this stage, running clvmd and dlm should generate an error.  Go to the web interface pacemaker <a href="http://192.168.0.1/">192.168.0.1</a> : 2224.  If the cluster does not appear, then add it to “Edd existing”.  Next, go to Resources - dlm - optional arguments and set the value of allow_stonith_disabled = true <br><br>  20) Set the queue for loading resources <br><br><pre> <code class="bash hljs">pcs constraint order start DRBDClone <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> dlm-clone pcs constraint order start dlm-clone <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> clvmd-clone</code> </pre><br>  21) Forbid LVM from writing the cache and clearing it.  On both nodes <br><br><pre> <code class="bash hljs">sed -i <span class="hljs-string"><span class="hljs-string">'s/write_cache_state = 1/write_cache_state = 0/'</span></span> /etc/lvm/lvm.conf rm /etc/lvm/cache/*</code> </pre><br>  22) Edit /etc/lvm/lvm.conf so that lvm does not see / dev / sdb.  On both nodes <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># This configuration option has an automatic default value. # filter = [ "a|.*/|" ] filter = [ "r|^/dev/sdb$|" ]</span></span></code> </pre><br>  23) Create a CLVM partition.  We do it only on one node <br><br><pre> <code class="bash hljs">$ vgcreate -Ay -cy cl_vg /dev/drbd0 Physical volume <span class="hljs-string"><span class="hljs-string">"/dev/drbd0"</span></span> successfully created. Clustered volume group <span class="hljs-string"><span class="hljs-string">"cl_vg"</span></span> successfully created $ lvcreate -l100%FREE -n r0 cl_vg Logical volume <span class="hljs-string"><span class="hljs-string">"r0"</span></span> created.</code> </pre><br>  24) Mark up partition in gfs2 <br><br><pre> <code class="bash hljs">mkfs.gfs2 -j2 -p lock_dlm -t drbd-gfs2:r0 /dev/cl_vg/r0</code> </pre><br>  25) Next we add the mounting of this section in the pacemaker and tell it to boot after clvmd <br><br><pre> <code class="bash hljs">pcs resource create fs ocf:heartbeat:Filesystem device=<span class="hljs-string"><span class="hljs-string">"/dev/cl_vg/r0"</span></span> directory=<span class="hljs-string"><span class="hljs-string">"/mnt/"</span></span> fstype=<span class="hljs-string"><span class="hljs-string">"gfs2"</span></span> --<span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> pcs constraint order start clvmd-clone <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> fs-clone</code> </pre><br>  26) Now it’s time ctdb, which will run samba <br><br><pre> <code class="bash hljs">yum install -y samba ctdb cifs-utils</code> </pre><br>  27) Edit the config /etc/ctdb/ctdbd.conf <br><br><pre> <code class="bash hljs">CTDB_RECOVERY_LOCK=<span class="hljs-string"><span class="hljs-string">"/mnt/ctdb/.ctdb.lock"</span></span> CTDB_NODES=/etc/ctdb/nodes CTDB_MANAGES_SAMBA=yes CTDB_LOGGING=file:/var/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>/ctdb.log CTDB_DEBUGLEVEL=NOTICE</code> </pre><br>  28) Create a file with a list of nodes.  ATTENTION!  After each ip in the list of nodes, there must be a newline.  Otherwise, the node will fail at initialization. <br><br><pre> <code class="bash hljs">cat /etc/ctdb/nodes 192.168.0.1 192.168.0.2</code> </pre><br>  29) Add to /etc/samba/smb.conf configuration <br><br><pre> <code class="bash hljs">[global] clustering = yes private dir = /mnt/ctdb lock directory = /mnt/ctdb idmap backend = tdb2 passdb backend = tdbsam [<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>] comment = Cluster Share path = /mnt browseable = yes writable = yes</code> </pre><br>  30) Finally, we create the ctdb resource and indicate that it should load after <br><br><pre> <code class="bash hljs">pcs constraint order start fs-clone <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> samba</code> </pre><br>  And now about the problem that I have not yet decided.  If the node is rebooted, the whole bundle collapses, since drbd takes time to activate / dev / drbd0.  DLM does not see the partition, because it is not yet activated and does not start, etc.  Workaround - activate partition manually and restart pacemaker resources. <br><br><pre> <code class="bash hljs">vgchage -ay pcs resource refresh</code> </pre></div>