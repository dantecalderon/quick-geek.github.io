<div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/_2/bs/h8/_2bsh8brje9tatdcz3fpvpv-l1g.jpeg" alt="Timeline bike"></div><br><p>  Russian-speaking developers always have something to tell: to share some of their unique experiences and opinions.  But in the format of a video blog, due to the high complexity of the recording, only a few now do it. </p><br><p>  Under the cut talked about his difficult way to record and edit video using free software, Ruby scripting and improvised tools. <a name="habracut"></a></p><br><h1 id="teoriya">  Theory </h1><br><p>  I began by studying the theory of recording video blogs on English-language YouTube videos.  And from the Russian-language materials - <a href="https://universarium.org/course/733">this course</a> turned out to be quite useful (in particular, the module about the video blog and the first video about building a frame from the module about the report).  I also briefly familiarized myself with the popular features of proprietary video editors in order to more consciously approach the choice of a free editor. </p><br><p>  He did not risk investing in the light: there is not enough time to study it and search for the best option, and a superficial study of cheap options indicates a potential rake such as flickering and poor color rendering.  With daylight, I did not have great difficulties, it suffices just for short videos. </p><br><h1 id="videoredaktor">  Video editor </h1><br><p>  Existing free video editing tools contain a number of known problems: from bad UI solutions and freezes that turn editing to infinity, to memory leaks, crashes and unexpected artifacts that only appear after the final rendering. </p><br><p>  There are a lot of problems and it took time to select a video editor and study its bugs, just to learn how to cope with basic things.  Eventually he stopped at <strong>Pitivi</strong> , simply because he spent so much time on searching and experimenting. </p><br><h2 id="zvuk-iz-flatpak">  Sound from Flatpak </h2><br><p>  A supported installation method for Pitivi requires Flatpak.  For a while I avoided it because  I do not have systemd and PulseAudio in the system. </p><br><p>  It turns out systemd is <a href="https://flatpak.org/faq/">not required for a</a> long time.  But PulseAudio - it was <s>necessary to install and configure</s> it was easier to <a href="https://github.com/alopatindev/gentoo-overlay-alopatindev/blob/8f9809869c2877089d1884d483d73c938249bdc5/sys-apps/flatpak/files/flatpak-alsa.patch">modify Flatpak</a> .  But it would be better to put PulseAudio, it's just a bit tedious and unclear whether to expect from him problems with recording sound on existing hardware or not. </p><br><p>  Install Pitivi, remove PulseAudio configs, run: </p><br><pre><code class="bash hljs">$ sudo flatpak remote-add --<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo $ sudo flatpak install flathub org.pitivi.Pitivi $ sudo find {/var/lib,~/.<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share}/flatpak/runtime -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -name <span class="hljs-string"><span class="hljs-string">'*pulseaudio*.conf'</span></span> -delete $ flatpak run --device=alsa --branch=stable --arch=x86_64 --<span class="hljs-built_in"><span class="hljs-built_in">command</span></span>=pitivi org.pitivi.Pitivi</code> </pre> <br><p>  No sound.  Let's try to run something simpler, for example <code>aplay</code> : </p><br><pre> <code class="bash hljs">$ sudo find /var/lib/flatpak/app/org.pitivi.Pitivi/x86_64 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> d -path <span class="hljs-string"><span class="hljs-string">'*/files/bin'</span></span> -<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cp `<span class="hljs-built_in"><span class="hljs-built_in">which</span></span> aplay` {} \; $ flatpak run --device=alsa --branch=stable --arch=x86_64 --<span class="hljs-built_in"><span class="hljs-built_in">command</span></span>=aplay org.pitivi.Pitivi /dev/urandom ALSA lib dlmisc.c:162:(snd_dlsym_verify) unable to verify version <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> symbol _snd_pcm_empty_open ALSA lib dlmisc.c:283:(snd1_dlobj_cache_get) symbol _snd_pcm_empty_open is not defined inside [<span class="hljs-built_in"><span class="hljs-built_in">builtin</span></span>] aplay: main:828: audio open error: No such device or address</code> </pre> <br><p>  Probably the <code>alsa-lib</code> included in Flatpak was compiled with <code>--with-versioned</code> .  A quick fix is ​​to replace <code>libasound.so</code> system one: </p><br><pre> <code class="bash hljs">$ sudo find /var/lib/flatpak -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -name libasound.so.2.0.0 -<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cp /usr/lib64/libasound.so.2.0.0 {} \; $ find ~/.<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/flatpak -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -name libasound.so.2.0.0 -<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cp /usr/lib64/libasound.so.2.0.0 {} \; <span class="hljs-comment"><span class="hljs-comment"># если устанавливали от прав пользователя что-то</span></span></code> </pre> <br><p>  For me this was not enough: </p><br><pre> <code class="bash hljs">$ flatpak run --device=alsa --branch=stable --arch=x86_64 --<span class="hljs-built_in"><span class="hljs-built_in">command</span></span>=aplay org.pitivi.Pitivi /dev/urandom ALSA lib /var/tmp/portage/media-libs/alsa-lib-1.1.6-r1/work/alsa-lib-1.1.6/src/pcm/pcm_direct.c:1943:(snd1_pcm_direct_parse_open_conf) The field ipc_gid must be a valid group (create group audio) aplay: main:828: audio open error: Invalid argument</code> </pre> <br><p>  We also need the ALSA config: </p><br><pre> <code class="bash hljs">$ sudo find /var/lib/flatpak -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> d -name etc -<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cp /etc/asound.conf {} \; $ find ~/.<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/flatpak -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> d -name etc -<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cp /etc/asound.conf {} \; <span class="hljs-comment"><span class="hljs-comment"># если устанавливали от прав пользователя что-то $ flatpak run --device=alsa --branch=stable --arch=x86_64 --command=aplay org.pitivi.Pitivi /dev/urandom</span></span></code> </pre> <br><p>  Finally, you can use Pitivi. </p><br><div class="spoiler">  <b class="spoiler_title">Rendering settings for Pitivi, which came as a result</b> <div class="spoiler_text"><ul><li>  container format: MP4 </li><li>  video <br><ul><li>  codec x264enc </li><li>  advanced <br><ul><li>  encoding pass / type: constant quantizer </li><li>  constant quantizer: 18 </li><li>  bitrate: 16384 kbit / s </li><li>  speed quality preset: ultrafast </li><li>  psychovisual tuning preset: film </li></ul></li></ul></li><li>  audio <br><ul><li>  libav ALAC </li></ul></li><li>  at my own risk and fear I use “Never render from proxy files” </li><li>  everything else is default </li></ul></div></div><br><h2 id="drugie-effekty">  Other effects </h2><br><p>  I do some animation effects for text using a screencast of full-screen pages that are laid out using reveal.js and animate.css.  In reveal.js for some slides I add a transition sound: </p><br><pre> <code class="html hljs xml"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">section</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">style</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"font-size: 5em"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">audio</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">data-autoplay</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">src</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"/path/to/sound.wav"</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">audio</span></span></span><span class="hljs-tag">&gt;</span></span> #1 <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">section</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br><p>  It turned out to be important to record a screencast with 60 FPS, if the text is very large.  I do the screencast like this: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh SOUND_INPUT=shared_input_loopback CHANNELS=2 SOUND_RATE=48000 FRAMERATE=60 DRAW_MOUSE=0 VIDEO_SIZE=$(xdpyinfo | awk '/dimensions:/ { print $2; exit }') OUTPUT="${HOME}/video/screen/$(date --rfc-3339=seconds).mp4" ffmpeg \ -thread_queue_size 512 \ -video_size "${VIDEO_SIZE}" \ -framerate "${FRAMERATE}" \ -f x11grab \ -draw_mouse "${DRAW_MOUSE}" \ -i :0.0+0,0 \ -thread_queue_size 512 \ -f alsa \ -ac "${CHANNELS}" \ -i "${SOUND_INPUT}" \ -ar "${SOUND_RATE}" \ -vcodec libx264 -preset ultrafast -crf 18 \ -acodec alac \ -f ipod \ "${OUTPUT}"</span></span></code> </pre> <br><p>  In my case, <code>shared_input_loopback</code> is a device from the <a href="">asound.conf config</a> . </p><br><p>  Also, this <a href="https://github.com/transitive-bullshit/ffmpeg-concat">add-</a> on <code>ffmpeg</code> for <a href="https://gl-transitions.com/gallery">transitions</a> between clips has proven to be useful. </p><br><h1 id="zapis-video">  Video recording </h1><br><p>  The handset had a Meizu MX4 phone, on which I decided to use the front camera and record using Open Camera.  It took some time to train yourself to look at the camera and control its position in space, without making typical mistakes, like cutting off the head.  At the same time, speak quite clearly, loudly, gesticulating and generating at least some kind of facial expression.  But that was only the beginning. </p><br><p>  What prompted me to do automatic video cutting, and even at the recording stage? </p><br><ol><li>  Pitivi brakes and bugs when editing, especially when using the <a href="http://www.pitivi.org/manual/trimming.html">Ripple Move / Edit</a> tool, leading to the need to periodically restart Pitivi. </li><li>  For me, the process of manual video cutting is one of the most boring things.  It is clear that it is not possible to fully automate this (at least without a scenario in which the pauses necessary for the realization of what was said are not indicated), but at least this process can be optimized. </li></ol><br><p>  Here are the requirements for the future bike I set myself: </p><br><ol><li>  Record video using an Android phone, and sound using a laptop. </li><li>  Focus control camera. </li><li>  Ability to stop recording to save or delete the last recorded fragment. </li><li>  Downloading video from a phone via USB, with retries and <a href="https://gist.github.com/alopatindev/e94ff95ea834500abe2da81ac2a7764f">resume</a> , without blocking the ability to record the next piece. </li><li>  <a href="https://nerd.mmccoo.com/2017/06/19/automatically-aligning-multiple-videoaudio-clips-in-kdenlive/">Sync</a> audio. </li><li>  Determining the presence of voice and throwing pauses. </li><li>  The ability to quickly play the last recorded video fragments, with already discarded pauses. </li></ol><br><p>  Why so much control over the devices during the recording phase?  Why not just start recording for several hours in a row, and then edit it?  There are many reasons: </p><br><ol><li>  Banal lack of disk space. </li><li>  The tendency of the phone to overheat and quickly discharge during a long recording. </li><li>  Malfunction of the touch screen due to the fact that the phone has been in the water.  And somehow you need to control the focus.  And the next press would create an unnecessary vibration of the device. </li><li>  Problems with transferring large files due to poor USB port power on my laptop.  In theory, this can be solved using a <a href="https://android.stackexchange.com/questions/12491/adb-constantly-disconnects-shows-device-offline/82596">USB hub</a> with additional power.  Using a network is too slow. </li><li>  The desire to quickly review the last recorded fragments to make sure that there are no errors and to rewrite them promptly until the planet has turned in the wrong place in front of the sun. </li><li>  The desire to throw out obviously bad doubles as early as possible in order not to waste time and disk space on them in the future. </li><li>  The need to synchronize long audio recorded by phone and laptop.  There may be out of sync with the video due to the fact that frames of audio streams are thrown away both when recording from a laptop and when recording from a phone (which can certainly be solved somehow, but you don’t want to risk and waste time on experiments).  It is easier to synchronize small fragments separately, then the possible out of sync will not be noticeable. </li><li>  The need to handle the situation when Open Camera restarts recording due to the 4 GiB video size reaching.  Probably would have to modify the Open Camera.  If this limitation on 4 GiB cannot be removed or increased, it would be necessary to throw an event at the laptop so that he would note that the recording was restarted at this point. </li></ol><br><p>  It is easier to record in small fragments and make a primitive automation of all that is possible.  As the main language for the development of the bike chose Ruby.  Actually now I would probably choose Python, but at that time I was just learning Ruby, and I’m running languages ​​that are new to me in such weird experiments. </p><br><h2 id="avtomaticheskaya-narezka-video">  Automatic video slicing </h2><br><p>  Information on the network on this topic is not very much.  I remembered late about <a href="https://graphics.stanford.edu/papers/roughcut/">Stanford and Adobe research</a> (which is not scary, I still need a less sophisticated solution). </p><br><p>  The cutting takes place in 2 stages: at the recording stage - rough, at the rendering stage - more accurate, with the ability to manually correct too much trimmed fragments.  Rough implemented using <abbr title="Voice Activity Detection">VAD</abbr> from WebRTC.  More accurate - using Google Speech (if more specifically - using a modification of the <a href="https://github.com/agermanidis/autosub">autosub</a> project, to generate subtitles for video).  I am sure that there will be more successful solutions, it just turned out to be the best of what we managed to do quickly. </p><br><p>  If you want to develop something similar using <code>ffmpeg</code> - stick to the principle of not trying to do too much in one <code>ffmpeg</code> call.  Take intermediate files and monitor each step, so you don’t have to search for strange unguided bugs, such as improper cuts or unused effects. </p><br><p>  I run the <a href="http://github.com/alopatindev/vlog-toolset/"><strong>resulting disgrace</strong></a> something like this: </p><br><pre> <code class="plaintext hljs">$ bin/vlog-recorder \ --project /path/to/project \ --debug true \ --sound-settings ' --device=usb_card --format=dat' # аргументы к arecord r - (RE)START recording s - STOP and SAVE current clip S - STOP and SAVE current clip, don't use auto trimming d - STOP and DELETE current clip p - PLAY last saved clip f - FOCUS camera on center h - show HELP q / Ctrl+C - QUIT [ stopped ] [ battery: 100% / 36°C ]</code> </pre> <br><p>  The arguments to the <code>arecord</code> I need to explicitly specify the device in order to avoid periodic glitches, which are most likely due to the ALSA-vskogo dsnoop plugin.  You can also open the log to control the process of downloading files from your phone: <code>tail -f /path/to/project/log.txt</code> . </p><br><p>  Quickly srenerit in one video for preview, you can like this: </p><br><pre> <code class="bash hljs">$ bin/vlog-render \ --project /path/to/project \ --language ru \ --video-filters <span class="hljs-string"><span class="hljs-string">'hqdn3d,hflip,curves=psfile=/path/to/curves.acv,vignette'</span></span> \ --speed 1.3 \ --fps 60 \ --preview <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  The argument <code>--video-filters</code> are <a href="https://ffmpeg.org/ffmpeg-filters.html">filters</a> passed to <code>ffmpeg</code> .  The video will automatically open in the <code>mpv</code> player. </p><br><p>  You can also swap or throw out the remaining unnecessary duplicates by editing the appeared file / <code>/path/to/project/render.conf</code> , which can be detected through the recognized voice.  The idea, by the way, is <a href="https://github.com/OpenNewsLabs/autoEdit_2">not new</a> .  You can also speed up individual fragments and edit unsuccessful video cuts, if there are any.  Next time, <code>vlog-render</code> re-read <code>render.conf</code> and apply the changes. </p><br><p>  To prepare fragments for a video editor, you need to specify <code>--preview false</code> .  In addition to the fragments that will lie in the <code>output</code> , it still <code>output.mp4</code> them into one file <code>output.mp4</code> , because initially I was not sure: </p><br><ul><li>  will i use small clips in Pitivi </li><li>  or upload one long video for further cutting (so that you can apply a number of effects to the “group” of clips). </li></ul><br><p>  I mainly use the first option.  The second was useful in one video with a bad light: there I used only a piece of <code>output.mp4</code> .  For the second option, the <code>vlog-play-segments</code> script can also be useful: with it, you can quickly see all the pauses between clips in descending order of duration.  This will help more accurately <code>render.conf</code> and save time later editing this long piece of video in Pitivi. </p><br><p>  The resulting small clips can be downloaded at once to the timeline in Pitivi: select all the imported clips and drag them using drag-n-drop. </p><br><h2 id="kreplenie-dlya-telefona">  Phone mount </h2><br><p>  I didn’t want to search for a suitable phone stand, and my hands were already itching to write down anything.  We take the piece of cardboard that came to hand and cut out the phone mount to fit our needs: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kl/u8/tj/klu8tjf8huzxyqf71-ci7xnah_w.jpeg" alt="Phone mount"></div><br><p>  The stand is mounted on the laptop display to minimize the distance between the script and the camera. </p><br><h1 id="zapis-zvuka">  Sound recording </h1><br><p>  Acceptable sound is <a href="https://news.usc.edu/141042/why-we-believe-something-audio-sound-quality/">very critical</a> .  At hand was the microphone Boya BY-M1.  Although it is advertised as an omnidirectional microphone, a good sound in practice is obtained only when you use it as unidirectional. </p><br><p>  The microphone stand is even simpler to do: take the bottle from the pomegranate juice that came to hand, roll an adhesive tape and assemble this designer together: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xp/qt/1u/xpqt1ue6ikhdrwwh6oqr1x83vo0.jpeg" alt="Microphone stand"></div><br><p>  You can also put a towel under this design to suppress part of the vibrations from the table and at the same time adjust the height. </p><br><h2 id="zvukovaya-karta">  Sound card </h2><br><p>  In my case, this is ASUS Xonar U3.  It turned out, however, that it is not compatible with such a microphone: the microphone has a <a href="https://pikabu.ru/story/likbez_po_garnituram_i_naushnikam_sovmestimost_peredelka_remont_4596598">CTIA</a> plug designed for telephones.  The problem was solved by an adapter to TRS plugs for a microphone and headphones.  And it was not easy to find it: manufacturers of such adapters rarely write details.  In my case, some Cablexpert CCA-418W helped. </p><br><p>  Another problem with this card is in <a href="https://manual.audacityteam.org/man/dc_offset.html">DC offset</a> in the right channel when recording.  That does not interfere, because  I'm still in mono.  And for software that does not allow to set up mono, a <a href="">redirection of a</a> good channel to a bad one was done using ALSA. </p><br><p>  Also this card is afraid of overheating.  You need to keep it away from the cooler, otherwise it will slow down and record sound in jerks. </p><br><h2 id="obrabotka-zvuka">  Sound processing </h2><br><p>  I edit the sound in the headphones (in my case it is the Pioneer SE-M390), at a volume higher than the one on which I usually listen to music.  Algorithm like this: </p><br><ol><li>  With the help of Pitivi, I render a separate sound (using the same ALAC and MP4).  I often make several separate tracks, selecting specific layers in Pitivi and temporarily removing unnecessary ones. </li><li>  If the resulting files are immediately loaded into Audacity, we <strong>will lose</strong> stretching / compression of the audio stream, which then may lead to out-of-sync video and audio.  What is not obvious, this does not happen with all videos.  To prevent this from happening, simply apply these stretch / compressions: <code>ffmpeg -async 1 -i input.mp4 output.flac</code> </li><li>  Load all tracks into Audacity.  Add background music if needed. </li><li>  For all tracks, set the desired volume using Gain. </li><li>  We apply the effects of Noise Reduction (double in my case), Compressor and Equalization according to the tips from <a href="https://www.youtube.com/watch%3Fv%3DO5H7xRzjVkw%26t%3D2m44s">this video</a> . </li><li>  Align and increase the volume at the track with voice.  One of the classic methods is Normalize, Amplify, Limiter and again Normalize, but I have not yet managed to get the desired sound quality with this approach.  <s>I temporarily do this: first, I do Gain for the whole track so that the loudest part sounds without overloads, and then manually apply Amplify for individual fragments.</s>  <strong>Update</strong> : another powerful way is RMS Normalize, Limiter and Normal Normalize.  RMS Normalize and Limiter settings can be taken <a href="https://wiki.audacityteam.org/wiki/Audiobook_Mastering">from here</a> .  Still, this method was not useful to me, because  I still decided to switch to another microphone (Zoom H1n) with a built-in Limiter, which suits me (so, with the new microphone, I most likely have to do only normal Normalize, instead of all these things). </li><li>  A microphone sometimes records sound with some defects that look like clicks.  They can be removed using the Spectral edit multi tool effect.  Most often, it has to be applied several times in succession for a selected area, using Ctrl + R.  <strong>Update</strong> : thanks to a new microphone, I found out that these defects are related to something external, most likely it is a combination of <a href="https://www.gravyforthebrain.com/secrets-preventing-mouth-clicks/">noise in the mouth</a> and other extraneous sounds. </li><li>  We export from Audacity to FLAC and merge everything into one file: <code>ffmpeg -i sound.flac -an -i video.mp4 -c copy output.mkv</code> </li><li>  At least I checked the first video at different volumes and different devices. </li></ol><br><h1 id="rezultat">  Result </h1><br><p>  Taking the opportunity to relax the rules, I invite you to visit the resulting <a href="https://www.youtube.com/channel/UCjNAnQpPQydNLTHcVz0s44A"><strong>YouTube channel</strong></a> , where I share insights on effective learning programming and related disciplines. </p><br><p>  Good luck in developing programs and creating video blogs! </p><br><p>  <strong>Update</strong> : <a href="https://alopatindev.github.io/2019/02/05/video-recording-with-automatic-jump-cuts-using-open-source-and-coding/">translated</a> this article for your English-language blog. </p></div>